{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cddb1a4-e418-45c6-a9c6-10fbb13b738d",
   "metadata": {},
   "source": [
    "# Pytorch lightning bolts SAC algo\n",
    "\n",
    "Original file: \n",
    "https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/sac_model.py#L28-L384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228eb008-707f-486e-9054-61fda5903c52",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ec208-786a-41ba-a4f3-b354664eca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import Tensor, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecafbb3-da98-4c35-bee8-102014d534c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DONE from pl_bolts.datamodules.experience_source import Experience, ExperienceSourceDataset\n",
    "from pl_bolts.models.rl.common.agents import SoftActorCriticAgent\n",
    "from pl_bolts.models.rl.common.memory import MultiStepBuffer\n",
    "from pl_bolts.models.rl.common.networks import MLP, ContinuousMLP\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24486d0f-6e74-4046-be21-b388e8a6dff2",
   "metadata": {},
   "source": [
    "## *Experience* and *ExperienceSourceDataset*\n",
    "\n",
    "Source: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/datamodules/experience_source.py\n",
    "\n",
    "This part replaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ddb28a-a6d4-4dbb-8f97-67652c451924",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pl_bolts.datamodules.experience_source import Experience, ExperienceSourceDataset\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10a276b-0fab-4fa5-a748-9930de7cfb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from collections import deque, namedtuple\n",
    "from typing import Callable, Iterator, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"])\n",
    "\n",
    "\n",
    "class ExperienceSourceDataset(IterableDataset):\n",
    "    \"\"\"Basic experience source dataset.\n",
    "    Takes a generate_batch function that returns an iterator. The logic for the experience source and how the batch is\n",
    "    generated is defined the Lightning model itself\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generate_batch: Callable) -> None:\n",
    "        self.generate_batch = generate_batch\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        iterator = self.generate_batch()\n",
    "        return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8816b22-0f6e-4a8c-9f40-dc5f52853411",
   "metadata": {},
   "source": [
    "## *MultipStepBuffer*\n",
    "\n",
    "Source: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/common/memory.py\n",
    "\n",
    "This part replaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42815282-70f3-47d3-9aaf-ea139c77af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pl_bolts.models.rl.common.memory import MultiStepBuffer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7451a1a-4138-4f76-9874-5fed0e1b0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuple for storing experience steps gathered in training\n",
    "import collections\n",
    "from collections import deque, namedtuple\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"])\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    \"\"\"Basic Buffer for storing a single experience at a time.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: size of the buffer\n",
    "        \"\"\"\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    # pylint: disable=unused-argument\n",
    "    def sample(self, *args) -> Union[Tuple, List[Tuple]]:\n",
    "        \"\"\"\n",
    "        returns everything in the buffer so far it is then reset\n",
    "        Returns:\n",
    "            a batch of tuple np arrays of state, action, reward, done, next_state\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in range(self.__len__())))\n",
    "\n",
    "        self.buffer.clear()\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )\n",
    "\n",
    "\n",
    "class ReplayBuffer(Buffer):\n",
    "    \"\"\"Replay Buffer for storing past experiences allowing the agent to learn from them.\"\"\"\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        \"\"\"Takes a sample of the buffer.\n",
    "        Args:\n",
    "            batch_size: current batch_size\n",
    "        Returns:\n",
    "            a batch of tuple np arrays of state, action, reward, done, next_state\n",
    "        \"\"\"\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=np.bool),\n",
    "            np.array(next_states),\n",
    "        )\n",
    "\n",
    "\n",
    "class MultiStepBuffer(ReplayBuffer):\n",
    "    \"\"\"N Step Replay Buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, n_steps: int = 1, gamma: float = 0.99) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            capacity: max number of experiences that will be stored in the buffer\n",
    "            n_steps: number of steps used for calculating discounted reward/experience\n",
    "            gamma: discount factor when calculating n_step discounted reward of the experience being stored in buffer\n",
    "        \"\"\"\n",
    "        super().__init__(capacity)\n",
    "\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.history = deque(maxlen=self.n_steps)\n",
    "        self.exp_history_queue = deque()\n",
    "\n",
    "    def append(self, exp: Experience) -> None:\n",
    "        \"\"\"Add experience to the buffer.\n",
    "        Args:\n",
    "            exp: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        self.update_history_queue(exp)  # add single step experience to history\n",
    "        while self.exp_history_queue:  # go through all the n_steps that have been queued\n",
    "            experiences = self.exp_history_queue.popleft()  # get the latest n_step experience from queue\n",
    "\n",
    "            last_exp_state, tail_experiences = self.split_head_tail_exp(experiences)\n",
    "\n",
    "            total_reward = self.discount_rewards(tail_experiences)\n",
    "\n",
    "            n_step_exp = Experience(\n",
    "                state=experiences[0].state,\n",
    "                action=experiences[0].action,\n",
    "                reward=total_reward,\n",
    "                done=experiences[0].done,\n",
    "                new_state=last_exp_state,\n",
    "            )\n",
    "\n",
    "            self.buffer.append(n_step_exp)  # add n_step experience to buffer\n",
    "\n",
    "    def update_history_queue(self, exp) -> None:\n",
    "        \"\"\"Updates the experience history queue with the lastest experiences. In the event of an experience step is\n",
    "        in the done state, the history will be incrementally appended to the queue, removing the tail of the\n",
    "        history each time.\n",
    "        Args:\n",
    "            env_idx: index of the environment\n",
    "            exp: the current experience\n",
    "            history: history of experience steps for this environment\n",
    "        \"\"\"\n",
    "        self.history.append(exp)\n",
    "\n",
    "        # If there is a full history of step, append history to queue\n",
    "        if len(self.history) == self.n_steps:\n",
    "            self.exp_history_queue.append(list(self.history))\n",
    "\n",
    "        if exp.done:\n",
    "            if 0 < len(self.history) < self.n_steps:\n",
    "                self.exp_history_queue.append(list(self.history))\n",
    "\n",
    "            # generate tail of history, incrementally append history to queue\n",
    "            while len(self.history) > 2:\n",
    "                self.history.popleft()\n",
    "                self.exp_history_queue.append(list(self.history))\n",
    "\n",
    "            # when there are only 2 experiences left in the history,\n",
    "            # append to the queue then update the env stats and reset the environment\n",
    "            if len(self.history) > 1:\n",
    "                self.history.popleft()\n",
    "                self.exp_history_queue.append(list(self.history))\n",
    "\n",
    "            # Clear that last tail in the history once all others have been added to the queue\n",
    "            self.history.clear()\n",
    "\n",
    "    def split_head_tail_exp(self, experiences: Tuple[Experience]) -> Tuple[List, Tuple[Experience]]:\n",
    "        \"\"\"Takes in a tuple of experiences and returns the last state and tail experiences based on if the last\n",
    "        state is the end of an episode.\n",
    "        Args:\n",
    "            experiences: Tuple of N Experience\n",
    "        Returns:\n",
    "            last state (Array or None) and remaining Experience\n",
    "        \"\"\"\n",
    "        last_exp_state = experiences[-1].new_state\n",
    "        tail_experiences = experiences\n",
    "\n",
    "        if experiences[-1].done and len(experiences) <= self.n_steps:\n",
    "            tail_experiences = experiences\n",
    "\n",
    "        return last_exp_state, tail_experiences\n",
    "\n",
    "    def discount_rewards(self, experiences: Tuple[Experience]) -> float:\n",
    "        \"\"\"Calculates the discounted reward over N experiences.\n",
    "        Args:\n",
    "            experiences: Tuple of Experience\n",
    "        Returns:\n",
    "            total discounted reward\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        for exp in reversed(experiences):\n",
    "            total_reward = (self.gamma * total_reward) + exp.reward\n",
    "        return total_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3d8fd6-61dc-4dcf-a879-2db324860d78",
   "metadata": {},
   "source": [
    "## *MLP* and *ContinuousMLP*\n",
    "\n",
    "Source: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/common/networks.py\n",
    "\n",
    "Imports a custom MultiVariateNormal: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/common/distributions.py\n",
    "\n",
    "This part replaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116bbf6-3eb2-4660-94fe-5b164ab4606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pl_bolts.models.rl.common.networks import MLP, ContinuousMLP\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c395a043-60ed-43d5-becc-72da0c96b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import FloatTensor, Tensor, nn\n",
    "from torch.distributions import Categorical, Normal, MultivariateNormal\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TanhMultivariateNormal(torch.distributions.MultivariateNormal):\n",
    "    \"\"\"The distribution of X is an affine of tanh applied on a normal distribution.\n",
    "    X = action_scale * tanh(Z) + action_bias\n",
    "    Z ~ Normal(mean, variance)\n",
    "    \n",
    "    AJ Zerouali, 23/06/21: They forgot about the devices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_bias, action_scale, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.action_bias = action_bias\n",
    "        self.action_scale = action_scale\n",
    "\n",
    "    def rsample_with_z(self, sample_shape=torch.Size()):\n",
    "        \"\"\"Samples X using reparametrization trick with the intermediate variable Z.\n",
    "        Returns:\n",
    "            Sampled X and Z\n",
    "        \"\"\"\n",
    "        z = super().rsample()\n",
    "        '''\n",
    "        # DEBUG\n",
    "        print(f\"z.device = {z.device}\")\n",
    "        print(f\"type(z) = {type(z)}\")\n",
    "        print(f\"self.action_scale.device = {self.action_scale.device}\")\n",
    "        print(f\"self.action_bias.device = {self.action_bias.device}\")\n",
    "        #print(f\"next(self.parameters()).is_cuda = {next(self.parameters()).is_cuda}\")\n",
    "        '''\n",
    "        \n",
    "        action_scale = torch.Tensor(self.action_scale).to(z.device)\n",
    "        action_bias = torch.Tensor(self.action_bias).to(z.device)\n",
    "        \n",
    "        output = (action_scale * torch.tanh(z) + action_bias, z)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def log_prob_with_z(self, value, z):\n",
    "        \"\"\"Computes the log probability of a sampled X.\n",
    "        Refer to the original paper of SAC for more details in equation (20), (21)\n",
    "        Args:\n",
    "            value: the value of X\n",
    "            z: the value of Z\n",
    "        Returns:\n",
    "            Log probability of the sample\n",
    "        \"\"\"\n",
    "        action_scale = torch.Tensor(self.action_scale).to(z.device)\n",
    "        action_bias = torch.Tensor(self.action_bias).to(z.device)\n",
    "        \n",
    "        value = (value - action_bias) / action_scale\n",
    "        z_logprob = super().log_prob(z)\n",
    "        correction = torch.log(action_scale * (1 - value ** 2) + 1e-7).sum(1)\n",
    "        return z_logprob - correction\n",
    "\n",
    "    def rsample_and_log_prob(self, sample_shape=torch.Size()):\n",
    "        \"\"\"Samples X and computes the log probability of the sample.\n",
    "        Returns:\n",
    "            Sampled X and log probability\n",
    "        \"\"\"\n",
    "        \n",
    "        z = super().rsample()\n",
    "        z_logprob = super().log_prob(z)\n",
    "        value = torch.tanh(z)\n",
    "        \n",
    "        action_scale = torch.Tensor(self.action_scale).to(z.device)\n",
    "        action_bias = torch.Tensor(self.action_bias).to(z.device)        \n",
    "        \n",
    "        correction = torch.log(action_scale * (1 - value ** 2) + 1e-7).sum(1)\n",
    "        return action_scale * value + action_bias, z_logprob - correction\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        fz, z = self.rsample_with_z(sample_shape)\n",
    "        return fz\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        \n",
    "        action_scale = torch.Tensor(self.action_scale).to(value.device)\n",
    "        action_bias = torch.Tensor(self.action_bias).to(value.device)\n",
    "        \n",
    "        value = (value - action_bias) / action_scale\n",
    "        z = torch.log(1 + value) / 2 - torch.log(1 - value) / 2\n",
    "        return self.log_prob_with_z(value, z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9ced1bd-e5e1-41aa-9855-50801cca2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple MLP network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_shape: Tuple[int], n_actions: int, hidden_size: int = 128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape: observation shape of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        \"\"\"Forward pass through network.\n",
    "        Args:\n",
    "            x: input to network\n",
    "        Returns:\n",
    "            output of network\n",
    "        \"\"\"\n",
    "        return self.net(input_x.float())\n",
    "\n",
    "class ContinuousMLP(nn.Module):\n",
    "    \"\"\"MLP network that outputs continuous value via Gaussian distribution.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int],\n",
    "        n_actions: int,\n",
    "        hidden_size: int = 128,\n",
    "        action_bias: int = 0,\n",
    "        action_scale: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_shape: observation shape of the environment\n",
    "            n_actions: dimension of actions in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "            action_bias: the center of the action space\n",
    "            action_scale: the scale of the action space\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.action_bias = action_bias\n",
    "        self.action_scale = action_scale\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(input_shape[0], hidden_size), nn.ReLU(), nn.Linear(hidden_size, hidden_size), nn.ReLU()\n",
    "        )\n",
    "        self.mean_layer = nn.Linear(hidden_size, n_actions)\n",
    "        self.logstd_layer = nn.Linear(hidden_size, n_actions)\n",
    "\n",
    "    def forward(self, x: FloatTensor) -> TanhMultivariateNormal:\n",
    "        \"\"\"Forward pass through network. Calculates the action distribution.\n",
    "        Args:\n",
    "            x: input to network\n",
    "        Returns:\n",
    "            action distribution\n",
    "        \"\"\"\n",
    "        # DEBUG\n",
    "        #print(f\"x.device = {x.device}\")\n",
    "        #print(f\"next(self.parameters()).is_cuda = {next(self.parameters()).is_cuda}\")\n",
    "        \n",
    "        x = self.shared_net(x.float())\n",
    "        batch_mean = self.mean_layer(x)\n",
    "        logstd = torch.clamp(self.logstd_layer(x), -20, 2)\n",
    "        batch_scale_tril = torch.diag_embed(torch.exp(logstd))\n",
    "        output = TanhMultivariateNormal(action_bias=self.action_bias, \n",
    "                                        action_scale=self.action_scale, \n",
    "                                        loc=batch_mean, \n",
    "                                        scale_tril=batch_scale_tril,)\n",
    "        return output\n",
    "\n",
    "    def get_action(self, x: FloatTensor) -> Tensor:\n",
    "        \"\"\"Get the action greedily (without sampling)\n",
    "        Args:\n",
    "            x: input to network\n",
    "        Returns:\n",
    "            mean action\n",
    "        \"\"\"\n",
    "        x = self.shared_net(x.float())\n",
    "        batch_mean = self.mean_layer(x)\n",
    "        return self.action_scale * torch.tanh(batch_mean) + self.action_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd73d2-1dc4-48c8-99f0-464afd169b4e",
   "metadata": {},
   "source": [
    "## *Agent* and *SoftActorCriticAgent*\n",
    "\n",
    "Source: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/common/agents.py\n",
    "\n",
    "This part replaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dc3334-0ab6-4a7d-9bf9-a357da4379de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from pl_bolts.models.rl.common.agents import SoftActorCriticAgent\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e391cc3c-dde3-468d-bae1-42e4c6786143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Agent(ABC):\n",
    "    \"\"\"Basic agent that always returns 0.\"\"\"\n",
    "\n",
    "    def __init__(self, net: nn.Module):\n",
    "        self.net = net\n",
    "\n",
    "    def __call__(self, state: Tensor, device: str, *args, **kwargs) -> List[int]:\n",
    "        \"\"\"Using the given network, decide what action to carry.\n",
    "        Args:\n",
    "            state: current state of the environment\n",
    "            device: device used for current batch\n",
    "        Returns:\n",
    "            action\n",
    "        \"\"\"\n",
    "        return [0]\n",
    "\n",
    "class SoftActorCriticAgent(Agent):\n",
    "    \"\"\"Actor-Critic based agent that returns a continuous action based on the policy.\"\"\"\n",
    "\n",
    "    def __call__(self, states: Tensor, device: str) -> List[float]:\n",
    "        \"\"\"Takes in the current state and returns the action based on the agents policy.\n",
    "        Args:\n",
    "            states: current state of the environment\n",
    "            device: the device used for the current batch\n",
    "        Returns:\n",
    "            action defined by policy\n",
    "        \"\"\"\n",
    "        if not isinstance(states, list):\n",
    "            states = [states]\n",
    "\n",
    "        if not isinstance(states, Tensor):\n",
    "            states = torch.tensor(states, device=device)\n",
    "\n",
    "        dist = self.net(states)\n",
    "        actions = [a for a in dist.sample().cpu().numpy()]\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def get_action(self, states: Tensor, device: str) -> List[float]:\n",
    "        \"\"\"Get the action greedily (without sampling)\n",
    "        Args:\n",
    "            states: current state of the environment\n",
    "            device: the device used for the current batch\n",
    "        Returns:\n",
    "            action defined by policy\n",
    "        \"\"\"\n",
    "        if not isinstance(states, list):\n",
    "            states = [states]\n",
    "\n",
    "        if not isinstance(states, Tensor):\n",
    "            states = torch.tensor(states, device=device)\n",
    "\n",
    "        actions = [self.net.get_action(states).cpu().numpy()]\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b687f-c2fb-4357-8582-18cd1f373415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47901ca3-6388-404a-8c4c-1417e0e624bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Soft Actor-Critic Algo implementation:\n",
    "\n",
    "Source: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/sac_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a48c100-2bce-49fc-8b4e-92fa4b7931cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import Tensor, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a82edc-02c2-4ada-805d-5394d782dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: str,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.02,\n",
    "        eps_last_frame: int = 150000,\n",
    "        sync_rate: int = 1,\n",
    "        gamma: float = 0.99,\n",
    "        policy_learning_rate: float = 3e-4,\n",
    "        q_learning_rate: float = 3e-4,\n",
    "        target_alpha: float = 5e-3,\n",
    "        batch_size: int = 128,\n",
    "        replay_size: int = 1000000,\n",
    "        warm_start_size: int = 10000,\n",
    "        avg_reward_len: int = 100,\n",
    "        min_episode_reward: int = -21,\n",
    "        seed: int = 123,\n",
    "        batches_per_epoch: int = 10000,\n",
    "        n_steps: int = 1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Environment\n",
    "        self.env = gym.make(env)\n",
    "        self.test_env = gym.make(env)\n",
    "\n",
    "        self.obs_shape = self.env.observation_space.shape\n",
    "        self.n_actions = self.env.action_space.shape[0]\n",
    "\n",
    "        # Model Attributes\n",
    "        self.buffer = None\n",
    "        self.dataset = None\n",
    "\n",
    "        self.policy = None\n",
    "        self.q1 = None\n",
    "        self.q2 = None\n",
    "        self.target_q1 = None\n",
    "        self.target_q2 = None\n",
    "        self.build_networks()\n",
    "\n",
    "        self.agent = SoftActorCriticAgent(self.policy)\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Metrics\n",
    "        self.total_episode_steps = [0]\n",
    "        self.total_rewards = [0]\n",
    "        self.done_episodes = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "        # Average Rewards\n",
    "        self.avg_reward_len = avg_reward_len\n",
    "\n",
    "        for _ in range(avg_reward_len):\n",
    "            self.total_rewards.append(torch.tensor(min_episode_reward, device=self.device))\n",
    "\n",
    "        self.avg_rewards = float(np.mean(self.total_rewards[-self.avg_reward_len :]))\n",
    "\n",
    "        self.state = self.env.reset()\n",
    "\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def run_n_episodes(self, env, n_epsiodes: int = 1) -> List[int]:\n",
    "        \"\"\"Carries out N episodes of the environment with the current agent without exploration.\n",
    "\n",
    "        Args:\n",
    "            env: environment to use, either train environment or test environment\n",
    "            n_epsiodes: number of episodes to run\n",
    "        \"\"\"\n",
    "        total_rewards = []\n",
    "\n",
    "        for _ in range(n_epsiodes):\n",
    "            episode_state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = self.agent.get_action(episode_state, self.device)\n",
    "                next_state, reward, done, _ = env.step(action[0])\n",
    "                episode_state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards.append(episode_reward)\n",
    "\n",
    "        return total_rewards\n",
    "\n",
    "    def populate(self, warm_start: int) -> None:\n",
    "        \"\"\"Populates the buffer with initial experience.\"\"\"\n",
    "        if warm_start > 0:\n",
    "            self.state = self.env.reset()\n",
    "\n",
    "            for _ in range(warm_start):\n",
    "                action = self.agent(self.state, self.device)\n",
    "                next_state, reward, done, _ = self.env.step(action[0])\n",
    "                exp = Experience(state=self.state, action=action[0], reward=reward, done=done, new_state=next_state)\n",
    "                self.buffer.append(exp)\n",
    "                self.state = next_state\n",
    "\n",
    "                if done:\n",
    "                    self.state = self.env.reset()\n",
    "\n",
    "    def build_networks(self) -> None:\n",
    "        \"\"\"Initializes the SAC policy and q networks (with targets)\"\"\"\n",
    "        action_bias = torch.from_numpy((self.env.action_space.high + self.env.action_space.low) / 2)\n",
    "        action_scale = torch.from_numpy((self.env.action_space.high - self.env.action_space.low) / 2)\n",
    "        self.policy = ContinuousMLP(self.obs_shape, self.n_actions, action_bias=action_bias, action_scale=action_scale)\n",
    "\n",
    "        concat_shape = [self.obs_shape[0] + self.n_actions]\n",
    "        self.q1 = MLP(concat_shape, 1)\n",
    "        self.q2 = MLP(concat_shape, 1)\n",
    "        self.target_q1 = MLP(concat_shape, 1)\n",
    "        self.target_q2 = MLP(concat_shape, 1)\n",
    "        self.target_q1.load_state_dict(self.q1.state_dict())\n",
    "        self.target_q2.load_state_dict(self.q2.state_dict())\n",
    "\n",
    "    def soft_update_target(self, q_net, target_net):\n",
    "        \"\"\"Update the weights in target network using a weighted sum.\n",
    "\n",
    "        w_target := (1-a) * w_target + a * w_q\n",
    "\n",
    "        Args:\n",
    "            q_net: the critic (q) network\n",
    "            target_net: the target (q) network\n",
    "        \"\"\"\n",
    "        for q_param, target_param in zip(q_net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                (1.0 - self.hparams.target_alpha) * target_param.data + self.hparams.target_alpha * q_param\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "        Args:\n",
    "            x: environment state\n",
    "\n",
    "        Returns:\n",
    "            q values\n",
    "        \"\"\"\n",
    "        output = self.policy(x).sample()\n",
    "        return output\n",
    "\n",
    "    def train_batch(\n",
    "        self,\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Contains the logic for generating a new batch of data to be passed to the DataLoader.\n",
    "\n",
    "        Returns:\n",
    "            yields a Experience tuple containing the state, action, reward, done and next_state.\n",
    "        \"\"\"\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "\n",
    "        while True:\n",
    "            self.total_steps += 1\n",
    "            action = self.agent(self.state, self.device)\n",
    "\n",
    "            next_state, r, is_done, _ = self.env.step(action[0])\n",
    "\n",
    "            episode_reward += r\n",
    "            episode_steps += 1\n",
    "\n",
    "            exp = Experience(state=self.state, action=action[0], reward=r, done=is_done, new_state=next_state)\n",
    "\n",
    "            self.buffer.append(exp)\n",
    "            self.state = next_state\n",
    "\n",
    "            if is_done:\n",
    "                self.done_episodes += 1\n",
    "                self.total_rewards.append(episode_reward)\n",
    "                self.total_episode_steps.append(episode_steps)\n",
    "                self.avg_rewards = float(np.mean(self.total_rewards[-self.avg_reward_len :]))\n",
    "                self.state = self.env.reset()\n",
    "                episode_steps = 0\n",
    "                episode_reward = 0\n",
    "\n",
    "            states, actions, rewards, dones, new_states = self.buffer.sample(self.hparams.batch_size)\n",
    "\n",
    "            for idx, _ in enumerate(dones):\n",
    "                yield states[idx], actions[idx], rewards[idx], dones[idx], new_states[idx]\n",
    "\n",
    "            # Simulates epochs\n",
    "            if self.total_steps % self.hparams.batches_per_epoch == 0:\n",
    "                break\n",
    "\n",
    "    def loss(self, batch: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Calculates the loss for SAC which contains a total of 3 losses.\n",
    "\n",
    "        Args:\n",
    "            batch: a batch of states, actions, rewards, dones, and next states\n",
    "        \"\"\"\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        rewards = rewards.unsqueeze(-1)\n",
    "        dones = dones.float().unsqueeze(-1)\n",
    "\n",
    "        # actor\n",
    "        dist = self.policy(states)\n",
    "        new_actions, new_logprobs = dist.rsample_and_log_prob()\n",
    "        new_logprobs = new_logprobs.unsqueeze(-1)\n",
    "\n",
    "        new_states_actions = torch.cat((states, new_actions), 1)\n",
    "        new_q1_values = self.q1(new_states_actions)\n",
    "        new_q2_values = self.q2(new_states_actions)\n",
    "        new_qmin_values = torch.min(new_q1_values, new_q2_values)\n",
    "\n",
    "        policy_loss = (new_logprobs - new_qmin_values).mean()\n",
    "\n",
    "        # critic\n",
    "        states_actions = torch.cat((states, actions), 1)\n",
    "        q1_values = self.q1(states_actions)\n",
    "        q2_values = self.q2(states_actions)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_dist = self.policy(next_states)\n",
    "            new_next_actions, new_next_logprobs = next_dist.rsample_and_log_prob()\n",
    "            new_next_logprobs = new_next_logprobs.unsqueeze(-1)\n",
    "\n",
    "            new_next_states_actions = torch.cat((next_states, new_next_actions), 1)\n",
    "            next_q1_values = self.target_q1(new_next_states_actions)\n",
    "            next_q2_values = self.target_q2(new_next_states_actions)\n",
    "            next_qmin_values = torch.min(next_q1_values, next_q2_values) - new_next_logprobs\n",
    "            target_values = rewards + (1.0 - dones) * self.hparams.gamma * next_qmin_values\n",
    "\n",
    "        q1_loss = F.mse_loss(q1_values, target_values)\n",
    "        q2_loss = F.mse_loss(q2_values, target_values)\n",
    "\n",
    "        return policy_loss, q1_loss, q2_loss\n",
    "\n",
    "    def training_step(self, batch: Tuple[Tensor, Tensor], _):\n",
    "        \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "        based on the minibatch recieved.\n",
    "\n",
    "        Args:\n",
    "            batch: current mini batch of replay data\n",
    "            _: batch number, not used\n",
    "        \"\"\"\n",
    "        policy_optim, q1_optim, q2_optim = self.optimizers()\n",
    "        policy_loss, q1_loss, q2_loss = self.loss(batch)\n",
    "\n",
    "        policy_optim.zero_grad()\n",
    "        self.manual_backward(policy_loss)\n",
    "        policy_optim.step()\n",
    "\n",
    "        q1_optim.zero_grad()\n",
    "        self.manual_backward(q1_loss)\n",
    "        q1_optim.step()\n",
    "\n",
    "        q2_optim.zero_grad()\n",
    "        self.manual_backward(q2_loss)\n",
    "        q2_optim.step()\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.soft_update_target(self.q1, self.target_q1)\n",
    "            self.soft_update_target(self.q2, self.target_q2)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"total_reward\": self.total_rewards[-1],\n",
    "                \"avg_reward\": self.avg_rewards,\n",
    "                \"policy_loss\": policy_loss,\n",
    "                \"q1_loss\": q1_loss,\n",
    "                \"q2_loss\": q2_loss,\n",
    "                \"episodes\": self.done_episodes,\n",
    "                \"episode_steps\": self.total_episode_steps[-1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def test_step(self, *args, **kwargs) -> Dict[str, Tensor]:\n",
    "        \"\"\"Evaluate the agent for 10 episodes.\"\"\"\n",
    "        test_reward = self.run_n_episodes(self.test_env, 1)\n",
    "        avg_reward = sum(test_reward) / len(test_reward)\n",
    "        return {\"test_reward\": avg_reward}\n",
    "\n",
    "    def test_epoch_end(self, outputs) -> Dict[str, Tensor]:\n",
    "        \"\"\"Log the avg of the test results.\"\"\"\n",
    "        rewards = [x[\"test_reward\"] for x in outputs]\n",
    "        avg_reward = sum(rewards) / len(rewards)\n",
    "        self.log(\"avg_test_reward\", avg_reward)\n",
    "        return {\"avg_test_reward\": avg_reward}\n",
    "\n",
    "    def _dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        self.buffer = MultiStepBuffer(self.hparams.replay_size, self.hparams.n_steps)\n",
    "        self.populate(self.hparams.warm_start_size)\n",
    "\n",
    "        self.dataset = ExperienceSourceDataset(self.train_batch)\n",
    "        return DataLoader(dataset=self.dataset, batch_size=self.hparams.batch_size)\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader.\"\"\"\n",
    "        return self._dataloader()\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get test loader.\"\"\"\n",
    "        return self._dataloader()\n",
    "\n",
    "    def configure_optimizers(self) -> Tuple[Optimizer]:\n",
    "        \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "        policy_optim = optim.Adam(self.policy.parameters(), self.hparams.policy_learning_rate)\n",
    "        q1_optim = optim.Adam(self.q1.parameters(), self.hparams.q_learning_rate)\n",
    "        q2_optim = optim.Adam(self.q2.parameters(), self.hparams.q_learning_rate)\n",
    "        return policy_optim, q1_optim, q2_optim\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(\n",
    "        arg_parser: argparse.ArgumentParser,\n",
    "    ) -> argparse.ArgumentParser:\n",
    "        \"\"\"Adds arguments for DQN model.\n",
    "\n",
    "        Note:\n",
    "            These params are fine tuned for Pong env.\n",
    "\n",
    "        Args:\n",
    "            arg_parser: parent parser\n",
    "        \"\"\"\n",
    "        arg_parser.add_argument(\n",
    "            \"--sync_rate\",\n",
    "            type=int,\n",
    "            default=1,\n",
    "            help=\"how many frames do we update the target network\",\n",
    "        )\n",
    "        arg_parser.add_argument(\n",
    "            \"--replay_size\",\n",
    "            type=int,\n",
    "            default=1000000,\n",
    "            help=\"capacity of the replay buffer\",\n",
    "        )\n",
    "        arg_parser.add_argument(\n",
    "            \"--warm_start_size\",\n",
    "            type=int,\n",
    "            default=10000,\n",
    "            help=\"how many samples do we use to fill our buffer at the start of training\",\n",
    "        )\n",
    "        arg_parser.add_argument(\"--batches_per_epoch\", type=int, default=10000, help=\"number of batches in an epoch\")\n",
    "        arg_parser.add_argument(\"--batch_size\", type=int, default=128, help=\"size of the batches\")\n",
    "        arg_parser.add_argument(\"--policy_lr\", type=float, default=3e-4, help=\"policy learning rate\")\n",
    "        arg_parser.add_argument(\"--q_lr\", type=float, default=3e-4, help=\"q learning rate\")\n",
    "        arg_parser.add_argument(\"--env\", type=str, required=True, help=\"gym environment tag\")\n",
    "        arg_parser.add_argument(\"--gamma\", type=float, default=0.99, help=\"discount factor\")\n",
    "\n",
    "        arg_parser.add_argument(\n",
    "            \"--avg_reward_len\",\n",
    "            type=int,\n",
    "            default=100,\n",
    "            help=\"how many episodes to include in avg reward\",\n",
    "        )\n",
    "        arg_parser.add_argument(\n",
    "            \"--n_steps\",\n",
    "            type=int,\n",
    "            default=1,\n",
    "            help=\"how many frames do we update the target network\",\n",
    "        )\n",
    "\n",
    "        return arg_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e9ce1-b163-4ab7-bcb6-24730e836d99",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training:\n",
    "\n",
    "Source: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/sac_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1662b115-11cd-4d77-afa3-943c3ac5c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli_main():\n",
    "    parser = argparse.ArgumentParser(add_help=False)\n",
    "\n",
    "    # trainer args\n",
    "    parser = Trainer.add_argparse_args(parser)\n",
    "\n",
    "    # model args\n",
    "    parser = SAC.add_model_specific_args(parser)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model = SAC(**args.__dict__)\n",
    "\n",
    "    # save checkpoints based on avg_reward\n",
    "    checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"avg_reward\", mode=\"max\", verbose=True)\n",
    "\n",
    "    seed_everything(123)\n",
    "    trainer = Trainer.from_argparse_args(args, deterministic=True, callbacks=checkpoint_callback)\n",
    "\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582e9cc1-cdc8-4aa8-967b-d8ecd1f55dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b23fd9-b8d2-4a03-bc21-91ab2c490ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import Tensor, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8789994c-876d-4bc3-80f2-c95e7855fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_lightning_bolts_template.pl_bolts_sac import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff53ac9-5d7e-4bec-b450-94e6dd5e0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_model = SAC(env = \"HalfCheetah-v4\",\n",
    "                eps_start = 1.0,\n",
    "                eps_end = 0.02,\n",
    "                eps_last_frame = 150000,\n",
    "                sync_rate = 5,\n",
    "                gamma = 0.98,\n",
    "                policy_learning_rate = 3e-4,\n",
    "                q_learning_rate = 3e-4,\n",
    "                target_alpha = 5e-3,\n",
    "                batch_size = 128,\n",
    "                replay_size = 1000000,\n",
    "                warm_start_size = 1000,\n",
    "                avg_reward_len = 100,\n",
    "                seed = 101,\n",
    "                batches_per_epoch = 1000,\n",
    "                n_steps = 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8c102-e046-44d0-a4cb-fde905b7786e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1629f24-774a-4021-b9aa-63452540eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_model = SAC(env = \"HalfCheetah-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78e94c-fbaf-42db-b0f8-65ff076219f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba6b065-47aa-4c0b-94ee-7b1da2ab7356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 101\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# save checkpoints based on avg_reward\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"avg_reward\", mode=\"max\", verbose=True)\n",
    "seed_everything(101)\n",
    "trainer = Trainer(accelerator=\"gpu\", max_steps=100000,  callbacks = checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbcb03-1570-4a4b-9edb-338f61ad9cd1",
   "metadata": {},
   "source": [
    "Moment de vérité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50ca7939-2f43-4171-885a-fcbfd9d8aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA RTX A4000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | policy    | ContinuousMLP | 20.4 K\n",
      "1 | q1        | MLP           | 3.2 K \n",
      "2 | q2        | MLP           | 3.2 K \n",
      "3 | target_q1 | MLP           | 3.2 K \n",
      "4 | target_q2 | MLP           | 3.2 K \n",
      "--------------------------------------------\n",
      "33.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "33.2 K    Total params\n",
      "0.133     Total estimated model params size (MB)\n",
      "/notebooks/Deep_Forecasting/RL_lightning_bolts_template/pl_bolts_agents.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  states = torch.tensor(states, device=device)\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e6ce2bf76af4f9cacec6188ee7af43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:234: UserWarning: You called `self.log('total_reward', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:234: UserWarning: You called `self.log('episodes', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:234: UserWarning: You called `self.log('episode_steps', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "  warning_cache.warn(\n",
      "Epoch 0, global step 30000: 'avg_reward' reached -21.00000 (best -21.00000), saving model to '/notebooks/Deep_Forecasting/lightning_logs/version_7/checkpoints/epoch=0-step=30000.ckpt' as top 1\n",
      "Epoch 1, global step 60000: 'avg_reward' was not in top 1\n",
      "Epoch 2, global step 90000: 'avg_reward' was not in top 1\n",
      "Epoch 3, global step 100002: 'avg_reward' was not in top 1\n",
      "`Trainer.fit` stopped: `max_steps=100000` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(sac_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b36e06-4ffb-47fc-a003-1c4e6b9f7337",
   "metadata": {},
   "source": [
    "Follow the instructions here to get the Tensorboard link:\n",
    "https://docs.paperspace.com/gradient/notebooks/tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fc3f48-6450-41af-aac4-5cbf60139f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sac_model.state_dict(), \"./sac_lightning_v7_gym0262_230621\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fdcd1a0-d4c5-4f67-b2fa-6854895084ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "test_model = SAC(env = \"HalfCheetah-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "997f0cf5-9e6a-41b3-99a9-6000d0d7476d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.load_state_dict(torch.load(\"./sac_lightning_v6_230621\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdf31bc-7a6f-4808-813f-df0272240b3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d22dd30-b2c8-4eea-8ac9-635ba53737bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.0 has a number of critical issues with `gym.make` such that the `reset` and `step` functions are called before returning the environment. It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch import Tensor, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8648cc-1848-4bf6-b35d-0c1c991a45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_lightning_bolts_template.pl_bolts_sac import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc0938b-d3a4-416d-a9fc-7a77efc6f9c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym import wrappers\n",
    "from pyvirtualdisplay import Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "865ba64e-cd5a-4d9f-9a10-7f5fe93c3c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = SAC(env = \"HalfCheetah-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da873ac8-1968-4a87-be98-a403f190a953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.load_state_dict(torch.load(\"./sac_lightning_v7_gym0262_230621\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d365ad3b-3d9f-40b9-abad-786823dd22b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SAC(\n",
       "  (policy): ContinuousMLP(\n",
       "    (shared_net): Sequential(\n",
       "      (0): Linear(in_features=17, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (mean_layer): Linear(in_features=128, out_features=6, bias=True)\n",
       "    (logstd_layer): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       "  (q1): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=23, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q2): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=23, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (target_q1): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=23, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (target_q2): MLP(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=23, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e8a4230-b341-41a0-9766-0d32338c6a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6a0379-a6b2-45d0-8919-553c60186bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b7ee096-0852-4bca-b25c-3ad2e9a7cff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cb68c08-dc4f-4da9-bcbf-0bfe4540e330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd3fa939-c322-4ab5-887b-e63f0b1f155a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/Deep_Forecasting/RL_lightning_bolts_template/pl_bolts_agents.py:66\u001b[0m, in \u001b[0;36mSoftActorCriticAgent.get_action\u001b[0;34m(self, states, device)\u001b[0m\n\u001b[1;32m     63\u001b[0m     states \u001b[38;5;241m=\u001b[39m [states]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(states, Tensor):\n\u001b[0;32m---> 66\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m actions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mget_action(states)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actions\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "test_model.agent.get_action(torch.Tensor(s), test_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45a83c3a-fa40-4ecb-a321-beefe9c61f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5478,  0.8676, -0.7957,  0.3067, -0.3930,  0.0500], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(torch.Tensor(s).to(test_model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f2297-c8c1-4bfd-bd3c-17b31e1f5b26",
   "metadata": {},
   "source": [
    "### gym 0.26.2\n",
    "\n",
    "In this version, you need to include *render_mode* in the env instantiation to be able to record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9b246e-f1bd-49ba-ae9c-d1392a8fabf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger for wrapper.RecordVideo() object\n",
    "def epsd_trigger(episode_id: int) -> bool:\n",
    "    '''\n",
    "        Records all episodes\n",
    "    '''\n",
    "    if episode_id < 10:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a08ecc-d259-45bc-a462-a3eec98e4423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f5176972d00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e940768e-4070-466d-81cd-172ffb52187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v4\", render_mode = \"rgb_array\")\n",
    "env = wrappers.RecordVideo(env = env, \n",
    "                           video_folder=\"vids/\",\n",
    "                           name_prefix=\"SAC_pl_gym0262_2306212113\",\n",
    "                           episode_trigger = epsd_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d45d1c07-3c99-48bd-ada9-42e05c974cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-0.mp4.\n",
      "Moviepy - Writing video /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-0.mp4\n",
      "Moviepy - Building video /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-1.mp4.\n",
      "Moviepy - Writing video /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-1.mp4\n",
      "Moviepy - Building video /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-2.mp4.\n",
      "Moviepy - Writing video /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /notebooks/Deep_Forecasting/vids/SAC_pl_gym0262_2306212113-episode-2.mp4\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state, _ = env.reset()\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done and step<5001:\n",
    "        step += 1\n",
    "        #env.render()\n",
    "        # Get action\n",
    "        with torch.no_grad():\n",
    "            # Convert to torch tensors\n",
    "            state_ = torch.FloatTensor(np.array(state)).to(test_model.device)\n",
    "            # Get actions and UPolicy output\n",
    "            #action_ = actor(control, state_, t_)\n",
    "            action_ = test_model(state_)\n",
    "            # Get np arrays\n",
    "            action = action_.cpu().detach().numpy()\n",
    "        \n",
    "        observation, reward, done, trunc, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode: {0},\\tSteps: {1},\\tscore: {2}\"\n",
    "                  .format(episode, step, total_reward)\n",
    "            )\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677eacc-87a9-45c1-8ad8-6a9c4932abad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff952426-aa3f-4416-ad52-d80bc1c0ad42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148765f7-cabf-4220-95c6-f07eafb8fe11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b6041fe-d10f-4f2f-accb-3e423e3ee288",
   "metadata": {},
   "source": [
    "### gym v0.24.0\n",
    "\n",
    "The code below does not work with gym 0.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09bcd022-a595-4b24-bb24-8b547ec0f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger for wrapper.RecordVideo() object\n",
    "def epsd_trigger(episode_id: int) -> bool:\n",
    "    '''\n",
    "        Records all episodes\n",
    "    '''\n",
    "    if episode_id < 10:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f88508-b2aa-4eef-af3e-a4d169e0b57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7fe2a928b340>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3726ca11-9fba-4975-b269-7e3a86327105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/gym/utils/env_checker.py:144: UserWarning: \u001b[33mWARN: Agent's minimum observation space value is -infinity. This is probably too low.\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/gym/utils/env_checker.py:148: UserWarning: \u001b[33mWARN: Agent's maxmimum observation space value is infinity. This is probably too high\u001b[0m\n",
      "  logger.warn(\n",
      "/usr/local/lib/python3.9/dist-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /notebooks/Deep_Forecasting/vids folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "env = wrappers.RecordVideo(env = env, \n",
    "                           video_folder=\"vids/\",\n",
    "                           name_prefix=\"SAC_pl_2306211908\",\n",
    "                           episode_trigger = epsd_trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e115e24a-8e1b-47a6-b560-957e4cbdc449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0,\tSteps: 1000,\tscore: -338.7753927132449\n",
      "Episode: 1,\tSteps: 1000,\tscore: -155.0876923827959\n",
      "Episode: 2,\tSteps: 1000,\tscore: -137.1122364004564\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done and step<5001:\n",
    "        step += 1\n",
    "        env.render()\n",
    "        # Get action\n",
    "        with torch.no_grad():\n",
    "            # Convert to torch tensors\n",
    "            state_ = torch.FloatTensor(np.array(state)).to(test_model.device)\n",
    "            # Get actions and UPolicy output\n",
    "            #action_ = actor(control, state_, t_)\n",
    "            action_ = test_model(state_)\n",
    "            # Get np arrays\n",
    "            action = action_.cpu().detach().numpy()\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Episode: {0},\\tSteps: {1},\\tscore: {2}\"\n",
    "                  .format(episode, step, total_reward)\n",
    "            )\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ddafca-3462-43f7-9cca-a3274eb58c49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a66d1-dc46-42d6-89fb-ec1f6747b011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ae5c3-f5c7-4d18-9f4c-a3c3f1e0fe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd9b73-8ee8-4fa0-a3df-08a328b0fd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf47b5-80c6-4b74-875d-10d13e754bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2110ead-396c-49f3-98a5-9cdf8ce46df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c44457-8b96-493e-8a6f-beae01b6e6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
