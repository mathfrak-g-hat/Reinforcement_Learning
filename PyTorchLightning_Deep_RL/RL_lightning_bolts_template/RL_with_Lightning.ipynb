{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2813d35-2360-4dd2-957b-7e4e281bed61",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with PyTorch Lightning\n",
    "### AJ Zerouali, 2023/06/19\n",
    "\n",
    "Goals of this notebook:\n",
    "- Get familiar with *pytorch-lightning*'s functionalities.\n",
    "- Implement an RL algorithm using *pytorch-lightning*.\n",
    "\n",
    "\n",
    "References:\n",
    "- Tutorial on DQN (old): https://www.pytorchlightning.ai/blog/en-lightning-reinforcement-learning-building-a-dqn-with-pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd15afd-fd9e-4fbe-b18b-dd82666f8ee5",
   "metadata": {},
   "source": [
    "## 1) Deep Q-learning on cartpole\n",
    "\n",
    "We follow the tutorial here:\n",
    "\n",
    "https://www.pytorchlightning.ai/blog/en-lightning-reinforcement-learning-building-a-dqn-with-pytorch-lightning\n",
    "\n",
    "GitHub:\n",
    "\n",
    "https://github.com/Lightning-AI/lightning/blob/1.9.5/examples/pl_domain_templates/reinforce_learn_Qnet.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b052a81-88b4-4384-a632-96dac36581e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import collections\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import IterableDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "# from pytorch_lightning import cli_lightning_logo, LightningModule, seed_everything, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593a4d56-49bf-4fad-9f6f-dacc8cde2f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import deque, namedtuple, OrderedDict\n",
    "from typing import Iterator, List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f992971-115f-4c34-b7e1-1e97d0bd7a98",
   "metadata": {},
   "source": [
    "### 1.a - Usual RL classes\n",
    "\n",
    "- Replay buffer.\n",
    "- Main neural net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47fa967d-04ea-46d3-a1eb-d1b8ca01920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural net\n",
    "class DQN(nn.Module):\n",
    "    '''\n",
    "        :param observation_space_dim:\n",
    "        :param n_actions: No. of actions in discrete action space\n",
    "        :param n_hidden: No. of units in hidden layer\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 observation_space_dim: int,\n",
    "                 n_actions: int,\n",
    "                 n_hidden: int,\n",
    "                ):\n",
    "        \n",
    "        # Init.\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(observation_space_dim, n_hidden),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(n_hidden, n_actions)\n",
    "                                ),\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee14283-38a4-43af-8d1e-8e8349c89d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer for storing past experiences allowing the agent to learn from them\n",
    "    Args:\n",
    "        capacity: size of the buffer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size: int) -> None:\n",
    "        self.buffer = collections.deque(maxlen=buffer_size)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, state, action, reward, done, state_next) -> None:\n",
    "        \"\"\"\n",
    "        Add experience to the buffer\n",
    "        Args:\n",
    "            experience: tuple (state, action, reward, done, new_state)\n",
    "        \"\"\"\n",
    "        transition = (state, action, reward, done, state_next)\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "\n",
    "        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n",
    "                np.array(dones, dtype=np.bool), np.array(next_states))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089e7c5-f000-4ab0-9815-aef6ea13cb7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Test this replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84849966-1643-43f9-8ddb-d9c6b5977715",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# old code\n",
    "'''\n",
    "        ###############################\n",
    "        ##### REPLAY BUFFER CLASS #####\n",
    "        ###############################\n",
    "'''\n",
    "class replayBuffer(object):\n",
    "    def __init__(self, buffer_size, name_buffer=''):\n",
    "        self.name_buffer = name_buffer\n",
    "        self.buffer_size = buffer_size  #choose buffer size\n",
    "        self.num_exp = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def add(self, s, a, r, s2, t):\n",
    "        experience=(s, a, r, s2, t)\n",
    "        if self.num_exp < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_exp +=1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def count(self):\n",
    "        return self.num_exp\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.num_exp < batch_size:\n",
    "            batch = random.sample(self.buffer, self.num_exp)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s, a, r, s2, t = map(np.stack, zip(*batch))\n",
    "\n",
    "        return s, a, r, s2, t\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_exp=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eff3e127-10cb-495b-8683-1faa6963ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "replay_buffer = ReplayBuffer(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d75ddbac-0e60-4b2c-83d3-c85bab1fb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill buffer with random numbers\n",
    "action_space_dim = 6\n",
    "observation_space_dim = 17\n",
    "n_transitions = 200\n",
    "# s,a ,r, done, s_\n",
    "s = np.random.uniform(size = (observation_space_dim,))\n",
    "for i in range(n_transitions):\n",
    "    # Generate\n",
    "    a = np.random.normal(size = (action_space_dim,))\n",
    "    r = np.random.uniform(low=-10.0, high=10.0)\n",
    "    done = (i == 200-1)\n",
    "    s_ = np.random.uniform(size = (observation_space_dim,))\n",
    "    # Store in buffer\n",
    "    replay_buffer.append(s, a, r, done, s_)\n",
    "    # Update s\n",
    "    s = s_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96805ac5-36dd-4421-ad1b-1f275733d965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4732/541907074.py:29: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.array(dones, dtype=np.bool), np.array(next_states))\n"
     ]
    }
   ],
   "source": [
    "# Get a batch:\n",
    "s_batch, a_batch, r_batch, done_batch, s_next_batch = replay_buffer.sample(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2fec08-8cf7-4550-83e0-1ba0954c6031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb9af34a-b59c-4b85-beab-91e32cfdbb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64549436, 0.25604908, 0.13039131, 0.75992778, 0.49294839,\n",
       "       0.02917197, 0.45112308, 0.66362518, 0.96089943, 0.20814165,\n",
       "       0.23920129, 0.89725009, 0.10262505, 0.35812957, 0.43796378,\n",
       "       0.07583482, 0.02144812])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_batch[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c836cd-09bb-4084-bbb7-8a201adddaf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "371deaff-a215-410b-95b6-dfe045eb0304",
   "metadata": {},
   "source": [
    "**Comment:** The way they implement the agent is different from what we're used to (e.g. stable_baselines3). Notably:\n",
    "- The network is not one of the agent's attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3eead-eeba-40a1-8d5a-3d90d5a2c15c",
   "metadata": {},
   "source": [
    "### 2) Dataset\n",
    "\n",
    "Lightning forces us to work with pytorch's dataset class. Obviously, this is to enventually instantiate a dataloader for the replay buffer, and the dataset class we will use will have a *ReplayBuffer* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "483117b5-0aaa-437d-88cc-6a01ed7a01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable Dataset containing the ReplayBuffer\n",
    "    which will be updated with new experiences during training\n",
    "    Args:\n",
    "        buffer: replay buffer\n",
    "        sample_size: number of experiences to sample at a time\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, replay_buffer: ReplayBuffer, batch_size: int = 128) -> None:\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.batch_size = batch_size # What does this do?\n",
    "\n",
    "    def __iter__(self) -> Tuple:\n",
    "        states, actions, rewards, dones, new_states = self.replay_buffer.sample(self.batch_size)\n",
    "        for i in range(len(dones)): # AJZ: check that this works properly\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c0eebd-c437-443f-b630-fca76d76ff96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Test RLDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8bbe671d-d81f-45de-b9e9-da5f7d873e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate replay buffer\n",
    "replay_buffer = ReplayBuffer(1024)\n",
    "\n",
    "# Fill buffer with random numbers\n",
    "action_space_dim = 6\n",
    "observation_space_dim = 17\n",
    "n_transitions = 200\n",
    "# s,a ,r, done, s_\n",
    "s = np.random.uniform(size = (observation_space_dim,))\n",
    "for i in range(n_transitions):\n",
    "    # Generate\n",
    "    a = np.random.normal(size = (action_space_dim,))\n",
    "    r = np.random.uniform(low=-10.0, high=10.0)\n",
    "    done = (i == 200-1)\n",
    "    s_ = np.random.uniform(size = (observation_space_dim,))\n",
    "    # Store in buffer\n",
    "    replay_buffer.append(s, a, r, done, s_)\n",
    "    # Update s\n",
    "    s = s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b0cfe48-8813-4843-a820-cf7fc076bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate RLDataset and dataloader\n",
    "DQN_dataset = RLDataset(replay_buffer= replay_buffer, batch_size=64)\n",
    "DQN_dataloader = DataLoader(dataset = DQN_dataset,\n",
    "                            batch_size = 64,\n",
    "                            sampler = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d89efe2-0031-43dd-a050-7c45e30d0dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4732/541907074.py:29: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  np.array(dones, dtype=np.bool), np.array(next_states))\n"
     ]
    }
   ],
   "source": [
    "s, a, r, done, s_ = next(iter(DQN_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2fa27db-6015-4b5e-a621-181659d5540e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s.shape = torch.Size([64, 17])\n",
      "a.shape = torch.Size([64, 6])\n",
      "r.shape = torch.Size([64])\n",
      "type(done) = <class 'torch.Tensor'>\n",
      "s_.shape = torch.Size([64, 17])\n"
     ]
    }
   ],
   "source": [
    "print(f\"s.shape = {s.shape}\")\n",
    "print(f\"a.shape = {a.shape}\")\n",
    "print(f\"r.shape = {r.shape}\")\n",
    "print(f\"type(done) = {type(done)}\")\n",
    "print(f\"s_.shape = {s_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57ae0a85-d228-445a-8e8f-af55f61e54f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1570185-0284-4e95-9e6b-6473cb5e4adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1478f8-eea6-470d-8e5f-bf2d429202d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4596b10a-af5a-45de-98b0-a211fbc8ff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949471d-05d5-494c-8577-272e74d971bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c3f5c6f-263b-40c4-9bd5-95c749cceace",
   "metadata": {},
   "source": [
    "### Lightning module\n",
    "\n",
    "The main part of the implementation, where we construct a subclass of *pl.LightningModule*. A minimal *lightningModule* has the following elements:\n",
    "\n",
    "#### 1) The constructor:\n",
    "\n",
    "In the constructor we initialize the environment, the DQN and its target, and the replay buffer.\n",
    "\n",
    "\n",
    "#### 2) The *forward()* method:\n",
    "\n",
    "For the case of deep Q-learning, the *forward()* method of the *LightningModule* will simply wrap that of the DQN module.\n",
    "\n",
    "#### 3) Defining the loss function - *dqn_mse_loss()*:\n",
    "\n",
    "For convenience, we will add a taget network. Our loss will then be the MSE of the state-action value function approximated by both the target and main DQN. We add a method that outputs the loss in the *LightningModule* class.\n",
    "\n",
    "#### 4) The *configure_optimizers()* method:\n",
    "\n",
    "This function will just assign the DQN parameters to an Adam optimizer. The optimizer is not implemented as an attribute however.\n",
    "\n",
    "#### 5) The *train_dataloader()* method:\n",
    "\n",
    "One of the key parts of *pytorch-lightning*. The name is a little misleading, because this method will not train the dataloader, it will rather construct the dataloader used for training.\n",
    "\n",
    "#### 6) *train_step()*\n",
    "\n",
    "This key method of the *LightningModule* class contains all the instructions of a training iteration. **More comments**\n",
    "\n",
    "What's peculiar:\n",
    "- The deep Q-learning algorithm is not written once and for all in one *train()* method.\n",
    "- The implementation of the tutorial relies on a *play_step()* method of the agent.\n",
    "- The *grad_zero()* and *step()* instructions are not explicitly called in what we implement. I think they're left in the trainer object training loop, and this is not discussed in this DQN tutorial. For an alternative approach, see https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/sac_model.py#L28-L384.\n",
    "- The SAC implementation also relies on an agent class: https://github.com/Lightning-Universe/lightning-bolts/blob/0.5.0/pl_bolts/models/rl/common/agents.py. This agent class is called in the algorithm implementation for 2 things: (1) In the constructor for initialization; (2) In the training and deployment methods, to call only the *get_action()* method of the agent. I think *get_action()* can simply be a method of the *LightningModule*.\n",
    "- The last two pages are from the Lightning-Bolts repo, a separate extra to Lightning.\n",
    "- For important parts of the *LightningModule* implementation: https://lightning.ai/docs/pytorch/stable/common/lightning_module.html.\n",
    "\n",
    "I think it's better to implement the agent directly as a *LightningModule*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b8d9f-a1c9-4d7d-b836-eabc5e17d42e",
   "metadata": {},
   "source": [
    "**To do list:**\n",
    "\n",
    "    class SAC(LightningModule):\n",
    "        def __init__(\n",
    "            self,\n",
    "            env: str,\n",
    "            eps_start: float = 1.0,\n",
    "            eps_end: float = 0.02,\n",
    "            eps_last_frame: int = 150000,\n",
    "            sync_rate: int = 1,\n",
    "            gamma: float = 0.99,\n",
    "            policy_learning_rate: float = 3e-4,\n",
    "            q_learning_rate: float = 3e-4,\n",
    "            target_alpha: float = 5e-3,\n",
    "            batch_size: int = 128,\n",
    "            replay_size: int = 1000000,\n",
    "            warm_start_size: int = 10000,\n",
    "            avg_reward_len: int = 100,\n",
    "            min_episode_reward: int = -21,\n",
    "            seed: int = 123,\n",
    "            batches_per_epoch: int = 10000,\n",
    "            n_steps: int = 1,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__()\n",
    "            #### ADD RL ATTRIBUTES HERE ####\n",
    "\n",
    "        def run_n_episodes(self, env, n_epsiodes: int = 1) -> List[int]:\n",
    "            \"\"\"Carries out N episodes of the environment with the current agent without exploration.\n",
    "            Args:\n",
    "                env: environment to use, either train environment or test environment\n",
    "                n_epsiodes: number of episodes to run\n",
    "            \"\"\"\n",
    "\n",
    "        def populate(self, warm_start: int) -> None:\n",
    "            \"\"\"Populates the buffer with initial experience.\"\"\"\n",
    "\n",
    "        def build_networks(self) -> None:\n",
    "            \"\"\"Initializes the SAC policy and q networks (with targets)\"\"\"\n",
    "\n",
    "        def soft_update_target(self, q_net, target_net):\n",
    "            \"\"\"Update the weights in target network using a weighted sum.\n",
    "\n",
    "        def forward(self, x: Tensor) -> Tensor:\n",
    "            \"\"\"Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "            Args:\n",
    "                x: environment state\n",
    "            Returns:\n",
    "                q values\n",
    "            \"\"\"\n",
    "            output = self.policy(x).sample()\n",
    "            return output\n",
    "\n",
    "        def train_batch(\n",
    "            self,\n",
    "        ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
    "            \"\"\"Contains the logic for generating a new batch of data to be passed to the DataLoader.\n",
    "            Returns:\n",
    "                yields a Experience tuple containing the state, action, reward, done and next_state.\n",
    "            \"\"\"\n",
    "\n",
    "        def loss(self, batch: Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "            \"\"\"Calculates the loss for SAC which contains a total of 3 losses.\n",
    "            Args:\n",
    "                batch: a batch of states, actions, rewards, dones, and next states\n",
    "            \"\"\"\n",
    "        def training_step(self, batch: Tuple[Tensor, Tensor], _):\n",
    "            \"\"\"Carries out a single step through the environment to update the replay buffer. Then calculates loss\n",
    "            based on the minibatch recieved.\n",
    "            Args:\n",
    "                batch: current mini batch of replay data\n",
    "                _: batch number, not used\n",
    "            \"\"\"\n",
    "\n",
    "        def test_step(self, *args, **kwargs) -> Dict[str, Tensor]:\n",
    "            \"\"\"Evaluate the agent for 10 episodes.\"\"\"\n",
    "\n",
    "        def test_epoch_end(self, outputs) -> Dict[str, Tensor]:\n",
    "            \"\"\"Log the avg of the test results.\"\"\"\n",
    "\n",
    "        def _dataloader(self) -> DataLoader:\n",
    "            \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "\n",
    "        def train_dataloader(self) -> DataLoader:\n",
    "            \"\"\"Get train loader.\"\"\"\n",
    "            return self._dataloader()\n",
    "\n",
    "        def test_dataloader(self) -> DataLoader:\n",
    "            \"\"\"Get test loader.\"\"\"\n",
    "            return self._dataloader()\n",
    "\n",
    "        def configure_optimizers(self) -> Tuple[Optimizer]:\n",
    "            \"\"\"Initialize Adam optimizer.\"\"\"\n",
    "            policy_optim = optim.Adam(self.policy.parameters(), self.hparams.policy_learning_rate)\n",
    "            q1_optim = optim.Adam(self.q1.parameters(), self.hparams.q_learning_rate)\n",
    "            q2_optim = optim.Adam(self.q2.parameters(), self.hparams.q_learning_rate)\n",
    "            return policy_optim, q1_optim, q2_optim\n",
    "\n",
    "        @staticmethod\n",
    "        def add_model_specific_args(\n",
    "            arg_parser: argparse.ArgumentParser,\n",
    "        ) -> argparse.ArgumentParser:\n",
    "            \"\"\"Adds arguments for DQN model.\n",
    "            Note:\n",
    "                These params are fine tuned for Pong env.\n",
    "            Args:\n",
    "                arg_parser: parent parser\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade9898-c467-42ad-b3bf-29774775bf26",
   "metadata": {},
   "source": [
    "Now for the agent class, Lightning has the following abstract class:\n",
    "\n",
    "    class Agent(ABC):\n",
    "        \"\"\"Basic agent that always returns 0.\"\"\"\n",
    "\n",
    "        def __init__(self, net: nn.Module):\n",
    "            self.net = net\n",
    "\n",
    "        def __call__(self, state: Tensor, device: str, *args, **kwargs) -> List[int]:\n",
    "            \"\"\"Using the given network, decide what action to carry.\n",
    "            Args:\n",
    "                state: current state of the environment\n",
    "                device: device used for current batch\n",
    "            Returns:\n",
    "                action\n",
    "            \"\"\"\n",
    "            return [0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd79ba-f0d5-4547-bc7f-697cf4e4ae6b",
   "metadata": {},
   "source": [
    "#### Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b581d-5539-4b73-b44c-249a42671ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea759f8-b0da-4100-a517-8586358ece61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef3288-272d-48c9-bc3e-dc11237fefd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05e15fb6-b224-42d5-9448-c38d4eee85de",
   "metadata": {},
   "source": [
    "#### Algorithm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f869b6-60ac-4e18-a51e-170188f48aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(pl.LightningModule):\n",
    "    \n",
    "    # Constructor (CRUCIAL)\n",
    "    def __init__(self,\n",
    "                 train_env: gym.Env,\n",
    "                 gamma: float = 0.99,\n",
    "                 dqn_lr: float = 1e-4,\n",
    "                 batch_size: int = 128,\n",
    "                 buffer_size: int = 1000000,\n",
    "                 eps_start: float = 1.0,\n",
    "                 eps_end: float = 0.02,\n",
    "                 eps_last_frame: int = 150000,\n",
    "                 sync_rate: int = 1,\n",
    "                 target_alpha: float = 5e-3,\n",
    "                 n_warmup_steps: int = 10000,\n",
    "                 avg_reward_len: int = 100,\n",
    "                 #min_episode_reward: int = -21, # ?\n",
    "                 seed: int = 101,\n",
    "                 batches_per_epoch: int = 10000,\n",
    "                 n_steps: int = 1,\n",
    "                 **kwargs,\n",
    "                ):\n",
    "        '''\n",
    "            Explain constructor params...\n",
    "        '''\n",
    "        \n",
    "        # Mandatory torch call\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Assign constructor attr\n",
    "        self.train_env = train_env\n",
    "        self.test_env = None\n",
    "        self.observation_space_shape = self.train_env.observation_space.shape\n",
    "        ### Discrete action space here\n",
    "        self.n_actions = self.train_env.action_space.n #### INCORRECT\n",
    "        \n",
    "        # Model attributes\n",
    "        ### Don't assign the dataloader as a class attribute\n",
    "        self.replay_buffer = None\n",
    "        self.dataset = None\n",
    "        self.net = None\n",
    "        self.target_net = None\n",
    "        ### Build model \n",
    "        self._build_model()\n",
    "        \n",
    "        # Save mdoel hparams (lightning mandatory call)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        \n",
    "        # Metrics\n",
    "        self.total_episode_steps = [0]\n",
    "        self.total_rewards = [0]\n",
    "        self.done_episodes = 0\n",
    "        self.total_steps = 0\n",
    "\n",
    "        # Average Rewards\n",
    "        self.avg_reward_len = avg_reward_len\n",
    "\n",
    "        for _ in range(avg_reward_len):\n",
    "            self.total_rewards.append(torch.tensor(min_episode_reward, device=self.device))\n",
    "\n",
    "        self.avg_rewards = float(np.mean(self.total_rewards[-self.avg_reward_len :]))\n",
    "\n",
    "        \n",
    "        # Transition book-keeping\n",
    "        self.state = None\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size = self.hparams.buffer_size)\n",
    "        \n",
    "        # See docs (looks important)\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        \n",
    "    # Build the model\n",
    "    def _build_model(self) -> None:\n",
    "        '''\n",
    "            Initializes the DQN, and target net\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # Instantiate DQN\n",
    "        ### Will use this class for box2d or classic control envs,\n",
    "        ### in which case: observation_space_dim = observation_space_shape[0]\n",
    "        self.net = DQN(observation_space_dim = self.hparams.observation_space_shape[0], \n",
    "                       n_actions = self.hparams.n_actions, \n",
    "                       n_hidden = 128)\n",
    "        self.target_net = DQN(observation_space_dim = self.hparams.observation_space_shape[0],\n",
    "                              n_actions = self.hparams.n_actions, \n",
    "                              n_hidden = 128)\n",
    "        ### Copy params from DQN\n",
    "        self.target_net.load_state_dict(self.net.state_dict())\n",
    "    \n",
    "    # Get actions from DQN (necessary for RL)\n",
    "    def get_action(self, state: th.Tensor, device: th.device):\n",
    "        \"\"\"\n",
    "            Computes action from DQN output\n",
    "        \"\"\"\n",
    "        if not isinstance(state, th.Tensor):\n",
    "            state = th.tensor(state, device=device)\n",
    "\n",
    "        \n",
    "        q_val = self.net(state)\n",
    "        q_val_max, action_star = th.max(q_val, dim=1)\n",
    "        return action_star.detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    # Run n_episodes (for testing)\n",
    "    def run_n_episodes(self, \n",
    "                       test_env: gym.Env,\n",
    "                       n_episodes: int = 1,\n",
    "                       max_episode_length: int = 10000,\n",
    "                      ):\n",
    "        \"\"\"\n",
    "            Runs a number of episodes in a test environment without exploration.\n",
    "            Actions are obtained from current DQN using self.get_actions().\n",
    "            Called by the test_step() method.\n",
    "            \n",
    "            :param env: environment to use, either train environment or test environment\n",
    "            :param n_episodes: number of episodes to run\n",
    "            :param max_episode_length: Maximal num. of steps per episode\n",
    "            \n",
    "            :return total_rewards_hist:\n",
    "        \"\"\"\n",
    "        # Init rwrds list\n",
    "        total_rewards_hist = []\n",
    "        \n",
    "        # Main loop\n",
    "        for i in range(n_episodes):\n",
    "            \n",
    "            # Initializations\n",
    "            state = test_env.reset()\n",
    "            done = False\n",
    "            episode_tot_rwrd = 0\n",
    "            step = 0\n",
    "            \n",
    "            # Episodic loop\n",
    "            while not done and (step<max_episode_length):\n",
    "                action = self.get_action(state, self.device)\n",
    "                ## NOTE: gym >= 0.26.2 and gymnasium\n",
    "                state_next, reward, done, truncated, _ \\\n",
    "                    = test_env.step(action[0])\n",
    "                episode_tot_rwrd += reward\n",
    "                state = state_next\n",
    "                step+=1\n",
    "            \n",
    "            # Append episode total reward to hist\n",
    "            total_rewards_hist.append(episode_tot_rwrd)\n",
    "        \n",
    "        # Output\n",
    "        return total_rewards_hist\n",
    "    \n",
    "    # Necessary?\n",
    "    def populate(self, n_warmup_steps: int) -> None:\n",
    "        \"\"\"\n",
    "            Populates the replay buffer with the specified number\n",
    "            of warmup transitions in the training environment.\n",
    "            Resets env if done to continue warmup, and uses epsilon\n",
    "            greedy policy.\n",
    "            \n",
    "            :param n_warmup_steps: Num. of warmup steps to make\n",
    "        \"\"\"\n",
    "        if n_warmup_steps>0:\n",
    "            self.state = self.train_env.reset()\n",
    "            \n",
    "            for i in range(n_warmup_steps):\n",
    "                \n",
    "                # Get action following eps-greedy policy\n",
    "                ### Q: Where do we initialize self.epsilon?\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    ### WARNING:Review action shape\n",
    "                    action = np.array([self.test_env.action_space.sample()])\n",
    "                else:\n",
    "                    action = self.get_action(self.state, self.device)\n",
    "                \n",
    "                state_next, reward, done, truncated, _ \\\n",
    "                    = self.train_env.step(action[0])\n",
    "                self.replay_buffer.append(self.state, action, reward, done, state_next)\n",
    "                self.state = state_next\n",
    "                \n",
    "                if done:\n",
    "                    self.state = self.train_env.reset()\n",
    "            \n",
    "            \n",
    "    \n",
    "    # Training dataloader assignment (CRUCIAL)\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "            Initialize training dataloader\n",
    "        \"\"\"\n",
    "        self.dataset = RLDataset(self.replay_buffer)\n",
    "        return DataLoader(dataset = self.dataset, batch_size = self.hparams.batch_size)\n",
    "    \n",
    "    # Forward (CRUCIAL)\n",
    "    def forward(self, x: th.Tensor) -> th.Tensor:\n",
    "        \"\"\"\n",
    "            Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "            # NOTE: For an actor-critic algorithm with a stochastic policy, this function shou\n",
    "        \"\"\"\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "    \n",
    "    # Optimizers' initialization f'n (CRUCIAL)\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\"\n",
    "            Initialize optimizers for all class networks.\n",
    "            # NOTE: Should return a list or a tuple of Optimizer objects.\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr = self.hparams.dqn_lr)\n",
    "        return [optimizer]\n",
    "    \n",
    "    # Compute loss (CRUCIAL)\n",
    "    def loss(self, batch: Tuple[th.Tensor, th.Tensor, th.Tensor, th.Tensor, th.Tensor])-> th.Tensor:\n",
    "        \"\"\"\n",
    "            Method to calculate the loss value\n",
    "            # NOTE: Review this part. I hardly recognize what this does.\n",
    "        \"\"\"\n",
    "        state_b, action_b, reward_b, done_b, state_next_b = batch\n",
    "        \n",
    "        state_action_values = self.net(state_b).gather(1, \n",
    "                                                       action_b.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Evaluate target \n",
    "        with th.no_grad():\n",
    "            state_next_q_vals = self.target_net(state_next_b).max(1)[0]\n",
    "            state_next_q_vals[done_b] = 0.0\n",
    "            state_next_q_vals = state_next_q_vals.detach()\n",
    "        \n",
    "        # Bellman backup\n",
    "        expected_state_action_vals = self.hparams.gamma*state_next_q_vals+reward_b\n",
    "        \n",
    "        # Output\n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_vals)\n",
    "    \n",
    "    # Training step (CRUCIAL)\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"\n",
    "            Need to add comments here. Seems to be the the crux of the implementation.\n",
    "            Where is this executed.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get optimizer\n",
    "        dqn_optim = self.optimizers()\n",
    "                \n",
    "        # Get training batch\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.loss(batch)\n",
    "        ### Clarify this condition\n",
    "        #if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "        \n",
    "        # Gradient step\n",
    "        ## NOTE: You use a manual backward here\n",
    "        ## Important for actor-critic algos\n",
    "        dqn_optim.zero_grad()\n",
    "        self.manual_backward(loss)\n",
    "        dqn_optim.step()\n",
    "        \n",
    "        # Update target net\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "        \n",
    "        # Log dict\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"total_reward\": self.total_rewards_hist[-1],\n",
    "                \"avg_reward\": self.avg_rewards,\n",
    "                \"train_loss\": loss,\n",
    "                \"episodes\": self.done_episodes,\n",
    "                \"episode_steps\": self.total_episode_steps[-1],\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Output\n",
    "        return OrderedDict({\"loss\": loss, \"avg_reward\": self.avg_rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6946e-9087-4fc6-98c5-a7b17044453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    '''\n",
    "    # Initialize replay buffer\n",
    "    def _init_replay_buffer(self):\n",
    "        '''\n",
    "            #Initializes the replay buffer\n",
    "        '''\n",
    "        \n",
    "        # Instantiate replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(self.hparams.buffer_size)\n",
    "        \n",
    "        # Add \"n_warmup_steps\" transitions to the buffer\n",
    "        self.populate(self.hparams.n_warmup_steps)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db41507-93fa-4795-8a35-8e8a842ee5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e5436-1714-48f4-a0ca-ab12074453f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a6b27-170a-434c-a69f-17c99d183960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcead075-8cd1-4d09-aafa-b11e888aa77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd483d6c-6497-4d6a-be42-025f3fec3ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2b85b-dad3-4631-aebf-ba6b3ed17f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ecdedce-2053-4d6b-9a65-f96d0bd048bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "max(input) -> Tensor\n",
       "\n",
       "Returns the maximum value of all elements in the ``input`` tensor.\n",
       "\n",
       ".. warning::\n",
       "    This function produces deterministic (sub)gradients unlike ``max(dim=0)``\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = torch.randn(1, 3)\n",
       "    >>> a\n",
       "    tensor([[ 0.6763,  0.7445, -2.2369]])\n",
       "    >>> torch.max(a)\n",
       "    tensor(0.7445)\n",
       "\n",
       ".. function:: max(input, dim, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
       "   :noindex:\n",
       "\n",
       "Returns a namedtuple ``(values, indices)`` where ``values`` is the maximum\n",
       "value of each row of the :attr:`input` tensor in the given dimension\n",
       ":attr:`dim`. And ``indices`` is the index location of each maximum value found\n",
       "(argmax).\n",
       "\n",
       "If ``keepdim`` is ``True``, the output tensors are of the same size\n",
       "as ``input`` except in the dimension ``dim`` where they are of size 1.\n",
       "Otherwise, ``dim`` is squeezed (see :func:`torch.squeeze`), resulting\n",
       "in the output tensors having 1 fewer dimension than ``input``.\n",
       "\n",
       ".. note:: If there are multiple maximal values in a reduced row then\n",
       "          the indices of the first maximal value are returned.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "    dim (int): the dimension to reduce.\n",
       "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not. Default: ``False``.\n",
       "\n",
       "Keyword args:\n",
       "    out (tuple, optional): the result tuple of two output tensors (max, max_indices)\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> a = torch.randn(4, 4)\n",
       "    >>> a\n",
       "    tensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n",
       "            [ 1.1949, -1.1127, -2.2379, -0.6702],\n",
       "            [ 1.5717, -0.9207,  0.1297, -1.8768],\n",
       "            [-0.6172,  1.0036, -0.6060, -0.2432]])\n",
       "    >>> torch.max(a, 1)\n",
       "    torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n",
       "\n",
       ".. function:: max(input, other, *, out=None) -> Tensor\n",
       "   :noindex:\n",
       "\n",
       "See :func:`torch.maximum`.\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?th.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cd159d-2354-4f5a-88d5-b2ccf8744b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
