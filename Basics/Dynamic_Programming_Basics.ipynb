{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f43c12",
   "metadata": {},
   "source": [
    "# **Dynamic Programming (Basics)**\n",
    "### 2022/04/24, A. J. Zerouali\n",
    "\n",
    " \n",
    "\n",
    "## **1 - Introduction**\n",
    "Following Lazy Programmer's Reinforcement Learning course. I'm starting with Section 5 on Dynamic Programming. It will cover the following topics:\n",
    "* **Prediction:** The iterative policy evaluation algorithm;\n",
    "* **Control:** The policy iteration and value iteration methods for policy improvement.\n",
    "\n",
    "The contents are divided into the following sections:\n",
    "\n",
    "    1 - Introduction\n",
    "    2 - Iterative policy evaluation\n",
    "    3 - Policy iteration\n",
    "    4 - Value iteration\n",
    "    5 - Dynamic programming in optimal control\n",
    "\n",
    "### 22/04/21 note:\n",
    "Need to debug policy iteration, creating new file for that. Present notebook is becoming unreadable due to code. Consider switching to \\*.py files and including them in the notebook. \n",
    "### 22/04/22 note:\n",
    "The main algorithm works. Compare results of section 3 with those of Lecture 57.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa18e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### IMPORTANT: ALWAYS EXECUTE THIS CELL FIRST #####\n",
    "#####################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35775e7e",
   "metadata": {},
   "source": [
    "## 2 - Iterative policy evaluation\n",
    "\n",
    "Below are my notes from the relevant lectures. \n",
    "\n",
    "**From Lecture 48:**\n",
    "* For this part, we assume we know the *finite* MDP: The transition probabilities and the policy. \n",
    "* The goal is to evaluate the value function using the Bellman equation:\n",
    "\n",
    "$$V_\\pi(s) = \\sum_a\\pi(a|s)\\sum_{s',r}P(s',r|s,a)\\left(r+\\gamma V_\\pi(s')\\right)$$\n",
    "\n",
    "(Remark: We're assuming the rewards are also stochastic.)\n",
    "\n",
    "* In principle, since the state $\\mathcal{S}$ and action $\\mathcal{A}$ spaces are finite, we could view the Bellman equation as a linear system, but that's not optimal. \n",
    "* Iterative policy evaluation is a fixed-point algorithm.\n",
    "* Implementation remark: Arrays and lists are more computationally intensive than dictionaries. Lazy Programmer mentions that in lecture 48, in relation to the data structure used to store the \"value function\". He makes the same suggestions for the transition probabilities and the policy.\n",
    "\n",
    "**Lecture 49:**\n",
    "* This lecture describes the general layout of a reinforcement learning algorithm.\n",
    "\n",
    "**Lecture 50: Gridworld in code**\n",
    "* Link to GitHub repo: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rl/grid_world.py\n",
    "* I'll write a simplified version below. \n",
    "* Here's how an episode execution should look like:\n",
    "        g = GridWorld(rows, cols, initial_state)\n",
    "        while not g.game_over()\n",
    "            s = g.current_state()\n",
    "            a = policy(s)\n",
    "            r = g.move(a)\n",
    "    Above, the GridWorld methods used are:\n",
    "        # Return current state of the agent:\n",
    "        g.current_state()\n",
    "        # Return reward  \n",
    "        reward = g.move(action)\n",
    "        # Check if game (episode) is over, i.e. if current state terminal.\n",
    "        g.game_over()\n",
    "        \n",
    "**Lecture 51: Iterative policy evaluation**\n",
    "\n",
    "* We'll start with the deterministic version of GridWorld. There will be several helper functions to consider too and additional data structures.\n",
    "        \n",
    "\n",
    "* First we want to visualize the value function and the policy. We'll write it insuch a way that we can see it on a grid.\n",
    "\n",
    "* Regarding the transition probabilities, we will make a dictionary to store $p(s'|s,a)$. Note that the rewards are deterministic here, and that we'll need a helper function to build this dictionary.\n",
    "\n",
    "* The policy will also be a dictionary, where the keys are the admissible states and the values are the actions \"U\", \"D\", \"L\" and \"R\".\n",
    "\n",
    "* We'll create a function implementing the policy evaluation algorithm.\n",
    "\n",
    "* The code discussed in the lecture can be found here: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rl/iterative_policy_evaluation_deterministic.py\n",
    "\n",
    "* I copied Lazy Programmer's printing functions and modified them a little. For the actual implementation of iterative policy evaluation, I made a specific function, and I wrote a simplified version compared to his. I get the same results as those of Lecture 51.\n",
    "\n",
    "**Lecture 52: Windy GridWorld**\n",
    "\n",
    "* The idea here is that we'll make a slight generalization where the transition probabilities are not just 0 or 1. In The instructor's code, the transitions $P(s'|s,a)$ will now be encoded as a dictionary where the keys are the state-actions $(s,a)$, and where the coresponding values are dictionaries $\\{s': P(s'|s,a); s'\\in\\mathcal{S}\\}$. Although not immediately intuitive, this approach is better for generalizing iterative policy evaluation to stochastic policies, and for the \"Windy GridWorld\" environment.\n",
    "\n",
    "* Windy GridWorld is a variant where some of the squares in the grid push the agent to another state. The code for Windy Gridworld is in the same file, while the code for stochastic policy evaluation is at: https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rl/iterative_policy_evaluation_probabilistic.py.\n",
    "\n",
    "* I will have to modify the GridWorld class to account for stochastic transitions. The *set(attributes)* method needs to now take the transition probabilities into account, and the *get_next_state()* method will not be used anymore. The *move()* method is now more interesting, and calls upon *np.random.choice()*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec31ce",
   "metadata": {},
   "source": [
    "### Windy GridWorld Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669e07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### WINDY GRIDWORLD #####\n",
    "# Updated: 22/04/07, A. J. Zerouali\n",
    "# The Windy GridWorld environment used in Lazy Programmer's course.\n",
    "# This is a 3x4 grid, with wall at (1,1), +1 reward at the terminal \n",
    "# square (0,3), and -1 reward at the terminal square (1,3).\n",
    "# For the \"windy\" variant, the main changes occur in the move() method.\n",
    "# States are (i,j) tuples, actions are characters, containers are dictionaries.\n",
    "\n",
    "\n",
    "# GridWorld_simple with only 3x4 grid. This is the environment.\n",
    "class GridWorld_Windy_small():\n",
    "    def __init__(self, rows, cols, ini_state, non_term_states, term_states, actions):\n",
    "        # Attributes rows and cols are dimensions of the grid\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        # Coordinates of agent\n",
    "        self.i = ini_state[0]\n",
    "        self.j = ini_state[1]\n",
    "        # State and action spaces\n",
    "        self.non_term_states = non_term_states\n",
    "        self.term_states = term_states\n",
    "        self.actions = actions \n",
    "        # The next attributes are populated using the set() method\n",
    "        self.adm_actions = {}\n",
    "        self.rewards = {}\n",
    "        self.transition_probs = {}\n",
    "        \n",
    "    # Method setting up the actions, rewards, and transition probabilities\n",
    "    def set(self, rewards, adm_actions, transition_probs):\n",
    "        # INPUT: adm_actions: Dictionary of (i,j):[a_i] = (row,col):[action list]\n",
    "        #        rewards: Dictionary of (i,j):r = (row,col):reward\n",
    "        #        transition_probs: Dictionary of (i,j):{a_i:p_ij}= ...\n",
    "        #                          .. (row,col):{dictionary of probs for each action}\n",
    "        # WARNING: Do not confuse self.adm_actions with self.actions. Latter is the action space,\n",
    "        #          adm_actions are the accessible actions from a state (dict. {s_i:[a_ij]}).\n",
    "        self.rewards = rewards\n",
    "        self.adm_actions = adm_actions\n",
    "        self.transition_probs = transition_probs\n",
    "    \n",
    "    # Method that sets current state of agent\n",
    "    def set_state(self, s):\n",
    "        # INPUT: s: (i,j)=(row,col), coord. of agent\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    # Method to return current state of agent\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    # Method to check if current agent state is terminal\n",
    "    # Note: Lazy Prog not explciting terminal states\n",
    "    def is_terminal(self, s):\n",
    "        return (s in self.term_states)\n",
    "    \n",
    "    # HAS TO BE MODIFIED FOR WINDY GRIDWORLD\n",
    "    # Method to perform action in environment\n",
    "    def move(self, action):\n",
    "        # Input:  action: New action to execute\n",
    "        # Output: reward\n",
    "        # Comments: - Requires transition probabilities. \n",
    "        #           - Calls numpy.random.choice(), doesn't work with dictionaries.\n",
    "        \n",
    "        # Check if action is admissible in current state\n",
    "        if action in adm_actions[self.current_state()]:\n",
    "            \n",
    "            # Convert transition_probs to lists compatible with np.random.choice().\n",
    "            # Recall self.transition_probs[(self.current_state(), action)] is a dictionary,\n",
    "            # while np.random.choice() works with ints or ndarrays.\n",
    "            next_states = list(self.transition_probs[(self.current_state(), action)].keys())\n",
    "            next_states_probs = list(self.transition_probs[(self.current_state(), action)].values())\n",
    "            \n",
    "            # Generate a random index (this Numpy function is tricky)\n",
    "            rand_ind = np.random.choice(a = len(next_states), p = next_states_probs)\n",
    "            # Set new state of agent\n",
    "            s_new = next_states[rand_ind] # Not necessary, for debug\n",
    "            self.set_state(s_new)\n",
    "        # END IF\n",
    "   \n",
    "        # Return reward. If not in given dictionary, return 0\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    # Method to check if agent is currently in terminal state\n",
    "    def game_over(self):\n",
    "        # Output true if agent is in terminal states (0,3) or (1,3)\n",
    "        return ( (self.i, self.j) in self.term_states)\n",
    "    \n",
    "    # Method returnning all admissible states, i.e. not in the wall (1,1)\n",
    "    def all_states(self):\n",
    "        return (self.non_term_states | self.term_states )\n",
    "# END CLASS\n",
    "\n",
    "\"\"\"\n",
    "#### Scrap for move() method\n",
    "# Create lists compatible with np.random.choice().\n",
    "# self.transition_probs[(self.current_state(), action)] is a dictionary\n",
    "next_states = list(transition_probs[((1,2),\"U\")].keys())\n",
    "next_states_probs = list(transition_probs[((1,2),\"U\")].values())\n",
    "            \n",
    "# Generate a random index (this numpy function is tricky)\n",
    "rand_ind = np.random.choice(a = len(next_states), p = next_states_probs)\n",
    "# Set new state of agent\n",
    "s_new = next_states[rand_ind] # Not necessary, for debug\n",
    "print(s_new)\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "# Helper function to construct an environment.\n",
    "# Consists mainly of initializations.\n",
    "def windy_standard_grid(penalty=0):\n",
    "    # Input: penalty: Float. Penalty for moving to non terminal state.\n",
    "    # Output: env. Windy_GridWorld_small() object (the environment).\n",
    "    \n",
    "    # Start at bottom left (randomize later)\n",
    "    ini_state = (2,0)\n",
    "    # Action space \n",
    "    ACTION_SPACE = {\"U\", \"D\", \"L\", \"R\"}\n",
    "    # Non terminal states\n",
    "    NON_TERMINAL_STATES = {(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2), (2,3)}\n",
    "    # Terminal states\n",
    "    TERMINAL_STATES = {(0,3), (1,3)}\n",
    "    \n",
    "    # Instantiate:\n",
    "    env = GridWorld_Windy_small(3, 4, ini_state, NON_TERMINAL_STATES, TERMINAL_STATES, ACTION_SPACE)\n",
    "\n",
    "    \n",
    "    # Dictionary of rewards\n",
    "    # Not storing 0s if penalty=0\n",
    "    rewards = {(0,3):1, (1,3): -1}\n",
    "    # Poplate non terminal states for penalty != 0\n",
    "    if penalty != 0:\n",
    "        for s in NON_TERMINAL_STATES:\n",
    "            rewards[s] = penalty\n",
    "    \n",
    "    # Dictionary of admissible actions per state\n",
    "    adm_actions = {\n",
    "        (0,0): (\"D\", \"R\"),\n",
    "        (0,1): (\"L\", \"R\"),\n",
    "        (0,2): (\"L\", \"R\", \"D\"),\n",
    "        (1,0): (\"D\", \"U\"),\n",
    "        (1,2): (\"U\", \"D\", \"R\"),\n",
    "        (2,0): (\"U\", \"R\"),\n",
    "        (2,1): (\"L\", \"R\"),\n",
    "        (2,2): (\"U\", \"R\", \"L\"),\n",
    "        (2,3): (\"U\", \"L\"),\n",
    "    }\n",
    "    \n",
    "    # Dictionary of transition probabilities\n",
    "    # NOTE: I've modified the instructor's implementation.\n",
    "    #       I've removed all tautologies (agent doesn't stay in current state).\n",
    "    transition_probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        \n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        \n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        \n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        \n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        \n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        \n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "    }\n",
    "    \n",
    "    # Assign missing environment attributes\n",
    "    env.set(rewards, adm_actions, transition_probs)\n",
    "    \n",
    "    # Output line\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c4050",
   "metadata": {},
   "source": [
    "### Printing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9283909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe policy looks like this:\\n\\npi = {\\n    (2, 0): {'U': 1.0},\\n    (1, 0): {'U': 1.0},\\n    (0, 0): {'R': 1.0},\\n    (0, 1): {'R': 1.0},\\n    (0, 2): {'R': 1.0},\\n    (1, 2): {'U': 1.0},\\n    (2, 1): {'R': 1.0},\\n    (2, 2): {'U': 1.0},\\n    (2, 3): {'L': 1.0},\\n  }\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### PRINTING FUNCTIONS #####\n",
    "# 2022/04/06, AJ Zerouali\n",
    "# Modified from Lazy Prog's GitHub\n",
    "\n",
    "def print_values(Val_fn, env):\n",
    "    print(f\"## VALUE FUNCTION ##\")\n",
    "    for i in range(env.rows):\n",
    "        print(\"------------------------\")\n",
    "        for j in range(env.cols):\n",
    "            v = Val_fn.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "def print_policy(Pi_fn, env):\n",
    "    # REMARK: WILL ONLY PRINT A DETERMINISTIC POLICY WITH {(i,j):{\"action\":1.0}}\n",
    "    print(f\"##  POLICY  ##\")\n",
    "    for i in range(env.rows):\n",
    "        print(\"------------------------\")\n",
    "        for j in range(env.cols):\n",
    "            if (i,j) not in [(1,1), (0,3), (1,3)]:\n",
    "                # WARNING: Will only work if there's one and only one element\n",
    "                a = list(Pi_fn[(i,j)].keys())[0]\n",
    "                print(\"  %s  |\" % a, end=\"\")\n",
    "            elif (i,j) == (1,1):\n",
    "                print(\"  %s  |\" % \" \", end=\"\")\n",
    "        print(\"\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "\"\"\"\n",
    "The policy looks like this:\n",
    "\n",
    "pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163434f1",
   "metadata": {},
   "source": [
    "### Iterative policy evaluation\n",
    "\n",
    "A more general function for stochastic policies and non-trivial transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69a2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ITERATIVE POLICY EVALUATION #####\n",
    "## 2022/04/08, AJ Zerouali\n",
    "\n",
    "def iter_policy_eval(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma):\n",
    "    # ARGUMENTS:\n",
    "    #  Pi: Dict. Policy function to be evaluated, from main() function.\n",
    "    #  V_ini: Dict. Initial value fn, from main() function.\n",
    "    #  P_trans: Dict. Transition probabilities of MDP, from main() function.\n",
    "    #  Rwds: Dict. Rewards by (state, action, state_new), from main() function.\n",
    "    #  adm_actions: Dict. Admissible actions in a given state, from grid attributes.\n",
    "    #  non_term_states: Set. Non terminal states, from grid attributes.\n",
    "    #  term_states: Set. Terminal states only, from grid attributes.\n",
    "    #  epsilon: Float. Convergence threshold (for sup norm of value function), from main() function.\n",
    "    #  gamma: Float. Discount factor, from main() function.\n",
    "    \n",
    "    # OUTPUT:\n",
    "    #  V_pi: Dict. Value function corresp. to Pi\n",
    "    #  k: Number of iterations for convergence of policy eval.\n",
    "    \n",
    "    \n",
    "    # INITIALIZATIONS\n",
    "    # V_k and V_(k+1) ini. (get switched in while loop)\n",
    "    V_new = V_ini\n",
    "    for s in term_states:\n",
    "        V_new[s] = 0\n",
    "    V_old = {}\n",
    "    # Iteration counter ini\n",
    "    k = 0\n",
    "    # Stopping Boolean ini\n",
    "    V_is_stable = False\n",
    "    \n",
    "    \n",
    "    # MAIN LOOP\n",
    "    # Iterates over k\n",
    "    while not V_is_stable:\n",
    "        \n",
    "        # Initialize V_k and V_(k+1)\n",
    "        V_old = V_new\n",
    "        V_new = {}\n",
    "        for s in term_states:\n",
    "            V_new[s] = 0\n",
    "        # Initialize sup|V_(k+1) - V_k|\n",
    "        Delta_V = 0\n",
    "        \n",
    "        # EVALUATE V_(k+1)=V_new\n",
    "        # Loop over non terminal states\n",
    "        for s in non_term_states:  \n",
    "            \n",
    "            # COMPUTE V_(k+1)(s)\n",
    "            \n",
    "            # Initialize\n",
    "            V_s_new = 0\n",
    "            \n",
    "            # Loop over admissible actions in state s\n",
    "            for a in adm_actions[s]:\n",
    "                \n",
    "                # Add sum over s_ind only if pi(a|s) is non-zero:\n",
    "                if (Pi[s].get(a,0) != 0):\n",
    "                \n",
    "                    # This loop is only over non-trivial transitions\n",
    "                    for s_ind in P_trans[(s,a)].keys(): \n",
    "                        # UPDATE V_s_new\n",
    "                        V_s_new += Pi[s].get(a,0)*P_trans[(s,a)].get(s_ind,0) \\\n",
    "                                    *( Rwds.get(s_ind,0) + gamma*V_old[s_ind] )  \n",
    "                    # END FOR OVER s_ind\n",
    "                    \n",
    "                # END IF\n",
    "                \n",
    "            # END FOR OVER a\n",
    "            \n",
    "            # Assign V_(k+1)(s)\n",
    "            V_new[s] = V_s_new\n",
    "            \n",
    "            # Update sup|V_(k+1) - V_k|\n",
    "            Delta_V = max(Delta_V, abs(V_s_new-V_old.get(s,0)) )\n",
    "            \n",
    "        # END FOR OVER s     \n",
    "        \n",
    "        # Update stopping Boolean\n",
    "        V_is_stable = (Delta_V < epsilon)\n",
    "        \n",
    "        # Update iteration counter\n",
    "        k += 1\n",
    "    # END WHILE\n",
    "    \n",
    "    # Return V_pi and number of iterations\n",
    "    return V_new, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941d662",
   "metadata": {},
   "source": [
    "### Main\n",
    "\n",
    "This is where we run all of the above. I get the same results as Lazy Programmer's (5:56min in Lecture 53):\n",
    "\n",
    "    ### Value Function ###\n",
    "    ------------------------\n",
    "     0.81| 0.90| 1.00| 0.00|\n",
    "    ------------------------\n",
    "     0.73| 0.00|-0.05| 0.00|\n",
    "    ------------------------\n",
    "     0.31|-0.04|-0.04|-0.04|\n",
    "    ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4897f25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the policy pi specified above, iterative policy evaluation converged after N_iter=6 iterations. V_pi is:\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00|-0.05| 0.00|\n",
      "------------------------\n",
      " 0.31|-0.04|-0.04|-0.04|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "##### MAIN - ITERATIVE POLICY EVALUATION IN WINDY GRIDWORLD #####\n",
    "# 2022/04/08, AJ Zerouali\n",
    "# Loosely follows lectures 52-53.\n",
    "#\n",
    "\n",
    "# Create environment\n",
    "# adm_actions, rewards and transition_probs are attributes of grid\n",
    "grid = windy_standard_grid()\n",
    "\n",
    "\n",
    "### The policy dictionary ###\n",
    "pi = {\n",
    "    (2, 0): {'U': 0.5, 'R': 0.5},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "### Initial value function ###\n",
    "# Just a dictionary of 0s\n",
    "V = {}\n",
    "for s in grid.all_states():\n",
    "    V[s] = 0\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Compute V_pi\n",
    "# Signature: iter_policy_eval(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "V_pi, N_iter = iter_policy_eval(pi, V, grid.transition_probs, grid.rewards, grid.adm_actions,\\\n",
    "                                grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "# Print the value function function obtained\n",
    "print(f\"For the policy pi specified above, iterative policy evaluation converged after N_iter={N_iter} iterations. V_pi is:\")\n",
    "print_values(V_pi, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fd649e",
   "metadata": {},
   "source": [
    "### Test case (non windy)\n",
    "\n",
    "Below is a test case. Should give the following table of values:\n",
    "\n",
    "        Value function\n",
    "        \n",
    "        ------------------------\n",
    "         0.81| 0.90| 1.00| 0.00|\n",
    "        ------------------------\n",
    "         0.73| 0.00| 0.90| 0.00|\n",
    "        ------------------------\n",
    "         0.66| 0.73| 0.81| 0.73|\n",
    "        ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b59d7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Windy GridWorld deterministic test converged after N_iter=6 iterations. V_pi is:\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "##### (NON)WINDY GRIDWORLD (TEST) #####\n",
    "# Updated: 22/04/08, A. J. Zerouali\n",
    "# Test with non-windy case and deterministic policy\n",
    "\n",
    "def test_standard_grid():\n",
    "    # Start at bottom left (randomize later)\n",
    "    ini_state = (2,0)\n",
    "    # Action space \n",
    "    ACTION_SPACE = {\"U\", \"D\", \"L\", \"R\"}\n",
    "    # Non terminal states\n",
    "    NON_TERMINAL_STATES = {(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2), (2,3)}\n",
    "    # Terminal states\n",
    "    TERMINAL_STATES = {(0,3), (1,3)}\n",
    "    \n",
    "    # Instantiate:\n",
    "    # \n",
    "    env = GridWorld_Windy_small(3, 4, ini_state, NON_TERMINAL_STATES, TERMINAL_STATES, ACTION_SPACE)\n",
    "\n",
    "    \n",
    "    # Dictionary of rewards\n",
    "    # Not storing 0s\n",
    "    rewards = {(0,3):1, (1,3): -1}\n",
    "    \n",
    "    # Dictionary of admissible actions per state\n",
    "    adm_actions = {\n",
    "        (0,0): (\"D\", \"R\"),\n",
    "        (0,1): (\"L\", \"R\"),\n",
    "        (0,2): (\"L\", \"R\", \"D\"),\n",
    "        (1,0): (\"D\", \"U\"),\n",
    "        (1,2): (\"U\", \"D\", \"R\"),\n",
    "        (2,0): (\"U\", \"R\"),\n",
    "        (2,1): (\"L\", \"R\"),\n",
    "        (2,2): (\"U\", \"R\", \"L\"),\n",
    "        (2,3): (\"U\", \"L\"),\n",
    "    }\n",
    "    \n",
    "    # Dictionary of deterministic transitions:\n",
    "    transition_probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        \n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        \n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        \n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        \n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        \n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        \n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((1, 2), 'U'): {(0, 2): 1.0},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "    }\n",
    "    \n",
    "    # Assign missing environment attributes\n",
    "    env.set(rewards, adm_actions, transition_probs)\n",
    "    \n",
    "    # Output line\n",
    "    return env\n",
    "\n",
    "# Create environment\n",
    "# adm_actions, rewards and transition_probs are attributes of grid\n",
    "\n",
    "grid = test_standard_grid()\n",
    "\n",
    "\n",
    "### The policy dictionary ###\n",
    "pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "### Initial value function ###\n",
    "# Just a dictionary of 0s\n",
    "V = {}\n",
    "for s in grid.all_states():\n",
    "    V[s] = 0\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Compute V_pi\n",
    "# Signature: iter_policy_eval(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "V_pi, N_iter = iter_policy_eval(pi, V, grid.transition_probs, grid.rewards, grid.adm_actions,\\\n",
    "                                grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "# Print the value function function obtained\n",
    "print(f\"The Windy GridWorld deterministic test converged after N_iter={N_iter} iterations. V_pi is:\")\n",
    "print_values(V_pi, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17600da",
   "metadata": {},
   "source": [
    "## 3 - Policy improvement by policy iteration\n",
    "\n",
    "The control step of dynamic programming is where we improve the policy. This is done iteratively and calls upon policy evaluation at each step. Value iteration is more efficient as it blends these two steps (see next section).\n",
    "\n",
    "**Lecture 54: Policy improvement**\n",
    "* Idea is to start from a policy and improve it iteratively (obviously). Lazy Programmer only considers a deterministic policy in lecture 54 (similar to sections 4.2 and 4.3 of Sutton-Barto).\n",
    "\n",
    "* Suppose we have a deterministic policy $\\pi$ for which the value function is $V_\\pi(s) = Q_\\pi\\left(s,\\pi(s)\\right)$ (expectation reduces to one term only). Now consider the value $a'_s=\\arg\\max_aQ_\\pi(s,a)$ such that $Q_\\pi(s,a'_s)\\ge V_\\pi(s)$. Computationally, we get a higher state-action value by taking a value different from $\\pi(s)$ in the beginning, but still following $\\pi$ for the remaining steps of the episode. \n",
    "\n",
    "* Going further, if we now decide to *always* use action $a'_s$ when in state $s$, we are in fact following a new policy $\\tilde{\\pi}$ such that $\\tilde{\\pi}(s')=\\pi(s')$ for $s'\\ne s$ and $\\tilde{\\pi}(s)=\\arg\\max_aQ_\\pi(s,a)$. This is the basis of policy improvement.\n",
    "\n",
    "* The Policy Improvement Theorem states that if $Q_\\pi\\left(s,\\tilde{\\pi}(s_0)\\right)\\ge V_\\pi(s_0)$ for *some* state $s_0\\in\\mathcal{S}$, **then** $V_\\tilde{\\pi}(s)\\ge V_\\pi(s)$ **for all** $s\\in\\mathcal{S}$. Thus, improving $\\pi$ iteratively over the states guarantees obtaining an overall better policy $\\tilde{\\pi}$. This is the main idea behind policy iteration.\n",
    "\n",
    "* Note that the Policy Improvement Theorem is not that obvious. In particular, the Bellman equation for updates does not hold anymore since we're switching policies.\n",
    "\n",
    "* Suppose we improve the policy $\\pi$ to $\\tilde{\\pi}$ iteratively. We get a convergence once $\\tilde{\\pi}=\\pi$, which is decided once $V_\\tilde{\\pi}=V_\\pi\\$. In that case, the value function satisfies **Bellman's optimality equation**:\n",
    "\n",
    "$$V_\\pi(s) = \\max_a \\left\\{\\sum_{s'}p(s'|s,a)\\left(r(s,a,s')+\\gamma\\cdot V_\\pi(s')\\right)\\right\\}.$$\n",
    "\n",
    "  The policy satisfying this equation is typically denoted $\\pi^\\ast$, and $V^\\ast:=V_{\\pi^\\ast}$.\n",
    "\n",
    "**Lecture 55: Policy iteration**\n",
    "\n",
    "* This lecture covers the high level implementation of policy iteration, with an explicit algorithm for the theoretical considerations above. There are less things to note in this lecture and the next 2, so I'll just recap the important points.\n",
    "\n",
    "* The optimal value function is unique, but that is not necessarily the case for optimal policies. The simple 3x4 GridWorld example starting from the bottom left (2,0) is a perfect illustration: Assuming all rewards aside from the terminal states are 0, there are 2 optimal paths to reach the winning state (0,3), from the top of the wall (1,1) or from its right.\n",
    "\n",
    "* When implementing policy iteration, a good practice would be to store the value function, and check whether a new policy improvement step changes the value function. Without this verification, there is a possibility that the policy iteration loop never ends, and just alternates between the 2 optimal policies.\n",
    "\n",
    "**Lecture 56: Policy iteration (cont'd)**\n",
    "\n",
    "* Lazy Programmer walks the listener through his implementation of policy iteration for (non windy) GridWorld.\n",
    "\n",
    "* Some highlights: 4:00: Constructing a random policy. 5:00: Policy improvement. 6:25: Comments on Bellman optimality eq'n. 7:35: Comments on convergence.\n",
    "\n",
    "**Lecture 57: Policy iteration in Windy GridWorld**\n",
    "\n",
    "* Since there's a \"wind push\" in state (1,2), the optimal policy is less obvious. To also see how rewards have an effect on optimal policies, it's also useful to introduce penalties to visiting states other than terminal ones. (**Comment:** Note how this means I should modify the environment generation function.)\n",
    "\n",
    "* In this lecture, Lazy Prog tests 2 environments, and gives a good illustration of how the optimal (deterministic) policy changes with different penalty values. He shows the cases where the penalties are 0, -0.1, -0.2, -0.4, -0.5 and -2. Notice how in the last 2 cases, the agent simply goes immediately to the losing state when near it, since the paths to the winning state are too long (leading to much lower cumulative returns).\n",
    "\n",
    "**Functions that we'll need:**\n",
    "1) Need a function that will implement policy improvement.\n",
    "\n",
    "2) Policy iteration will call iterative policy evaluation and policy improvement function in (1).\n",
    "\n",
    "3) Need a function that generates a random initial policy.\n",
    "\n",
    "4) Need a function that compares two value functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680c869",
   "metadata": {},
   "source": [
    "### Random policy generator and value function comparator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a07e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM DETERMINISTIC POLICY GENERATOR #####\n",
    "## 2022/04/08, AJ Zerouali\n",
    "# Recall: rand_ind = np.random.choice(a = len(next_states), p = next_states_probs)\n",
    "\n",
    "def gen_random_policy(env):\n",
    "    # Input: env, Windy_GridWorld_simple object (environment).\n",
    "    # Output: Pi, a (deterministic) policy dictionary.\n",
    "    non_term_states = env.non_term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Pi = {}\n",
    "    \n",
    "    for s in non_term_states:\n",
    "        actions_list = list(adm_actions[s])\n",
    "        a_random = actions_list[np.random.randint(len(actions_list))]\n",
    "        Pi[s] = {a_random:1.0}\n",
    "    \n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e7e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### COMPARE VALUE FUNCTION #####\n",
    "## 2022/04/22, AJ Zerouali\n",
    "\n",
    "def compare_value_fns(V_old, V_new, non_term_states):\n",
    "    # ARGUMENTS: - V_old and V_new: Dictionaries of 2 value functions to compare\n",
    "    #            - non_term_states: Set of non-terminal states in the environment\n",
    "    # OUTPUT: delta_V = sup_{s in S} |V_old(s)- V_new(s)|\n",
    "    delta_V = 0\n",
    "    for s in non_term_states:\n",
    "        delta_V = max(delta_V, abs(V_old[s]-V_new[s]))\n",
    "        \n",
    "    return delta_V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f2077d",
   "metadata": {},
   "source": [
    "### Policy improvement function\n",
    "\n",
    "The main things to keep in mind:\n",
    "* Create a dictionary for $Q_\\pi$.\n",
    "* Use the *max()* function to extract the argmax from $Q_\\pi(s,\\cdot)$ for each $s$. Syntax is as follows:\n",
    "\n",
    "                {argmax in dict} = max(dict, key = dict.get)\n",
    "* Working with $V_\\pi$ is more memory efficient than $Q_\\pi$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c71dc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## POLICY IMPROVEMENT - improve_policy() ##\n",
    "###########################################\n",
    "# 2022/04/22 - A. J. Zerouali\n",
    "# This function is called in the main loop of the policy iteration algorithm.\n",
    "\n",
    "def improve_policy(Pi, V_pi, P_trans, Rwds, adm_actions, non_term_states, term_states, gamma):\n",
    "    \n",
    "    \n",
    "    # Initialize policy_is_stable\n",
    "    policy_is_stable = True\n",
    "    \n",
    "    for s in non_term_states:\n",
    "        \n",
    "        # Store old action\n",
    "        a_old = list(Pi[s].keys())[0]\n",
    "        \n",
    "        # Initialize Vs_dict (dictionary for Q_pi(s,-))\n",
    "        Vs_dict = {}\n",
    "        \n",
    "        # Loop over admissible actions\n",
    "        for a in adm_actions[s]:\n",
    "            \n",
    "            V_temp = 0\n",
    "            \n",
    "            # Loop over non-zero probability transitions\n",
    "            # Evaluate new V_pi(s)\n",
    "            for s_ind in P_trans[(s,a)].keys(): \n",
    "                V_temp += P_trans[(s,a)].get(s_ind,0)*\\\n",
    "                            ( Rwds.get(s_ind,0) + gamma*V_pi[s_ind] )\n",
    "            # END FOR over s_ind\n",
    "            \n",
    "            # Store V_temp in Vs_dict\n",
    "            Vs_dict[a] = V_temp     \n",
    "            \n",
    "        # END FOR over a in adm_actions[s]\n",
    "        \n",
    "        # Get argmax\n",
    "        a_new = max(Vs_dict, key = Vs_dict.get)\n",
    "        \n",
    "        # Update policy with argmax:\n",
    "        Pi[s] = {a_new:1.0}\n",
    "        # Update V? Not necessary, gets evaluated again at beginning of loop\n",
    "        \n",
    "        # CLARIFY WHY THIS IS THE WAY\n",
    "        if a_old != a_new:\n",
    "            policy_is_stable = False\n",
    "        \n",
    "    # END FOR s in non_term_states\n",
    "        \n",
    "    return Pi, policy_is_stable\n",
    "###########################################\n",
    "## END OF improve_policy()               ##\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ef3a9",
   "metadata": {},
   "source": [
    "### Policy iteration function\n",
    "\n",
    "This function calls both the policy evaluation and policy improvement functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca7fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## POLICY ITERATION ALGORITHM ##\n",
    "################################\n",
    "# 2022/04/22, AJ Zerouali\n",
    "\n",
    "\n",
    "def Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma):\n",
    "    \n",
    "    \n",
    "\n",
    "    # Initialize counter and looping Boolean\n",
    "    N_iter = 0\n",
    "    policy_is_stable = True #Necessary?\n",
    "    \n",
    "    # Init. V_old\n",
    "    V_old = V_ini\n",
    "\n",
    "    # Loop until policy_is_stable = True\n",
    "    while True:\n",
    "        #######################\n",
    "        ## POLICY EVALUATION ##\n",
    "        #######################\n",
    "\n",
    "        # Execute policy eval function\n",
    "        V_new, k = iter_policy_eval(Pi, V_old, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "\n",
    "        # DEBUG:\n",
    "        print(f\"Policy evaluation fn iter_policy_eval() converged after {k} iterations.\")\n",
    "\n",
    "        ###########################################\n",
    "        ## POLICY IMPROVEMENT - improve_policy() ##\n",
    "        ###########################################\n",
    "\n",
    "        Pi, policy_is_stable = improve_policy(Pi, V_new, P_trans, Rwds, adm_actions, non_term_states, term_states, gamma)\n",
    "\n",
    "        # Break condition (Tricky)####\n",
    "        # Update policy iteration counter\n",
    "        N_iter += 1\n",
    "\n",
    "        # Compare value functions:\n",
    "        delta_V = compare_value_fns(V_old, V_new, non_term_states)\n",
    "        # Update value function\n",
    "        V_old = V_new\n",
    "\n",
    "        # BREAK WHILE condition\n",
    "        #if policy_is_stable or N_iter>30:\n",
    "        #    break\n",
    "        if policy_is_stable:\n",
    "            break\n",
    "        elif delta_V<=epsilon:\n",
    "            break\n",
    "\n",
    "    # END WHILE not policy_is_stable\n",
    "\n",
    "    # DEBUG/REMINDER: In function, should finish with\n",
    "    return V_new, Pi, N_iter\n",
    "\n",
    "# END DEF Policy_Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b633243",
   "metadata": {},
   "source": [
    "### Windy GridWorld with various penalties\n",
    "\n",
    "In this part I'm attempting to reproduce the results of Lecture 57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecaad7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windy GridWorld environment with penalty = 0.0 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  L  |  D  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  U  |  R  |  L  |  U  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 4 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy_Iteration() converged after 6 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.48| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.1 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  L  |  L  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 45 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 4 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 9 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.46| 0.00|-0.04| 0.00|\n",
      "------------------------\n",
      " 0.31| 0.18| 0.06|-0.04|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.2 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  U  |  R  |  L  |  U  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 52 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 3 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.43| 0.70| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.19| 0.00|-0.15| 0.00|\n",
      "------------------------\n",
      "-0.03|-0.23|-0.34|-0.50|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.4 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  R  |  D  |\n",
      "------------------------\n",
      "  D  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 58 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 4 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 6 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.05| 0.50| 1.00| 0.00|\n",
      "------------------------\n",
      "-0.36| 0.00|-0.25| 0.00|\n",
      "------------------------\n",
      "-0.72|-0.96|-0.62|-0.96|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.5 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  R  |  R  |  L  |  U  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 60 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 3 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-0.14| 0.40| 1.00| 0.00|\n",
      "------------------------\n",
      "-0.63| 0.00|-0.30| 0.00|\n",
      "------------------------\n",
      "-1.06|-1.19|-0.77|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -2 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  D  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 74 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 4 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 4 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-2.99|-1.10| 1.00| 0.00|\n",
      "------------------------\n",
      "-4.69| 0.00|-1.00| 0.00|\n",
      "------------------------\n",
      "-6.15|-4.61|-2.90|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Windy GridWorld with various penalties\n",
    "# 2022/04/22, AJ Zerouali\n",
    "### SIGNATURES:\n",
    "# windy_standard_grid(penalty=0)\n",
    "# Policy_Iteration(Pi_ini, V_ini, ---grid attributes---)\n",
    "# print_values(Val_fn, env)\n",
    "# print_policy(Val_fn, env)\n",
    "\n",
    "# Discount factor and error threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Penalty list\n",
    "penalties = [0.0, -0.1, -0.2, -0.4, -0.5, -2]\n",
    "\n",
    "# Loop over penalties\n",
    "for pen in penalties:\n",
    "    \n",
    "    # Create environment\n",
    "    grid = windy_standard_grid(penalty=pen)\n",
    "    print(f\"Windy GridWorld environment with penalty = {pen} created ... \\n\")\n",
    "\n",
    "    # Initialize policy\n",
    "    Pi = gen_random_policy(grid)\n",
    "\n",
    "    # Print optimal (deterministic) policy\n",
    "    print(f\"Printing initial policy ...\")\n",
    "    print_policy(Pi, grid)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Initialize value function\n",
    "    V_ini = {}\n",
    "    for s in (grid.non_term_states | grid.term_states):\n",
    "        V_ini[s] = 0\n",
    "\n",
    "    ##############################\n",
    "    ## EXECUTE POLICY ITERATION ##\n",
    "    ##############################\n",
    "\n",
    "    print(f\"Executing policy iteration algorithm ...\")\n",
    "    # SIGNATURE: Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "    (V_star, Pi_star, N_iter) = Policy_Iteration(Pi, V_ini, grid.transition_probs, grid.rewards, grid.adm_actions, \\\n",
    "                                                grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    ## PRINT RESULTS ##\n",
    "    ###################\n",
    "\n",
    "    # Print N_iter\n",
    "    # Print optimal value function\n",
    "    print(f\"Policy_Iteration() converged after {N_iter} iterations ...\\n\")\n",
    "\n",
    "    # Print optimal value function\n",
    "    print(f\"Printing optimal value function ...\")\n",
    "    #print_values(V_star, grid) #\n",
    "    print_values(V_star, grid)\n",
    "\n",
    "    # Print optimal (deterministic) policy\n",
    "    print(f\"Printing optimal policy ...\")\n",
    "    #print_policy(Pi_star, grid)\n",
    "    print_policy(Pi_star, grid)\n",
    "    \n",
    "    # Separator\n",
    "    print(\"_____________________________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021939f",
   "metadata": {},
   "source": [
    "## 4 - Value iteration\n",
    "\n",
    "**Lecture 58:**\n",
    "\n",
    "* From a practical standpoint, policy iteration consists of 2 nested loops in which we need to wait for 2 different quantities to converge. This is not computationally efficient when the environment contains large action and state spaces.\n",
    "\n",
    "* To improve the previous algorithm, there are 2 ideas we can use. Firstly, the policy is not really needed. Although we are interested in the argmax:\n",
    "\n",
    "    $\\pi_{k+1}(s) = \\arg\\max_a \\sum_{s'}T(s'|s,a)\\{r(s,a)+\\gamma V_k(s')\\},$\n",
    "    \n",
    "    what we really use is the quantity:\n",
    "    \n",
    "    $V_{k+1}(s) = \\max_a \\sum_{s'}T(s'|s,a)\\{r(s,a)+\\gamma V_k(s')\\}.$\n",
    "\n",
    "    The second idea is to limit the number of policy evaluation iterations. (**Comment:** I'm not sure that I fully get this point, I would guess that instead of re-computing the value function of a policy at each iteration, we could work with a stored value function, find the argmax at each given $s\\in\\mathcal S$, and update the value function in that given state.)\n",
    "\n",
    "* Here's the pseudocode of value iteration:\n",
    "\n",
    "       Initalize V(s) with V(s_terminal) = 0, fix threshold epsilon\n",
    "        Loop:\n",
    "            Delta = 0\n",
    "            for s in non_term_states:\n",
    "                V_old = V(s)\n",
    "                V(s) = max_a sum_s' T(s'|s,a)[r+gamma*V(s')]\n",
    "                Delta = max(Delta, |V_old - V(s)|)\n",
    "            if Delta <epsilon:\n",
    "                break\n",
    "        \n",
    "        for s in non_term_states:\n",
    "            pi*(s)=argmax_a sum_s' T(s'|s,a)[r+gamma*V(s')]\n",
    "\n",
    "    Notice here that the first \"while\" loop computes $V^\\ast = V_{\\pi^\\ast}$, and the second \"for\" loop extracts the optimal policy $\\pi^\\ast$.\n",
    "\n",
    "* The main advantage of value iteration is that we have only one infinite loop. One issue that could arise is when there's more than one optimal policy.\n",
    "\n",
    "* Interesting point: Value iteration was proposed by Bellman in 1957. The crux is that the RHS of the Bellman optimality equation is treated as an update rule.\n",
    "\n",
    "**Lecture 59: Value iteration in code**\n",
    "\n",
    "* After writing my own implementation (without looking at lecture 59), I re-executed the previous cases studied: Windy GridWorld with various penalties and non-WindyGridworld. I obtained the same results as before.\n",
    "\n",
    "* Note that Value Iteration considerably reduces the execution time.\n",
    "\n",
    "* The next subsection contains my implementation of value iteration. The next ones are where I do the tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c137b",
   "metadata": {},
   "source": [
    "### a) Value Iteration Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c81a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## VALUE ITERATION ALGORITHM  ##\n",
    "################################\n",
    "## 2022/04/24, AJ Zerouali\n",
    "# This is Bellman's famous algorithm of 1957.\n",
    "# REMARK: Check the break condition is correct.\n",
    "\n",
    "def Value_Iteration(env, epsilon, gamma):\n",
    "    # ARGUMENTS: - Pi: Necessary?\n",
    "    #            - env: Environment. Gives the state and action spaces.\n",
    "    #            - epsilon: Convergence threshold \n",
    "    #            - gamma: Discount factor\n",
    "    # OUTPUT:    - V_star: Optimal value function\n",
    "    #            - Pi_star: Optimal policy\n",
    "    #            - N_iter: No. of iterations \n",
    "    # NOTE: Pi_star is obtained from the actions that gave the last update of V_new = V_star\n",
    "    \n",
    "    # Initialize env. attributes\n",
    "    term_states = env.term_states\n",
    "    non_term_states = env.non_term_states\n",
    "    P_trans = env.transition_probs\n",
    "    Rwds = env.rewards\n",
    "    adm_actions = env.adm_actions\n",
    "    \n",
    "    # Initialize V and Q to zero\n",
    "    V_new = {}\n",
    "    Q = {}\n",
    "    for s in term_states:\n",
    "        V_new[s] = 0.0\n",
    "    for s in non_term_states:\n",
    "        V_new[s] = 0.0\n",
    "        Q[s] = {}\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            \n",
    "    # DEBUG: Initialize Pi_star\n",
    "    Pi_star = {}\n",
    "    \n",
    "    # Init. iteration counter\n",
    "    N_iter = 0\n",
    "    \n",
    "    ## MAIN LOOP\n",
    "    while True:\n",
    "        \n",
    "        V_old = V_new\n",
    "        \n",
    "        Delta_V = 0.0\n",
    "        \n",
    "        for s in non_term_states:\n",
    "            \n",
    "            # Store V_old(s)\n",
    "            Vs_old = V_old[s]\n",
    "            \n",
    "            # This loop computes V(s)\n",
    "            for a in adm_actions[s]:\n",
    "                \n",
    "                # Init. Q_sa\n",
    "                Q_sa = 0.0\n",
    "                \n",
    "                # This loop computes Q(s,a)\n",
    "                # Loop only over non-zero probability transitions\n",
    "                for s_ind in P_trans[(s,a)].keys():\n",
    "                    # Bellman update\n",
    "                    # Template: V_temp += P_trans[(s,a)].get(s_ind,0)*( Rwds.get(s_ind,0) + gamma*V_pi[s_ind] )\n",
    "                    Q_sa += P_trans[(s,a)].get(s_ind,0)*( Rwds.get(s_ind,0) + gamma*V_old[s_ind] )\n",
    "                \n",
    "                # Update Q(s,a)\n",
    "                Q[s][a] = Q_sa\n",
    "                \n",
    "            # END FOR a in adm_actions[s]\n",
    "            \n",
    "            # Get max over a's\n",
    "            V_new[s] = max(Q[s].values())\n",
    "            \n",
    "            # Pi_star debug:\n",
    "            # Store argmax\n",
    "            a_star = max(Q[s], key = Q[s].get)\n",
    "            Pi_star[s] = {a_star:1.0}\n",
    "            \n",
    "            # Update Delta_V\n",
    "            Delta_V = max(Delta_V, abs(Vs_old - V_new[s]))\n",
    "            \n",
    "        # END FOR s in non_term_states\n",
    "        \n",
    "        if Delta_V < epsilon:\n",
    "            break\n",
    "        \n",
    "        # Update iteration counter\n",
    "        N_iter += 1\n",
    "        \n",
    "    # END WHILE\n",
    "    \n",
    "    # Return optimal value fn and no. of iterations\n",
    "    return V_new, N_iter, Pi_star\n",
    "\n",
    "# END DEF Value_Iteration()\n",
    "\n",
    "\n",
    "################################\n",
    "##      FIND OPTIMAL POLICY   ##\n",
    "################################\n",
    "## This function simply extracts argmaxes.\n",
    "## Should necessarily be executed after value iteration.\n",
    "\n",
    "def Get_Pi_Star(V_star, env, epsilon, gamma):\n",
    "    # ARGUMENTS: - V_star: Optimal value function\n",
    "    #            - env: Environment. Gives the state and action spaces.\n",
    "    #            - epsilon: Convergence threshold \n",
    "    #            - gamma: Discount factor\n",
    "    # OUTPUT:    - Pi := Pi_star, optimal policy\n",
    "    \n",
    "    # Init. env. attributes\n",
    "    P_trans = env.transition_probs\n",
    "    Rwds = env.rewards\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "\n",
    "    # Init. Q and Pi (output)\n",
    "    Pi = {}\n",
    "    Q = {}\n",
    "    for s in non_term_states:\n",
    "        Pi[s] = {}\n",
    "        Q[s] = {}\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "    \n",
    "    # Here you should compute Q(s,a) then extract argmax\n",
    "    # Recall argmax for dictionary given by\n",
    "    # a_new = max(Vs_dict, key = Vs_dict.get)\n",
    "    for s in non_term_states:\n",
    "        \n",
    "        for a in adm_actions[s]:\n",
    "            \n",
    "            Q_sa = 0.0\n",
    "            \n",
    "            # Loop over non-zero transitions\n",
    "            for s_ind in P_trans[(s,a)].keys():\n",
    "                \n",
    "                # Bellman equation\n",
    "                Q_sa += P_trans[(s,a)].get(s_ind,0)*(Rwds.get(s_ind, 0)+gamma*V_star[s_ind])\n",
    "                \n",
    "            # END FOR s_ind in admissible\n",
    "            \n",
    "            # Store above sum\n",
    "            Q[s][a] = Q_sa\n",
    "            \n",
    "        # END FOR a in adm_actions[s]\n",
    "        \n",
    "        # Get argmax and store in Pi\n",
    "        a_star = max(Q[s], key = Q[s].get)\n",
    "        Pi[s] = {a_star:1.0}\n",
    "        #Pi[s] = {max(Q[s], key = Q[s].get):1.0}\n",
    "        \n",
    "    # END FOR s in non_term_states    \n",
    "    \n",
    "    # Return optimal policy\n",
    "    return Pi\n",
    "\n",
    "# END DEF Value_Iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb014240",
   "metadata": {},
   "source": [
    "### b) Non-windy GridWorld\n",
    "\n",
    "The optimal value function is:\n",
    "\n",
    "        Value function\n",
    "        \n",
    "        ------------------------\n",
    "         0.81| 0.90| 1.00| 0.00|\n",
    "        ------------------------\n",
    "         0.73| 0.00| 0.90| 0.00|\n",
    "        ------------------------\n",
    "         0.66| 0.73| 0.81| 0.73|\n",
    "        ------------------------\n",
    "        \n",
    "The optimal policy is:\n",
    "\n",
    "        ##  POLICY  ##\n",
    "        ------------------------\n",
    "          R  |  R  |  R  |\n",
    "        ------------------------\n",
    "          U  |     |  U  |\n",
    "        ------------------------\n",
    "          U  |  R  |  U  |  L  |\n",
    "        ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10c219d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test with non-windy GridWorld environment ... \n",
      "\n",
      "Executing value iteration algorithm ...\n",
      "Value_Iteration() converged after 3 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "------------------------\n",
      "Printing policy obtained from Value_Iteration()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n",
      "Printing policy obtained from Get_Pi_star()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "##### (NON)WINDY GRIDWORLD (TEST) #####\n",
    "# Updated: 22/04/24, A. J. Zerouali\n",
    "# Test with non-windy case and deterministic policy\n",
    "# Adapted from previous cell for Value Iteration.\n",
    "\n",
    "def test_standard_grid():\n",
    "    # Start at bottom left (randomize later)\n",
    "    ini_state = (2,0)\n",
    "    # Action space \n",
    "    ACTION_SPACE = {\"U\", \"D\", \"L\", \"R\"}\n",
    "    # Non terminal states\n",
    "    NON_TERMINAL_STATES = {(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2), (2,3)}\n",
    "    # Terminal states\n",
    "    TERMINAL_STATES = {(0,3), (1,3)}\n",
    "    \n",
    "    # Instantiate:\n",
    "    # \n",
    "    env = GridWorld_Windy_small(3, 4, ini_state, NON_TERMINAL_STATES, TERMINAL_STATES, ACTION_SPACE)\n",
    "\n",
    "    \n",
    "    # Dictionary of rewards\n",
    "    # Not storing 0s\n",
    "    rewards = {(0,3):1, (1,3): -1}\n",
    "    \n",
    "    # Dictionary of admissible actions per state\n",
    "    adm_actions = {\n",
    "        (0,0): (\"D\", \"R\"),\n",
    "        (0,1): (\"L\", \"R\"),\n",
    "        (0,2): (\"L\", \"R\", \"D\"),\n",
    "        (1,0): (\"D\", \"U\"),\n",
    "        (1,2): (\"U\", \"D\", \"R\"),\n",
    "        (2,0): (\"U\", \"R\"),\n",
    "        (2,1): (\"L\", \"R\"),\n",
    "        (2,2): (\"U\", \"R\", \"L\"),\n",
    "        (2,3): (\"U\", \"L\"),\n",
    "    }\n",
    "    \n",
    "    # Dictionary of deterministic transitions:\n",
    "    transition_probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        \n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        \n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        \n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        \n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        \n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        \n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((1, 2), 'U'): {(0, 2): 1.0},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "    }\n",
    "    \n",
    "    # Assign missing environment attributes\n",
    "    env.set(rewards, adm_actions, transition_probs)\n",
    "    \n",
    "    # Output line\n",
    "    return env\n",
    "\n",
    "# Create environment\n",
    "# adm_actions, rewards and transition_probs are attributes of grid\n",
    "\n",
    "grid = test_standard_grid()\n",
    "\n",
    "print(f\"Test with non-windy GridWorld environment ... \\n\")\n",
    "\n",
    "# Discount factor and error threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "##############################\n",
    "## EXECUTE VALUE ITERATION  ##\n",
    "##############################\n",
    "\n",
    "print(f\"Executing value iteration algorithm ...\")\n",
    "# SIGNATURE: V_star, N_iter, Pi_star = Value_Iteration(env, epsilon, gamma)\n",
    "(V_star, N_iter, Pi_star) = Value_Iteration(grid, epsilon, gamma)\n",
    "\n",
    "##############################\n",
    "##  GET OPTIMAL POLICY      ##\n",
    "##############################\n",
    "\n",
    "# SIGNATURE: Pi_star = Get_Pi_Star(V_star, env, epsilon, gamma)\n",
    "Pi_computed = Get_Pi_Star(V_star, grid, epsilon, gamma)\n",
    "\n",
    "\n",
    "###################\n",
    "## PRINT RESULTS ##\n",
    "###################\n",
    "\n",
    "# Print N_iter\n",
    "# Print optimal value function\n",
    "print(f\"Value_Iteration() converged after {N_iter} iterations ...\\n\")\n",
    "\n",
    "# Print optimal value function\n",
    "print(f\"Printing optimal value function ...\")\n",
    "print_values(V_star, grid)\n",
    "\n",
    "# Print optimal (deterministic) policy\n",
    "print(f\"Printing policy obtained from Value_Iteration()...\")\n",
    "print_policy(Pi_star, grid)\n",
    "\n",
    "# Print optimal (deterministic) policy\n",
    "print(f\"Printing policy obtained from Get_Pi_star()...\")\n",
    "print_policy(Pi_computed, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f1b13",
   "metadata": {},
   "source": [
    "### c) Windy GridWorld with various penalties\n",
    "\n",
    "Here I am redoing the cases of Lecture 57 with Value iteration. We get the same results with much faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f7d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windy GridWorld environment with penalty = 0.0 created ... \n",
      "\n",
      "Executing value iteration algorithm ...\n",
      "Value_Iteration() converged after 5 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.48| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------------\n",
      "Printing policy obtained from Value_Iteration()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "Printing policy obtained from Get_Pi_star()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.1 created ... \n",
      "\n",
      "Executing value iteration algorithm ...\n",
      "Value_Iteration() converged after 5 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.46| 0.00|-0.04| 0.00|\n",
      "------------------------\n",
      " 0.31| 0.18| 0.06|-0.04|\n",
      "------------------------\n",
      "Printing policy obtained from Value_Iteration()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "Printing policy obtained from Get_Pi_star()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.2 created ... \n",
      "\n",
      "Executing value iteration algorithm ...\n",
      "Value_Iteration() converged after 4 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.43| 0.70| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.19| 0.00|-0.15| 0.00|\n",
      "------------------------\n",
      "-0.03|-0.23|-0.34|-0.50|\n",
      "------------------------\n",
      "Printing policy obtained from Value_Iteration()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "------------------------\n",
      "Printing policy obtained from Get_Pi_star()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.4 created ... \n",
      "\n",
      "Executing value iteration algorithm ...\n",
      "Value_Iteration() converged after 3 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.05| 0.50| 1.00| 0.00|\n",
      "------------------------\n",
      "-0.36| 0.00|-0.25| 0.00|\n",
      "------------------------\n",
      "-0.72|-0.96|-0.62|-0.96|\n",
      "------------------------\n",
      "Printing policy obtained from Value_Iteration()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n",
      "Printing policy obtained from Get_Pi_star()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.5 created ... \n",
      "\n",
      "Executing value iteration algorithm ...\n",
      "Value_Iteration() converged after 3 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-0.14| 0.40| 1.00| 0.00|\n",
      "------------------------\n",
      "-0.63| 0.00|-0.30| 0.00|\n",
      "------------------------\n",
      "-1.06|-1.19|-0.77|-1.00|\n",
      "------------------------\n",
      "Printing policy obtained from Value_Iteration()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "Printing policy obtained from Get_Pi_star()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -2 created ... \n",
      "\n",
      "Executing value iteration algorithm ...\n",
      "Value_Iteration() converged after 3 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-2.99|-1.10| 1.00| 0.00|\n",
      "------------------------\n",
      "-4.69| 0.00|-1.00| 0.00|\n",
      "------------------------\n",
      "-6.15|-4.61|-2.90|-1.00|\n",
      "------------------------\n",
      "Printing policy obtained from Value_Iteration()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "Printing policy obtained from Get_Pi_star()...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Windy GridWorld with various penalties\n",
    "## This time using Value Iteration Algorithm instead of Policy Iteration\n",
    "# 2022/04/24\n",
    "\n",
    "# Discount factor and error threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Penalty list\n",
    "penalties = [0.0, -0.1, -0.2, -0.4, -0.5, -2]\n",
    "\n",
    "# Loop over penalties\n",
    "for pen in penalties:\n",
    "    \n",
    "    # Create environment\n",
    "    grid = windy_standard_grid(penalty=pen)\n",
    "    print(f\"Windy GridWorld environment with penalty = {pen} created ... \\n\")\n",
    "\n",
    "    ##############################\n",
    "    ## EXECUTE VALUE ITERATION  ##\n",
    "    ##############################\n",
    "\n",
    "    print(f\"Executing value iteration algorithm ...\")\n",
    "    # SIGNATURE: V_star, N_iter, Pi_star = Value_Iteration(env, epsilon, gamma)\n",
    "    (V_star, N_iter, Pi_star) = Value_Iteration(grid, epsilon, gamma)\n",
    "\n",
    "    ##############################\n",
    "    ##  GET OPTIMAL POLICY      ##\n",
    "    ##############################\n",
    "\n",
    "    # SIGNATURE: Pi_star = Get_Pi_Star(V_star, env, epsilon, gamma)\n",
    "    Pi_computed = Get_Pi_Star(V_star, grid, epsilon, gamma)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    ## PRINT RESULTS ##\n",
    "    ###################\n",
    "\n",
    "    # Print N_iter\n",
    "    # Print optimal value function\n",
    "    print(f\"Value_Iteration() converged after {N_iter} iterations ...\\n\")\n",
    "\n",
    "    # Print optimal value function\n",
    "    print(f\"Printing optimal value function ...\")\n",
    "    print_values(V_star, grid)\n",
    "\n",
    "    # Print optimal (deterministic) policy\n",
    "    print(f\"Printing policy obtained from Value_Iteration()...\")\n",
    "    print_policy(Pi_star, grid)\n",
    "\n",
    "    # Print optimal (deterministic) policy\n",
    "    print(f\"Printing policy obtained from Get_Pi_star()...\")\n",
    "    print_policy(Pi_computed, grid)\n",
    "    \n",
    "    # Separator\n",
    "    print(\"_____________________________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58457fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdb0509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da606589",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b8df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
