{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef997f52",
   "metadata": {},
   "source": [
    "# **Temporal Difference Learning (Basics)**\n",
    "### 2022/05/04, A. J. Zerouali\n",
    "\n",
    " \n",
    "\n",
    "## **1 - Introduction**\n",
    "\n",
    "### a) Contents\n",
    "\n",
    "This is Section 7 (Lect. 69-76) of Lazy Programmer's introductory RL course. It's divided into the following parts:\n",
    "\n",
    "1) TD(0) Prediction: Lectures 70-71\n",
    "\n",
    "2) SARSA: Lectures 72-73.\n",
    "\n",
    "3) Q-Learning: : Lect 74-75.\n",
    "\n",
    "Some notes from lecture 69:\n",
    "\n",
    "* TD learning is better adapted to continuing tasks compared to dynamic programming and Monte Carlo.\n",
    "\n",
    "* In DP we used boostrapping, in the sense that to obtain a new estimate of our target, we used the previous estimate. In contrast, MC only averaged all the estimates.\n",
    "\n",
    "* Temporal difference borrows ideas from both approaches. Like MC it is sample-based, but the estimates are done by bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd3f78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### IMPORTANT: ALWAYS EXECUTE THIS CELL FIRST #####\n",
    "#####################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Windy GridWorld class and helper functions\n",
    "from Windy_GridWorld import GridWorld_Windy_small, windy_standard_grid, test_standard_grid\n",
    "\n",
    "# Import RL functions: printing functions (for value fn and policy), random policy generators, value fn comparator.\n",
    "from RL_Fns_Windy_GridWorld import print_values, print_policy, gen_random_policy, gen_random_epslnsoft_policy, compare_value_fns\n",
    "\n",
    "# Import MC policy evaluation\n",
    "from Monte_Carlo_Windy_GridWorld import MC_Stochastic_Policy_Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c94949",
   "metadata": {},
   "source": [
    "## **2 - TD(0) prediction:**\n",
    "\n",
    "The summary of lecture 70 is as follows:\n",
    "\n",
    "* When implementing MC, we used the sample mean $V_k(s)=\\frac{1}{k}\\sum_{i=1}^{k}R_{s,i}$ (at sample $k$). A more efficient way of implementing this computation is to use the $(k-1)$ sample:\n",
    "\n",
    "$$V_k(s) = V_{k-1}(s)+\\frac{1}{k}\\left(R_{s,k}-V_{k-1}(s)\\right).$$\n",
    "\n",
    "* To account for long episodes (when simulating infinity), we view $\\frac{1}{k}$ as a learning rate, and replace it with a coefficient $\\alpha\\in]0,1]$. Lazy Programmer describes this as an \"exponentially weighted moving average\", Sutton and Barto call it the *step size* constant.\n",
    "\n",
    "* Going to the temporal difference update now, instead of considering the cumulative returns $R_{s,i}$, we boostrap with our previous estimate $V_{k-1}$, so that our update becomes:\n",
    "\n",
    "$$V_k(s) = V_{k-1}(s)+\\alpha\\left\\{r(s,a,s')+\\gamma V_{k-1}(s')-V_{k-1}(s)\\right\\}.$$\n",
    "\n",
    "* Note that as long as there is a one-step change of state, we can make this update with the reward $ r(s_t,a_t,s_{t+1})=r(s_{t+1}) $\n",
    "\n",
    "* **Question:** How do we choose the step size $\\alpha$? What does it really represent?\n",
    "\n",
    "* Now we turn to the pseudocode of TD(0) prediction:\n",
    "\n",
    "            Require: Policy Pi to be evaluated.\n",
    "            Init. V(s) for all states s.\n",
    "            While not converged:\n",
    "                s = env.reset()\n",
    "                while s non terminal:\n",
    "                    Sample a following Pi(-|s)\n",
    "                    Update r and s' following (s,a)\n",
    "                    Update V: V[s]=V[s]+alpha*(r[s,a,s']+gamma*V[s']-V[s])\n",
    "                    Set s=s'\n",
    "                    \n",
    "* Note that we are not generating an entire sample episode before estimating $V$. We are updating the latter at each new step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe01b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd8a93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "582b23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TD_0_Prediction(Pi, env, gamma, alpha, N_samples, T_max, epsilon):\n",
    "    '''\n",
    "    Function implementing TD(0) policy evaluation.<\n",
    "    \n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - alpha: Step size parameter, float in ]0,1[.\n",
    "                - N_samples: Max. no. of sample episodes.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - epsilon: Floar, conv. threshold\n",
    "                \n",
    "     OUTPUT:    - V:=V_Pi, value function obtained.\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    adm_actions = env.adm_actions\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Init. V\n",
    "    V = {}\n",
    "    for s in term_states:\n",
    "        V[s]= 0.0\n",
    "    for s in non_term_states:\n",
    "        V[s]= 0.0\n",
    "        \n",
    "    # Init. counters\n",
    "    N_iter = 0\n",
    "    T = 0\n",
    "    deltas = []\n",
    "    \n",
    "        \n",
    "    # Sample episode loop\n",
    "    while (N_iter<N_samples):\n",
    "        \n",
    "        # Generate s_0 randomly\n",
    "        s_old = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        # Lazy Programmer always starts at (2,0)\n",
    "        #s_old = (2, 0)\n",
    "        \n",
    "        # Init. delta_V\n",
    "        delta_V = 0\n",
    "        \n",
    "        # Episode steps loop\n",
    "        # Note: Should I really impose this max episode length?\n",
    "        while (s_old not in term_states) and (T<T_max):\n",
    "            \n",
    "            # Get random index and \n",
    "            # probs = list(Pi[s_old].values())\n",
    "            ind_rand = np.random.choice(len(Pi[s_old].values()), p=list(Pi[s_old].values()))\n",
    "            # Sample a following Pi(-|s_old)\n",
    "            a = list(Pi[s_old].keys())[ind_rand]\n",
    "            \n",
    "            # WARNING: Modify for other environments\n",
    "            # Compute new state\n",
    "            if a == 'U':\n",
    "                s_new = (s_old[0]-1, s_old[1])\n",
    "            elif a == 'D':\n",
    "                s_new = (s_old[0]+1, s_old[1])\n",
    "            elif a == 'L':\n",
    "                s_new = (s_old[0], s_old[1]-1)\n",
    "            elif a == 'R':\n",
    "                s_new = (s_old[0], s_old[1]+1)\n",
    "            \n",
    "            # Get reward\n",
    "            r_new = Rwds.get(s_new,0)\n",
    "            \n",
    "            # Update V[s]\n",
    "            V_old = V[s_old]\n",
    "            V[s_old] = V[s_old]+ alpha*(r_new+gamma*V[s_new] -V[s_old])\n",
    "            \n",
    "            # Update delta_V\n",
    "            delta_V = max(delta_V, np.abs(V_old-V[s_old]))\n",
    "            \n",
    "            \n",
    "            # Update state for next step\n",
    "            s_old = s_new\n",
    "            \n",
    "            # Update T\n",
    "            T += 1\n",
    "            \n",
    "        # Update deltas\n",
    "        deltas.append(delta_V)\n",
    "        \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "        \n",
    "        # END WHILE over episode steps     \n",
    "    # END WHILE over samples    \n",
    "\n",
    "    \n",
    "    return V, N_iter, deltas\n",
    "\n",
    "# END DEF TD_0_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec997942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running TD_0_Prediction with N_iter=90000 samples.\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.02| 0.02| 0.81| 0.00|\n",
      "------------------------\n",
      " 0.04| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.00| 0.00|-0.73| 0.00|\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nMonte Carlo evaluations of policies above:\\n\\n## Value function of Pi_eps_ini, eps = 0.1 ##\\nN_samples = 10000, T_max = 50\\n------------------------\\n 0.79| 0.88| 0.99| 0.00|\\n------------------------\\n 0.70| 0.00| 0.83| 0.00|\\n------------------------\\n 0.63| 0.65| 0.72| 0.63|\\n------------------------\\n\\n## Value function of Pi_opt ##\\nN_samples = 10000, T_max = 50\\n------------------------\\n 0.81| 0.90| 1.00| 0.00|\\n------------------------\\n 0.73| 0.00| 0.90| 0.00|\\n------------------------\\n 0.66| 0.73| 0.81| 0.73|\\n------------------------\\n\\n## Value function of Pi_unif ##\\nN_samples = 10000, T_max = 200\\n------------------------\\n 0.05| 0.13| 0.25| 0.00|\\n------------------------\\n-0.03| 0.00|-0.27| 0.00|\\n------------------------\\n-0.09|-0.19|-0.38|-0.77|\\n------------------------\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Discount factor, epsilon-greedy probability\n",
    "eps = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Policies\n",
    "Pi_eps_ini = {\n",
    "    (2, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 0)]), 'R':eps/len(grid.adm_actions[(2, 0)])},\n",
    "    (1, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 0)]), 'D':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 0): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 0)]), 'D':eps/len(grid.adm_actions[(0, 0)])},\n",
    "    (0, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 1)]), 'L':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 2): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 2)]), 'D':eps/len(grid.adm_actions[(0, 2)]),\\\n",
    "                                                             'L':eps/len(grid.adm_actions[(0, 2)])},\n",
    "    (1, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 2)]), 'D':eps/len(grid.adm_actions[(1, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(1, 2)])},\n",
    "    (2, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(2, 1)]), 'L':eps/len(grid.adm_actions[(2, 1)])},\n",
    "    (2, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 2)]), 'L':eps/len(grid.adm_actions[(2, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(2, 2)])},\n",
    "    (2, 3): {'L': (1-eps)+eps/len(grid.adm_actions[(2, 3)]), 'U':eps/len(grid.adm_actions[(2, 3)])},\n",
    "  }\n",
    "\n",
    "Pi_unif = {}\n",
    "for s in grid.non_term_states:\n",
    "    Pi_unif[s]={}\n",
    "    for a in grid.adm_actions[s]:\n",
    "        Pi_unif[s][a] = 1/len(grid.adm_actions[s])\n",
    "\n",
    "Pi_opt = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "# Initialize other algorithm arguments:\n",
    "T_max = 200\n",
    "N_samples = 90000\n",
    "alpha = 1\n",
    "eps = 0.05\n",
    "\n",
    "# Evaluate policy with TD(0) prediction\n",
    "V, N_iter, deltas = TD_0_Prediction(Pi_unif, grid, gamma, alpha, N_samples, T_max, epsilon)\n",
    "\n",
    "print(f\"Finished running TD_0_Prediction with N_iter={N_iter} samples.\")\n",
    "\n",
    "# Print the value function\n",
    "print_values(V, grid)\n",
    "\n",
    "# Plot the deltas\n",
    "plt.plot(deltas)\n",
    "\n",
    "'''\n",
    "Monte Carlo evaluations of policies above:\n",
    "\n",
    "## Value function of Pi_eps_ini, eps = 0.1 ##\n",
    "N_samples = 10000, T_max = 50\n",
    "------------------------\n",
    " 0.79| 0.88| 0.99| 0.00|\n",
    "------------------------\n",
    " 0.70| 0.00| 0.83| 0.00|\n",
    "------------------------\n",
    " 0.63| 0.65| 0.72| 0.63|\n",
    "------------------------\n",
    "\n",
    "## Value function of Pi_opt ##\n",
    "N_samples = 10000, T_max = 50\n",
    "------------------------\n",
    " 0.81| 0.90| 1.00| 0.00|\n",
    "------------------------\n",
    " 0.73| 0.00| 0.90| 0.00|\n",
    "------------------------\n",
    " 0.66| 0.73| 0.81| 0.73|\n",
    "------------------------\n",
    "\n",
    "## Value function of Pi_unif ##\n",
    "N_samples = 10000, T_max = 200\n",
    "------------------------\n",
    " 0.05| 0.13| 0.25| 0.00|\n",
    "------------------------\n",
    "-0.03| 0.00|-0.27| 0.00|\n",
    "------------------------\n",
    "-0.09|-0.19|-0.38|-0.77|\n",
    "------------------------\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1265942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ef1a3b2160>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(deltas[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ebe595",
   "metadata": {},
   "source": [
    "for the policy used above, MC policy evaluation yields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56fdb6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.04| 0.13| 0.25| 0.00|\n",
      "------------------------\n",
      "-0.04| 0.00|-0.24| 0.00|\n",
      "------------------------\n",
      "-0.09|-0.18|-0.37|-0.76|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "V_MC = MC_Stochastic_Policy_Eval(Pi_unif, grid, gamma, 10000, 50, all_visits_MC= False)\n",
    "print_values(V_MC, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbb4ea1",
   "metadata": {},
   "source": [
    "This algorithm doesn't give accurate estimations. These depend on the step size $\\alpha$, the starting state of the episodes, and differ significantly from one execution to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53c039",
   "metadata": {},
   "source": [
    "## **3 - SARSA:**\n",
    "\n",
    "SARSA stands for updates using tuples $(s,a,r,s',a')$. The basics are explained in Lecture 72:\n",
    "\n",
    "* SARSA is an on-policy control TD learning algorithm. As seen in MC, we now estimate $Q_\\pi$ instead of $V_\\pi$.\n",
    "\n",
    "* The SARSA update at step $t$ is:\n",
    "\n",
    "$$Q(s_t, a_t) = Q(s_t, a_t)+\\alpha\\left\\{r(s_t,a_t,s_{t+1})+\\gamma Q(s_{t+1},a_{t+1})-Q(s_t, a_t)\\right\\}.$$\n",
    "\n",
    "* It's worth noting that this control algorithm is still inspired by generalized policy iteration, and that the update above is just the $Q$ version of the TD(0) prediction update.\n",
    "\n",
    "* Typically, we expect an $\\varepsilon$-greedy policy as input for SARSA, as these are good candidates for the explore-exploit trade-off. An easy to remember explanation is that we want to assign the highest probability to the best action, given that this will maximize the returns, but at the same time we want to collect as many samples as possible by having actions of non-zero probabilies, and hence get more accurate estimates of state-action calues.\n",
    "\n",
    "* Pseudocode:\n",
    "\n",
    "            Init. Q(s,a) randomly for all (s,a).\n",
    "            while not converged:\n",
    "                s = env.reset()\n",
    "                a = epsilon_greedy{Q(s,-)}\n",
    "                while s not terminal:\n",
    "                    Compute s' and r=r(s,a,s')\n",
    "                    a' = epsilon_greedy{Q(s',-)}\n",
    "                    Update: Q(s,a) = Q(s,a) +alpha*(r+gamma*Q(s',a')-Q(s,a))\n",
    "                    Update: (s,a) = (s',a')\n",
    "\n",
    "* Note that in principle, we shouldn't be evaluating Q from an input policy, as we're aiming to get a policy from Q.\n",
    "\n",
    "* Lazy Programmer doesn't define a convergence criterion for the sample episode loop, he just gives a maximal number of samples. Another peculiarity is that even in Sutton and Barto, the pseudocode doesn't explicitly refer to storing the optimal policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6003a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA_Alg(Pi, env, gamma, alpha, N_samples, T_max, epsilon):\n",
    "    '''\n",
    "    Function implementing the Q-learning algorithm for TD control.\n",
    "    (Pi should be an epsilon-greedy policy according to instructor)\n",
    "    In principle, the input/behavior policy Pi is epsilon greedy,\n",
    "    while the output/target policy Pi_star is greedy.\n",
    "    I'm starting with a \n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - alpha: Step size parameter, float in ]0,1[.\n",
    "                - N_samples: Max. no. of sample episodes.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - epsilon: Floar, conv. threshold\n",
    "                \n",
    "     OUTPUT:    - V:=V_Pi, value function obtained.\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    adm_actions = env.adm_actions\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Init. Q and Pi_star\n",
    "    Q = {}\n",
    "    Pi_star = {}\n",
    "    for s in term_states:\n",
    "        Q[s]= {\"\":0.0}\n",
    "    for s in non_term_states:\n",
    "        Q[s] = {}\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            # I'm not sure about this last line...\n",
    "    # Should output a policy where Pi_star[s] = {a_star(s):1.0}\n",
    "        \n",
    "    # Init. counters\n",
    "    N_iter = 0\n",
    "    T = 0\n",
    "    deltas = []\n",
    "    \n",
    "        \n",
    "    # Sample episode loop\n",
    "    while (N_iter<N_samples):\n",
    "        \n",
    "        # Generate s_0 randomly\n",
    "        s_old = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        # Lazy Programmer always starts at (2,0)\n",
    "        #s_old = (2, 0)\n",
    "        if s_old in non_term_states:\n",
    "            ind_rand = np.random.choice(len(Pi[s_old].values()), p=list(Pi[s_old].values()))\n",
    "            a_old = list(Pi[s_old].keys())[ind_rand]\n",
    "        else:\n",
    "            a_old = \"\"\n",
    "        \n",
    "        # Init. delta_V\n",
    "        delta_Q = 0\n",
    "        \n",
    "        # Episode steps loop\n",
    "        # Note: Should I really impose this max episode length?\n",
    "        while (s_old not in term_states) and (T<T_max):\n",
    "            \n",
    "            # WARNING: Modify for other environments\n",
    "            # Compute new state\n",
    "            if a_old == 'U':\n",
    "                s_new = (s_old[0]-1, s_old[1])\n",
    "            elif a_old == 'D':\n",
    "                s_new = (s_old[0]+1, s_old[1])\n",
    "            elif a_old == 'L':\n",
    "                s_new = (s_old[0], s_old[1]-1)\n",
    "            elif a_old == 'R':\n",
    "                s_new = (s_old[0], s_old[1]+1)\n",
    "            \n",
    "            if s_new in non_term_states:\n",
    "                # Get random index and \n",
    "                # probs = list(Pi[s_old].values())\n",
    "                ind_rand = np.random.choice(len(Pi[s_new].values()), p=list(Pi[s_new].values()))\n",
    "                # Sample action following Pi(-|s_old)\n",
    "                a_new = list(Pi[s_new].keys())[ind_rand]\n",
    "            else:\n",
    "                a_new = \"\"\n",
    "            \n",
    "            # Get reward\n",
    "            r_new = Rwds.get(s_new,0)\n",
    "            \n",
    "            # Update Q(s,a)\n",
    "            Q[s_old][a_old] = Q[s_old][a_old]+ alpha*(r_new + gamma*Q[s_new][a_new]  - Q[s_old][a_old])\n",
    "            \n",
    "            # Update state for next step\n",
    "            s_old = s_new\n",
    "            a_old = a_new\n",
    "            \n",
    "            # Update T\n",
    "            T += 1\n",
    "            \n",
    "        # Update deltas\n",
    "        #deltas.append(delta_V)\n",
    "               \n",
    "        \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "        \n",
    "        # END WHILE over episode steps     \n",
    "    # END WHILE over samples    \n",
    "    \n",
    "    # Update Pi_star:\n",
    "    for s in non_term_states:\n",
    "        a_star = max(Q[s], key = Q[s].get)\n",
    "        Pi_star[s] = {a_star:1.0}\n",
    "    \n",
    "    return Pi_star, N_iter, Q, deltas\n",
    "\n",
    "# END DEF SARSA_Alg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c8721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "111f97ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running Q-learning with N_iter=90000 samples.\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Discount factor, epsilon-greedy probability\n",
    "eps = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Policies\n",
    "Pi_eps_ini = {\n",
    "    (2, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 0)]), 'R':eps/len(grid.adm_actions[(2, 0)])},\n",
    "    (1, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 0)]), 'D':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 0): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 0)]), 'D':eps/len(grid.adm_actions[(0, 0)])},\n",
    "    (0, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 1)]), 'L':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 2): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 2)]), 'D':eps/len(grid.adm_actions[(0, 2)]),\\\n",
    "                                                             'L':eps/len(grid.adm_actions[(0, 2)])},\n",
    "    (1, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 2)]), 'D':eps/len(grid.adm_actions[(1, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(1, 2)])},\n",
    "    (2, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(2, 1)]), 'L':eps/len(grid.adm_actions[(2, 1)])},\n",
    "    (2, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 2)]), 'L':eps/len(grid.adm_actions[(2, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(2, 2)])},\n",
    "    (2, 3): {'L': (1-eps)+eps/len(grid.adm_actions[(2, 3)]), 'U':eps/len(grid.adm_actions[(2, 3)])},\n",
    "  }\n",
    "\n",
    "Pi_unif = {}\n",
    "for s in grid.non_term_states:\n",
    "    Pi_unif[s]={}\n",
    "    for a in grid.adm_actions[s]:\n",
    "        Pi_unif[s][a] = 1/len(grid.adm_actions[s])\n",
    "\n",
    "Pi_opt = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "# Initialize other algorithm arguments:\n",
    "T_max = 200\n",
    "N_samples = 90000\n",
    "alpha = 1\n",
    "eps = 0.05\n",
    "\n",
    "# Execute SARSA algorithm\n",
    "Pi_star, N_iter, Q, deltas = SARSA_Alg(Pi_eps_ini, grid, gamma, alpha, N_samples, T_max, epsilon)\n",
    "\n",
    "print(f\"Finished running Q-learning with N_iter={N_iter} samples.\")\n",
    "print_policy(Pi_star, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f847d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac5506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e9829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34946480",
   "metadata": {},
   "source": [
    "## **4 - Q-Learning:**\n",
    "\n",
    "* Q-learning is one of the most popular reinforcement learning algorithms, and has many variations, most notably deep Q-learning. This algorithm was a breakthrough made by Chris Watkins in his 1989 PhD thesis, and the proof of its convergence is the subject of a 1992 paper by Watkins and Dayan.\n",
    "\n",
    "* The Q-learning algorithm can be described as follows:\n",
    "\n",
    "            Init. Q(s,a) randomly for all (s,a).\n",
    "            while not converged:\n",
    "                s = env.reset()\n",
    "                a = epsilon_greedy{Q(s,-)}\n",
    "                while s not terminal:\n",
    "                    Compute s' and r=r(s,a,s')\n",
    "                    a = epsilon_greedy{Q(s',-)}\n",
    "                    Update: Q(s,a) = Q(s,a) +alpha*(r+gamma*max_a'{Q(s',a')}-Q(s,a))\n",
    "                    Update: s = s'\n",
    "\n",
    "\n",
    "* In contrast with SARSA, Q-learning is its off-policy counterpart. The main update is not done just by bootstrapping the Bellman update, one immediately updates with the best possible action. \n",
    "\n",
    "* In principle, we have the *behavior* policy used to generate the episode steps (typically an $\\varepsilon$-greedy policy), but the *target* (i.e. output) policy should be the greedy one. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6854d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_Learn(Pi, env, gamma, alpha, N_samples, T_max, epsilon):\n",
    "    '''\n",
    "    Function implementing the Q-learning algorithm for TD control.\n",
    "    (Pi should be an epsilon-greedy policy according to instructor)\n",
    "    In principle, the input/behavior policy Pi is epsilon greedy,\n",
    "    while the output/target policy Pi_star is greedy.\n",
    "    I'm starting with a \n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - alpha: Step size parameter, float in ]0,1[.\n",
    "                - N_samples: Max. no. of sample episodes.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - epsilon: Floar, conv. threshold\n",
    "                \n",
    "     OUTPUT:    - V:=V_Pi, value function obtained.\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    adm_actions = env.adm_actions\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Init. Q and Pi_star\n",
    "    Q = {}\n",
    "    Pi_star = {}\n",
    "    for s in term_states:\n",
    "        Q[s]= {\"\":0.0}\n",
    "    for s in non_term_states:\n",
    "        Q[s] = {}\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            # I'm not sure about this last line...\n",
    "    # Should output a policy where Pi_star[s] = {a_star(s):1.0}\n",
    "        \n",
    "    # Init. counters\n",
    "    N_iter = 0\n",
    "    T = 0\n",
    "    deltas = []\n",
    "    \n",
    "        \n",
    "    # Sample episode loop\n",
    "    while (N_iter<N_samples):\n",
    "        \n",
    "        # Generate s_0 randomly\n",
    "        s_old = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        # Lazy Programmer always starts at (2,0)\n",
    "        #s_old = (2, 0)\n",
    "        \n",
    "        # Init. delta_V\n",
    "        delta_Q = 0\n",
    "        \n",
    "        # Episode steps loop\n",
    "        # Note: Should I really impose this max episode length?\n",
    "        while (s_old not in term_states) and (T<T_max):\n",
    "            \n",
    "            # Get random index and \n",
    "            # probs = list(Pi[s_old].values())\n",
    "            ind_rand = np.random.choice(len(Pi[s_old].values()), p=list(Pi[s_old].values()))\n",
    "            # Sample action following Pi(-|s_old)\n",
    "            a = list(Pi[s_old].keys())[ind_rand]\n",
    "            \n",
    "            # WARNING: Modify for other environments\n",
    "            # Compute new state\n",
    "            if a == 'U':\n",
    "                s_new = (s_old[0]-1, s_old[1])\n",
    "            elif a == 'D':\n",
    "                s_new = (s_old[0]+1, s_old[1])\n",
    "            elif a == 'L':\n",
    "                s_new = (s_old[0], s_old[1]-1)\n",
    "            elif a == 'R':\n",
    "                s_new = (s_old[0], s_old[1]+1)\n",
    "            \n",
    "            r_new = Rwds.get(s_new,0)\n",
    "            \n",
    "            ###########################\n",
    "            ## MODIFY FOR Q FUNCTION ##\n",
    "            ###########################\n",
    "            # Update V[s]\n",
    "            #V_old = V[s_old]\n",
    "            #V[s_old] = V[s_old]+ alpha*(r_new+gamma*V[s_new] -V[s_old])\n",
    "            # Update delta_V\n",
    "            #delta_V = max(delta_V, np.abs(V_old-V[s_old]))\n",
    "            \n",
    "            # Update Q(s,a)\n",
    "            a_star = max(Q[s_new], key = Q[s_new].get)\n",
    "            Q[s_old][a] = Q[s_old][a]+ alpha*(r_new + gamma*Q[s_new][a_star]  - Q[s_old][a])\n",
    "            \n",
    "            # Update state for next step\n",
    "            s_old = s_new\n",
    "            \n",
    "            # Update T\n",
    "            T += 1\n",
    "            \n",
    "        # Update deltas\n",
    "        #deltas.append(delta_V)\n",
    "               \n",
    "        \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "        \n",
    "        # END WHILE over episode steps     \n",
    "    # END WHILE over samples    \n",
    "    \n",
    "    # Update Pi_star:\n",
    "    for s in non_term_states:\n",
    "        a_star = max(Q[s], key = Q[s].get)\n",
    "        Pi_star[s] = {a_star:1.0}\n",
    "    \n",
    "    return Pi_star, N_iter, Q, deltas\n",
    "\n",
    "# END DEF Q_Learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a15f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd26bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a4903e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running Q-learning with N_iter=90000 samples.\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Discount factor, epsilon-greedy probability\n",
    "eps = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Policies\n",
    "Pi_eps_ini = {\n",
    "    (2, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 0)]), 'R':eps/len(grid.adm_actions[(2, 0)])},\n",
    "    (1, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 0)]), 'D':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 0): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 0)]), 'D':eps/len(grid.adm_actions[(0, 0)])},\n",
    "    (0, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 1)]), 'L':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 2): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 2)]), 'D':eps/len(grid.adm_actions[(0, 2)]),\\\n",
    "                                                             'L':eps/len(grid.adm_actions[(0, 2)])},\n",
    "    (1, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 2)]), 'D':eps/len(grid.adm_actions[(1, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(1, 2)])},\n",
    "    (2, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(2, 1)]), 'L':eps/len(grid.adm_actions[(2, 1)])},\n",
    "    (2, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 2)]), 'L':eps/len(grid.adm_actions[(2, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(2, 2)])},\n",
    "    (2, 3): {'L': (1-eps)+eps/len(grid.adm_actions[(2, 3)]), 'U':eps/len(grid.adm_actions[(2, 3)])},\n",
    "  }\n",
    "\n",
    "Pi_unif = {}\n",
    "for s in grid.non_term_states:\n",
    "    Pi_unif[s]={}\n",
    "    for a in grid.adm_actions[s]:\n",
    "        Pi_unif[s][a] = 1/len(grid.adm_actions[s])\n",
    "\n",
    "Pi_opt = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "# Initialize other algorithm arguments:\n",
    "T_max = 200\n",
    "N_samples = 90000\n",
    "alpha = 1\n",
    "eps = 0.05\n",
    "\n",
    "# Execute Q-learning algorithm\n",
    "Pi_star, N_iter, Q, deltas = Q_Learn(Pi_unif, grid, gamma, alpha, N_samples, T_max, epsilon)\n",
    "\n",
    "print(f\"Finished running Q-learning with N_iter={N_iter} samples.\")\n",
    "print_policy(Pi_star, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a7757b",
   "metadata": {},
   "source": [
    "At least the resulting policy is indeed optimal. The execution also seems much faster than MC control and SARSA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
