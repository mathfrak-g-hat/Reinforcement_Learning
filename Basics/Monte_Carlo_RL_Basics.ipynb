{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f43c12",
   "metadata": {},
   "source": [
    "# **Monte Carlo RL (Basics)**\n",
    "### 2022/05/02, A. J. Zerouali\n",
    "\n",
    " \n",
    "\n",
    "## **1 - Introduction**\n",
    "\n",
    "### a) Contents\n",
    "\n",
    "This is Section 6 (Lect. 61-68) of Lazy Programmer's introductory RL course. It's divided into the following parts:\n",
    "\n",
    "1) Monte Carlo prediction (policy evaluation): Lect 62-63.\n",
    "\n",
    "2) Monte Carlo control with exploring starts: : Lect 64-65.\n",
    "\n",
    "3) Monte Carlo control without exploring starts: : Lect 66-67.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444eace",
   "metadata": {},
   "source": [
    "### b) Import cells\n",
    "\n",
    "We'll continue working with GridWorld. I'll also include some helper functions written previously for DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa18e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### IMPORTANT: ALWAYS EXECUTE THIS CELL FIRST #####\n",
    "#####################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da4709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Windy GridWorld class and helper functions\n",
    "from Windy_GridWorld import GridWorld_Windy_small, windy_standard_grid, test_standard_grid\n",
    "# Import RL functions: printing functions (for value fn and policy), random policy generator, value fn comparator.\n",
    "from RL_Fns_Windy_GridWorld import print_values, print_policy, gen_random_policy, compare_value_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c8d835",
   "metadata": {},
   "source": [
    "## 2 - Monte Carlo policy evaluation\n",
    "\n",
    "This is the prediction part (Lectures 62-63). The pseudocode for MC \"prediction\" is as follows:\n",
    "\n",
    "            Require: Policy Pi to be evaluated, discount factor gamma.\n",
    "            Initialize V[s]=0, create dict. with returns[s]=[], for all states s.\n",
    "            Loop until convergence:\n",
    "                Generate an episode of length T following Pi: (s_0, a_0, r_1, ... a_(T-1), r_T, s_T)\n",
    "                G = 0                                           # Init. cumulative return of generated episode\n",
    "                for t in {(T-1), (T-2), ..., 1, 0}:\n",
    "                    G = gamma*G + r_(t+1)                       # Evaluate from end of episode\n",
    "                    if s_t not in {s_1, ..., s_(t-1)}:          # If this is the first visit of s_t\n",
    "                        Append G to returns[s_t]\n",
    "                        V[s_t] = mean(returns[s_t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cf1c2e",
   "metadata": {},
   "source": [
    "**22/04/25 - 15:30** This is my first try. I don't fully get the pseudocode. I'll start with initializations and by writing the MC prediction as a \"main\". Let's take a non-windy case and a specific deterministic policy.\n",
    "\n",
    "**About generating episodes:** This is actually what brought me to this course I think. How do you generate trajectories for RL, how do you store your samples, and how do you use Monte Carlo to compute the cumulative returns.\n",
    "\n",
    "* $t=0$: It looks like the only call to np.random will be done for $s_0$. Then following the policy, you get $a_0$, and using the grid rewards you get $r_1 = r(s_1)$.\n",
    "\n",
    "* You continue recursively. Given, use the policy and grid to get $(r_t = Rwds[s_t], s_t, a_t=\\pi(s_t))$\n",
    "\n",
    "* Exceptions: $r_0 = 0$ and $a_T$ is an empty character. (Beware: For T steps, the state list has T+1 entries)\n",
    "\n",
    "* There are 2 types of episodes: If you never transition to one of the terminal states (i.e. (0,3) or (1,3)), the episode length will be T_max (list of $s_t$ will be of length (T_max+1)). If you reach a terminal state at time T, then the episode is shorter.\n",
    "\n",
    "* I should perhaps store my episodes as lists of tuples (r_t, s_t, a_t)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bac5e4f",
   "metadata": {},
   "source": [
    "### a) The implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f7195d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### GENERATE EPISODE ###\n",
    "########################\n",
    "# 2022/04/26, AJ Zerouali\n",
    "# Format: Epsd = [(0, s_0, a_0), (r_1, s_1, a_1), ..., (r_T, s_T, '')]\n",
    "# COMMENT: The function below is taylored to GridWorld. The correct way\n",
    "#          to implement it is to use methods of the environment class.\n",
    "#          I'm not using the methods of the Windy_GridWorld class because\n",
    "#          the instructor's implementation is a little too clunky to my\n",
    "#          taste, and I\n",
    "\n",
    "def generate_episode(s_0, a_0, Pi, env, T_max):\n",
    "    # ARGUMENTS: Initial state and action; policy; environment; max episode length (in this order).\n",
    "    # OUTPUT: episode_rewards: Rewards list\n",
    "    #         episode_states: State list\n",
    "    #         episode_actions: Actions list\n",
    "    #         T: Episode length\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Step t=0\n",
    "    s_new = s_0\n",
    "    a_new = a_0\n",
    "    r_new = 0\n",
    "    \n",
    "    # Init. episode lists (format: step_t = (r_t, s_t, a_t))\n",
    "    episode_rewards = []\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    \n",
    "    # Store step 0\n",
    "    episode_rewards.append(r_new)\n",
    "    episode_states.append(s_new)\n",
    "    episode_actions.append(a_new)\n",
    "    \n",
    "        \n",
    "    # Init. episode length\n",
    "    T = 0\n",
    "    \n",
    "    ##### 0<t\n",
    "    while (s_new not in term_states) and (T<T_max):\n",
    "        \n",
    "        # Init. old step\n",
    "        r_old = r_new\n",
    "        s_old = s_new\n",
    "        a_old = a_new\n",
    "        \n",
    "        # WARNING: Modify for other environments\n",
    "        # Compute new state\n",
    "        if a_old == 'U':\n",
    "            s_new = (s_old[0]-1, s_old[1])\n",
    "        elif a_old == 'D':\n",
    "            s_new = (s_old[0]+1, s_old[1])\n",
    "        elif a_old == 'L':\n",
    "            s_new = (s_old[0], s_old[1]-1)\n",
    "        elif a_old == 'R':\n",
    "            s_new = (s_old[0], s_old[1]+1)\n",
    "        \n",
    "        # Compute new action\n",
    "        if s_new in non_term_states:\n",
    "            a_new = list(Pi[s_new].keys())[0]\n",
    "        elif s_new in term_states:\n",
    "            a_new = ''\n",
    "        \n",
    "        # Compute new reward\n",
    "        r_new = Rwds.get(s_new, 0)\n",
    "        \n",
    "        # Add step to episode\n",
    "        episode_rewards.append(r_new)\n",
    "        episode_states.append(s_new)\n",
    "        episode_actions.append(a_new)\n",
    "        \n",
    "        # Update\n",
    "        T += 1\n",
    "        \n",
    "    # END WHILE\n",
    "    \n",
    "    # Output line\n",
    "    return episode_rewards, episode_states, episode_actions, T\n",
    "    \n",
    "# END DEF generate_episode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb45e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfbdfb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62d7a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "###  MONTE CARLO POLICY EVALUATION ###\n",
    "######################################\n",
    "## 2022/04/26, AJ Zerouali\n",
    "# We are not using convergence thresholds here.\n",
    "# The function below generates a specified number of sample\n",
    "# episodes and averages returns to evaluate a given policy.\n",
    "# Can choose first visit MC or all visits MC with Boolean.\n",
    "\n",
    "def MC_Policy_Eval(Pi, env, gamma, N_samples, T_max, all_visits_MC):\n",
    "    # ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "    #            - N_samples: No. of samples for Monte Carlo expectation.\n",
    "    #            - T_max: Max. episode length minus 1.\n",
    "    #            - all_visits_MC: Boolean for all visits or first visit MC.\n",
    "    # OUTPUT:    - V:=V_Pi, value function obtained.\n",
    "    # NOTE: This function calls generate_episode().\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Init output and returns\n",
    "    V = {}\n",
    "    Returns = {}\n",
    "    for s in term_states:\n",
    "        V[s] = 0.0\n",
    "    for s in non_term_states:\n",
    "        V[s] = 0.0\n",
    "        Returns[s]=[]\n",
    "        # Init. counter\n",
    "        N_iter = 0\n",
    "    \n",
    "    # Main MC loop\n",
    "    while N_iter < N_samples:\n",
    "\n",
    "        ##########################\n",
    "        ##### Step t=0 setup #####\n",
    "        ##########################\n",
    "\n",
    "        # Count no. of non_term_states\n",
    "        N_non_term_states = len(non_term_states)\n",
    "\n",
    "        # Generate s_0 randomly from non_term_states, get a_0 from policy\n",
    "        s_0 = list(non_term_states)[np.random.randint(N_non_term_states)]\n",
    "        a_0 = list(Pi[s_0].keys())[0]\n",
    "\n",
    "        ########################\n",
    "        ##### Steps 1 to T #####\n",
    "        ########################\n",
    "\n",
    "        # Generate episode\n",
    "        # Signature: episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        # Step t of episode is (r_t, s_t, a_t) = (episode_rewards[t], episode_states[t], episode_actions[t])\n",
    "\n",
    "        ##################################\n",
    "        ### COMPUTE CUMULATIVE RETURNS ###\n",
    "        ##################################\n",
    "\n",
    "        # Init. storing variable\n",
    "        G = 0.0\n",
    "        \n",
    "        # First visit only MC\n",
    "        if not all_visits_MC:\n",
    "            # Loop over episode\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                \n",
    "                if s_t not in episode_states[:t]:\n",
    "                    Returns[s_t].append(G)\n",
    "                    V[s_t] = np.average(Returns[s_t])\n",
    "                    \n",
    "        # All visits MC\n",
    "        elif all_visits_MC:\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                Returns[s_t].append(G)\n",
    "                V[s_t] = np.average(Returns[s_t])\n",
    "        \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "\n",
    "    # END WHILE of MC loop\n",
    "    \n",
    "    # Output\n",
    "    return V\n",
    "\n",
    "# END DEF MC_Policy_Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83980ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Grid attributes\n",
    "non_term_states = grid.non_term_states\n",
    "term_states = grid.term_states\n",
    "adm_actions = grid.adm_actions\n",
    "Rwds = grid.rewards\n",
    "\n",
    "\n",
    "\n",
    "# Policy to be evaluated\n",
    "Pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "#epsilon = 1e-3\n",
    "\n",
    "# Max. episode length:\n",
    "T_max = 50\n",
    "N_samples =200\n",
    "\n",
    "# Evaluate V_Pi:\n",
    "# SIGNATURE: V = MC_Policy_Eval(Pi, env, gamma, N_samples, T_max, all_visits_MC)\n",
    "V = MC_Policy_Eval(Pi, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "V_lect = MC_Policy_Eval(Pi_lect, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1590d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c521bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_lect, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74948e7",
   "metadata": {},
   "source": [
    "### b) Notes\n",
    "\n",
    "Let us summarize Monte Carlo policy evaluation in words. We look in detail at the three steps of the main loop in the pseudocode:\n",
    "\n",
    "(1) Generate a random trajectory $\\tau=(s_0,a_0,\\cdots,a_{T-1},s_T)$, then loop $t=T-1,\\cdots,1,0$;\n",
    "\n",
    "(2) Add the return $\\sum\\gamma^t r(s_t,a_t)$ to the list $\\text{Returns}[s_t]$;\n",
    "\n",
    "(3) Update the average $V_\\pi(s_t)$\n",
    "\n",
    "We are given a policy $\\pi$, and the quantity we want to approximate with MC is:\n",
    "\n",
    "$$V_\\pi(s) = \\mathbb{E}_{\\tau\\sim\\text{Pr}_\\pi}\\left[R(\\tau)|s_0=s\\right],\\ \\forall s\\in\\mathcal{S},$$\n",
    "\n",
    "where $\\text{Pr}_\\pi$ is the probability distribution for trajectories induced by the policy, and where $R(\\tau)$ denotes the cumulative returns on the trajectory $\\tau=(s_0,a_0,\\cdots,a_{T-1},s_T)$. In principle, to estimate $V_\\pi(s)$ via MC, we would generate $N_s$ trajectories $(s^{(j)}_0,a^{(j)}_0,\\cdots,a^{(j)}_{T-1},s^{(j)}_T)$ that **start at the fixed state** $s=s^{(j)}_0$, and then take the empirical average:\n",
    "\n",
    "$$V_\\pi(s) \\simeq \\frac{1}{N_s}\\sum_{j=0}^{N_s}R(s,a^{(j)}_0,\\cdots,a^{(j)}_{T-1},s^{(j)}_T).$$\n",
    "\n",
    "For obvious reasons, generating several samples for each state in $\\mathcal S$ would be too computationally costly, so we instead generate a set of sample trajectories $\\{\\tau_1,\\cdots,\\tau_N\\}$ as in the first step above, and then update $V_\\pi(s)$ each time we have a new sample. On the one hand, this last part is the less intuitive one, which is why we address it in more detail in the next paragraph. On the other hand, we emphasize that the sample trajectories $\\{\\tau_1,\\cdots,\\tau_N\\}$ have random starting states $s_0^{(j)}$, unlike in the ab-initio MC sum above. \n",
    "\n",
    "Now we turn to steps (2) and (3), and assume that we have a new sample trajectory $\\tau_j$. Furthermore, assume we are using the first visit MC condition as in the pseudocode. When considering $s_t^{(j)}$, we check whether it appears earlier in the new sample $\\tau_j$ (i.e. $\\exists t'\\in [0,t-1]$ such that $s_{t'}^{(j)}=s_t^{(j)}$). We have two possibilities then:\n",
    "\n",
    "* If there's no such $t'$, then the trajectory $(s_t^{(j)},a_t^{(j)},\\cdots,s_T^{(j)})$ is **treated as a sample for the MC estimation of $V_\\pi(s^{(j)}_t)$, for which the variable $G=R(s_t^{(j)},a_t^{(j)},\\cdots,s_T^{(j)})$ in the pseudocode represents a sample return**.\n",
    "\n",
    "* If $s_t^{(j)}$ is not the first visit, then the sample trajectory is $(s_{t''}^{(j)},a_{t''}^{(j)},\\cdots,s_T^{(j)})$, where $t''$ is the first occurence time of the state $s_t^{(j)}=s_{t''}^{(j)}$. Like above, the variable $G$ is a sample return.\n",
    "\n",
    "In other words, from the available random trajectories $\\{\\tau_1,\\cdots,\\tau_j\\}$, we do not store samples starting at given states, and instead, we update the list of sample returns starting at state $s_t^{(j)}$. This is step (2) above. Step (3) is simply the update the Monte Carlo estimation of $V_\\pi(s_t^{(j)})$ (which is why it's the average of a returns list). \n",
    "\n",
    "The first point to clarify now is why we are looping backwards on each sample episode. On the one hand, looping from the beginning of the episode would mean that $G=R(s_t,a_t,\\cdots,s_T)$ will be updated with a sum $\\sum_{t'\\ge t} \\gamma^{t'}r(s_{t'},a_{t'})$ for each $t\\in\\{0,\\cdots,T\\}$ (and for each sample trajectory). This represents several orders of computations more than looping backwards, let alone the fact that we need to keep track of the extracted sample trajectories.\n",
    "\n",
    "The second point to clarify is what happens when we **do not use** first visit MC. Removing the condition in the pseudocode would give us more sample trajectories starting at state $s_t^{(j)}$, which would lead to a more accurate approximation of $V_\\pi(s_t^{(j)})$, but to longer execution times in complex models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7886a60",
   "metadata": {},
   "source": [
    "## 3 - Monte Carlo control - Exploring starts\n",
    "\n",
    "This section follows Lectures 64 and 65. The problem now is to determine the optimal policy $\\pi^\\ast$, and to do so, we use a strategy similar to value iteration. The first difference is that now we will work with the state-action value function $Q(s,a)$, which once computed will allow us to get $\\pi^\\ast(s)=\\arg\\max_a Q(s,a)$. \n",
    "\n",
    "A first point to keep in mind is that when gathering samples of $Q(s,a)$, we will need $a\\neq \\pi(s)$ to improve $\\pi$, meaning that the starting points of the randomly generated episodes will start with random $(s_0,a_0)$.\n",
    "\n",
    "A second difficulty that arises is ensuring that we implement an algorithm better than policy iteration, in the sense that we will update the $\\pi^\\ast(s)$ at each new sample episode. \n",
    "\n",
    "Here is the pseudocode of MC with exploring starts.\n",
    "\n",
    "            Require: Discount factor gamma.\n",
    "            Initialize: Random policy Pi, Q(s,a)=0 for all state-actions (s,a),\n",
    "                        create returns dict. and lists returns[(s,a)]=[]\n",
    "            Loop until convergence:\n",
    "                Generate initial state-action (s_0, a_0) randomly.\n",
    "                Generate episode sample of length T following Pi: (s_0, a_0, r_1, ... a_(T-1), r_T, s_T)\n",
    "                G = 0                                              # Init. cumulative return of generated episode\n",
    "                for t in {(T-1), (T-2), ..., 1, 0}:\n",
    "                    G = gamma*G + r_(t+1)                          # Evaluate sample return\n",
    "                    if (s_t, a_t) not in {(s_i,a_i)}_{i=0,...,t-1}:# If this is the first visit of (s_t,a_t)\n",
    "                        Append G to returns[s_t,a_t]               # Store sample cum. return\n",
    "                        Q[s_t, a_t] = average(returns[s_t, a_t])   # Update Q(s_t, a_t)\n",
    "                        Pi[s_t] = argmax_a {Q[s_t, a]}             # Update Pi*(s_t)\n",
    "\n",
    "**Exercise:** Find a way of improving the algorithm above.\n",
    "\n",
    "**Answer:** The main computational drawback of the pseudocode above is that the samples are stored but not used subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a32bfe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "###  MONTE CARLO CONTROL WITH EXPLORING STARTS - VERSION 3 ###\n",
    "##############################################################\n",
    "## 2022/05/02, AJ Zerouali\n",
    "# Monte Carlo with exploring starts (Lect. 64-65)\n",
    "# We are not using convergence thresholds here.\n",
    "# This function below generates a specified number of sample\n",
    "# episodes and averages returns to find the optimal policy.\n",
    "# Can choose first visit MC or all visits MC with Boolean.\n",
    "# This function also calls generate_episode() of previous sec.\n",
    "\n",
    "def MC_Ctrl_ExpStarts(Pi, env, gamma, N_samples, T_max, all_visits_MC):\n",
    "    '''\n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - N_samples: No. of samples for Monte Carlo expectation.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - all_visits_MC: Boolean for all visits or first visit MC.\n",
    "     OUTPUT:    - Pi_star: Optimal policy\n",
    "                - V_star: Optimal value function\n",
    "     NOTE: This function calls generate_episode().\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    \n",
    "    # Init. Q, returns, Pi_star, and V_star dictionaries\n",
    "    Pi_star = {}\n",
    "    Q = {}\n",
    "    Returns = {}\n",
    "    V_star = {}\n",
    "    for s in non_term_states:\n",
    "        Pi_star[s] = {}\n",
    "        Q[s]={}\n",
    "        Returns[s] = {}\n",
    "        V_star[s] = 0.0\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            Returns[s][a]=[]\n",
    "            \n",
    "    for s in term_states:\n",
    "        V_star[s] = 0.0\n",
    "            \n",
    "    \n",
    "    # Init. counter\n",
    "    N_iter = 0\n",
    "    \n",
    "    # Main MC loop\n",
    "    while (N_iter<N_samples):\n",
    "                \n",
    "        ##########################\n",
    "        ##### Step t=0 setup #####\n",
    "        ##########################\n",
    "\n",
    "        # Generate (s_0, a_0) randomly from non_term_states and adm_actions\n",
    "        # np.random.choice() works only for 1-dim'l arrays\n",
    "        s_0 = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        a_0 = np.random.choice(adm_actions[s_0])\n",
    "\n",
    "        ########################\n",
    "        ##### Steps 1 to T #####\n",
    "        ########################\n",
    "        \n",
    "        #print(f\"Generating sample episode no. {N_iter+1}...\\n\") # Debug\n",
    "        # Generate episode\n",
    "        # Signature: episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, grid, T_max)\n",
    "        # Step t of episode is (r_t, s_t, a_t) = (episode_rewards[t], episode_states[t], episode_actions[t])\n",
    "        #rint(f\"Sample episode N_iter={N_iter} has T={T} steps after t=0.\\n\") # Debug\n",
    " \n",
    "        #####################################################\n",
    "        ### COMPUTE CUMULATIVE RETURNS AND OPTIMAL ACTION ###\n",
    "        #####################################################\n",
    "\n",
    "        # First visit only MC\n",
    "        if not all_visits_MC:\n",
    "            # State-action iterable\n",
    "            episode_state_actions = list(zip(episode_states, episode_actions))\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            # Loop over episode\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the return\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                if (s_t, a_t) not in episode_state_actions[:t]:\n",
    "                    \n",
    "                    # Update sample returns and Q-function\n",
    "                    Returns[s_t][a_t].append(G)\n",
    "                    Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                    \n",
    "                    \n",
    "                    # Get a_star and update Pi_star\n",
    "                    a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                    Pi_star[s_t] = {a_star:1.0} \n",
    "                    \n",
    "                    \n",
    "        # END IF first visit MC\n",
    "        \n",
    "        # All visits MC\n",
    "        elif all_visits_MC:\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the returns\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                # Update sample returns and Q-function\n",
    "                Returns[s_t][a_t].append(G)\n",
    "                Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                \n",
    "                \n",
    "                # Get a_star and update Pi_star\n",
    "                a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                Pi_star[s_t] = {a_star:1.0} \n",
    "                \n",
    "                              \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "        \n",
    "        '''\n",
    "        # DEBUG:\n",
    "        print(f\"Completed episode N_iter={N_iter} with R={G}.\")\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        '''\n",
    "       \n",
    "    # END WHILE of MC loop\n",
    "    \n",
    "    # COMPUTE V\n",
    "    for s in non_term_states:\n",
    "        a = max(Q[s], key = Q[s].get)\n",
    "        # Pi_star[s] = {a:1.0}\n",
    "        V_star[s]=Q[s][a]\n",
    "    \n",
    "    # Output\n",
    "    return Pi_star, V_star, Q, Returns, N_iter\n",
    "\n",
    "# END DEF MC_Ctrl_ExpStarts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092c233",
   "metadata": {},
   "source": [
    "**22/04/27:** If we execute MC with exploring starts in non-windy GridWorld, we indeed get *an* optimal policy, but the values $V(2,2)$ and $V(2,3)$ are incorrect.\n",
    "\n",
    "Here is the non-windy GridWorld test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b6d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Policy to be evaluated\n",
    "Pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "#epsilon = 1e-3\n",
    "\n",
    "# Max. episode length:\n",
    "T_max = 50\n",
    "N_samples = 1000\n",
    "\n",
    "# Evaluate V_Pi:\n",
    "# SIGNATURE: Pi_star, V_star, Q, Returns = MC_Ctrl_ExpStarts(Pi, env, gamma, N_samples, T_max, all_visits_MC)\n",
    "#V = MC_Policy_Eval(Pi, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "Pi_star, V_star, Q, Returns, N_iter = MC_Ctrl_ExpStarts(Pi_lect, grid, gamma, N_samples, T_max, all_visits_MC = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf136c",
   "metadata": {},
   "source": [
    "The policy obtained looks optimal (although that depends on the number of samples collected):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55b4f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(Pi_star, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6163f1",
   "metadata": {},
   "source": [
    "The value function is off though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f13352f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59|-0.73|-0.81|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_star, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114ff8a",
   "metadata": {},
   "source": [
    "If we evaluate the optimal policy with MC, we get a different value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5839379",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_eval = MC_Policy_Eval(Pi_star, grid, gamma, 50, T_max, all_visits_MC = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6bb51fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_eval, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635bf1bc",
   "metadata": {},
   "source": [
    "These results could be attributed to the fact that we're not generating the MC samples until we reduce the error, we are only limiting the number of samples.\n",
    "\n",
    "The code above behaves very badly with a randomly generated policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0010fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the optimal policy obtained from MC_Ctrl_ExpStarts:\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  L  |  R  |\n",
      "------------------------\n",
      "  D  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "------------------------\n",
      "Printing the value fn obtained from MC_Ctrl_ExpStarts:\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.00| 0.00| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00|-0.81|\n",
      "------------------------\n",
      "Printing the value fn obtained from MC_Policy_Eval(Pi_star,...):\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.00| 0.00| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.81| 0.73|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Policy to be evaluated\n",
    "Pi_opt = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_rand = gen_random_policy(grid)\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "#epsilon = 1e-3\n",
    "\n",
    "# Max. episode length:\n",
    "T_max = 50\n",
    "N_samples = 20000\n",
    "\n",
    "# Seach for Pi_star:\n",
    "# SIGNATURE: Pi_star, V_star, Q, Returns = MC_Ctrl_ExpStarts(Pi, env, gamma, N_samples, T_max, all_visits_MC)\n",
    "#V = MC_Policy_Eval(Pi, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "Pi_star, V_star, Q, Returns, N_iter = MC_Ctrl_ExpStarts(Pi_rand, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "\n",
    "print(f\"Printing the optimal policy obtained from MC_Ctrl_ExpStarts:\")\n",
    "print_policy(Pi_star, grid)\n",
    "\n",
    "print(f\"Printing the value fn obtained from MC_Ctrl_ExpStarts:\")\n",
    "print_values(V_star, grid)\n",
    "\n",
    "V_eval = MC_Policy_Eval(Pi_star, grid, gamma, 50, T_max, all_visits_MC = False)\n",
    "print(f\"Printing the value fn obtained from MC_Policy_Eval(Pi_star,...):\")\n",
    "print_values(V_eval, grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c478b4",
   "metadata": {},
   "source": [
    "From these tests, it seems that it is a sampling issue. Half of the grid above hasn't been visited as we can see from the values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62580b8f",
   "metadata": {},
   "source": [
    "## 4 - Monte Carlo control - Epsilon greedy policy\n",
    "\n",
    "Last part of the Monte Carlo section of the course, lectures 66 and 67. Turning to MC without exploring starts. Main points:\n",
    "\n",
    "* When performing control with a given policy, it is possible to miss certain state-action pairs that aren't visited by the policy.\n",
    "\n",
    "* In principle, would need to fill the dictionary $Q_pi(s,a)$ for all $(s,a)\\in\\mathcal{S}\\times \\mathcal{A}_s$.\n",
    "\n",
    "* First new change is that the policy built is $\\varepsilon$-soft. In particular, we will build an $\\varepsilon$-greedy policy.\n",
    "\n",
    "* **Definition:** (Sutton-Barto, p.100) A policy is said to be soft if $\\pi(a|s)>0$ for all $(s,a)\\in\\mathcal{S}\\times\\mathcal{A}_s$, and gradually shifts closer to an optimal deterministic policy.\n",
    "\n",
    "* **Question:** Why is he resetting to one initial state $s_0$?\n",
    "\n",
    "* The new part in the implementation is how $a^\\ast$ is chosen in the policy update at time $t$. If $a^\\ast=\\arg\\max_a Q(s_t,a)$, choose \\$pi(a|s_t)=(1-\\varepsilon)+(\\varepsilon/|\\mathcal{A}|)$ if $a=a^\\ast$ and $\\pi(a|s_t)=\\varepsilon/|\\mathcal{A}|$ otherwise.\n",
    "\n",
    "* Lazy Programmer proposes a function that chooses an epsilon-greedy action.\n",
    "\n",
    "            def epsilon_greedy(Q,s,eps):\n",
    "                if random() < eps:\n",
    "                    return random action\n",
    "                else:\n",
    "                    return argmax_a Q(s,a)\n",
    "\n",
    "* Whereas exploring starts was an on-policy algorithm, the version without exploring starts is off-policy.\n",
    "\n",
    "* Will write an $\\varepsilon$-soft policy generator. For the execution however, will have to modify the policy to avoid infinite loops when testing MC control.\n",
    "\n",
    "* Will also have to write an episode generator following a stochastic policy. print_policy() will be useless then.\n",
    "\n",
    "\n",
    "\n",
    "The \"epsilon-greedy\" Monte Carlo pseudocode is as follows:\n",
    "\n",
    "            Require: Discount factor gamma.\n",
    "            Initialize: Pi = arbitrary eps-soft policy with Pi(a|s)>0 for all s and all a in adm_actions[s],\n",
    "                        Q(s,a)=0 for all state-actions (s,a),\n",
    "                        create returns dict. and lists returns[(s,a)]=[].\n",
    "            Loop until convergence:\n",
    "                Reset to initial state s_0.\n",
    "                Generate episode sample of length T following Pi: (s_0, a_0, r_1, ... a_(T-1), r_T, s_T)\n",
    "                G = 0                                              # Init. cumulative return of generated episode\n",
    "                for t in {(T-1), (T-2), ..., 1, 0}:\n",
    "                    G = gamma*G + r_(t+1)                          # Evaluate sample return\n",
    "                    if (s_t, a_t) not in {(s_i,a_i)}_{i=0,...,t-1}:# If this is the first visit of (s_t,a_t)\n",
    "                        Append G to returns[s_t,a_t]               # Store sample cum. return\n",
    "                        Q[s_t, a_t] = average(returns[s_t, a_t])   # Update Q(s_t, a_t)\n",
    "                        a* = argmax_a Q[s_t,a]\n",
    "                        for a in adm_actions[s_t]:                 # Update Pi*[a|s_t], eps-greedy policy\n",
    "                            if a==a*:\n",
    "                                Pi[a|s_t] = 1-eps+(eps/len(adm_actions[s_t]))\n",
    "                            else:\n",
    "                                Pi[a|s_t] = eps/len(adm_actions[s_t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7140a",
   "metadata": {},
   "source": [
    "Instead of cloggging this notebook with more functions, I will import them from Monte_Carlo_Windy_GridWorld.py. To deal with $\\varepsilon$-greedy policies which are stochastic, we need the stochastic versions of the episode generator, the MC policy evaluation, and I added a random $\\varepsilon$-soft policy generator in RL_Fns_WindyGridWorld.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca932a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Monte_Carlo_Windy_GridWorld import generate_episode_stochastic, MC_Stochastic_Policy_Eval\n",
    "from RL_Fns_Windy_GridWorld import gen_random_epslnsoft_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be33aa",
   "metadata": {},
   "source": [
    "Here's the implementation of the main algorithm in this subsection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2091b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "###  MONTE CARLO CONTROL WITH EPSILON-GREEDY POLICY ###\n",
    "#######################################################\n",
    "## 2022/05/02, AJ Zerouali\n",
    "# Monte Carlo without exploring starts (Lect. 64-65), following\n",
    "# an epsilon greedy scheme.\n",
    "# We are not using convergence thresholds here.\n",
    "# This function below generates a specified number of sample\n",
    "# episodes and averages returns to find the optimal policy.\n",
    "# Can choose first visit MC or all visits MC with Boolean.\n",
    "# This function also calls generate_episode() of previous sec.\n",
    "\n",
    "def MC_Ctrl_EpsGreedy(Pi, eps, env, gamma, N_samples, T_max, all_visits_MC):\n",
    "    '''\n",
    "     Epsilon-greedy Monte Carlo control algorithm.\n",
    "     \n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - eps: Epsilon float for output policy.\n",
    "                - N_samples: No. of samples for Monte Carlo expectation.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - all_visits_MC: Boolean for all visits or first visit MC.\n",
    "     OUTPUT:    - Pi_star: Optimal policy\n",
    "                - V_star: Optimal value function\n",
    "                - Q: State-action values from samples\n",
    "                - Returns: Dict. of return samples (by init. state-action)\n",
    "                - N_iter: No. of randomly generated episodes (<= N_samples)\n",
    "     NOTE: This function calls generate_episode().\n",
    "           Should take an eps-soft policy as input.\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    \n",
    "    # Init. Q, returns, Pi_star, and V_star dictionaries\n",
    "    Pi_star = {}\n",
    "    Q = {}\n",
    "    Returns = {}\n",
    "    V_star = {}\n",
    "    for s in non_term_states:\n",
    "        Pi_star[s] = {}\n",
    "        Q[s]={}\n",
    "        Returns[s] = {}\n",
    "        V_star[s] = 0.0\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            Returns[s][a]=[]\n",
    "            \n",
    "    for s in term_states:\n",
    "        V_star[s] = 0.0\n",
    "            \n",
    "    \n",
    "    # Init. counter\n",
    "    N_iter = 0\n",
    "    \n",
    "    # Main MC loop\n",
    "    while (N_iter<N_samples):\n",
    "                \n",
    "        ##########################\n",
    "        ##### Step t=0 setup #####\n",
    "        ##########################\n",
    "\n",
    "        # Generate (s_0, a_0) randomly from non_term_states and adm_actions\n",
    "        # np.random.choice() works only for 1-dim'l arrays\n",
    "        s_0 = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        a_0 = np.random.choice(adm_actions[s_0])\n",
    "\n",
    "        ########################\n",
    "        ##### Steps 1 to T #####\n",
    "        ########################\n",
    "        \n",
    "        #print(f\"Generating sample episode no. {N_iter+1}...\\n\") # Debug\n",
    "        # Generate episode\n",
    "        # Signature: episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        # Step t of episode is (r_t, s_t, a_t) = (episode_rewards[t], episode_states[t], episode_actions[t])\n",
    "        episode_rewards, episode_states, episode_actions, T = generate_episode_stochastic(s_0, a_0, Pi, grid, T_max)\n",
    "        #print(f\"Sample episode N_iter={N_iter} has T={T} steps after t=0.\\n\") # Debug\n",
    " \n",
    "        #####################################################\n",
    "        ### COMPUTE CUMULATIVE RETURNS AND OPTIMAL ACTION ###\n",
    "        #####################################################\n",
    "\n",
    "        # First visit only MC\n",
    "        if not all_visits_MC:\n",
    "            # State-action iterable\n",
    "            episode_state_actions = list(zip(episode_states, episode_actions))\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            # Loop over episode\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the return\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                if (s_t, a_t) not in episode_state_actions[:t]:\n",
    "                    \n",
    "                    # Update sample returns and Q-function\n",
    "                    Returns[s_t][a_t].append(G)\n",
    "                    Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                    \n",
    "                    # Get a_star and update Pi_star\n",
    "                    a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                    Pi_star[s_t][a_star] = 1-eps+eps/len(adm_actions[s_t])\n",
    "                    for a in adm_actions[s_t]:\n",
    "                        if a != a_star:\n",
    "                            Pi_star[s_t][a] = eps/len(adm_actions[s_t])\n",
    "                \n",
    "                    \n",
    "        # END IF first visit MC\n",
    "        \n",
    "        # All visits MC\n",
    "        elif all_visits_MC:\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the returns\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                # Update sample returns and Q-function\n",
    "                Returns[s_t][a_t].append(G)\n",
    "                Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                \n",
    "                # Get a_star and update Pi_star\n",
    "                a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                Pi_star[s_t][a_star] = 1-eps+eps/len(adm_actions[s_t])\n",
    "                for a in adm_actions[s_t]:\n",
    "                    if a != a_star:\n",
    "                        Pi_star[s_t][a] = eps/len(adm_actions[s_t])\n",
    "                \n",
    "                              \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "        \n",
    "        '''\n",
    "        # DEBUG:\n",
    "        print(f\"Completed episode N_iter={N_iter} with R={G}.\")\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        '''\n",
    "       \n",
    "    # END WHILE of MC loop\n",
    "    \n",
    "    # COMPUTE V\n",
    "    ## CHANGE IN EPSILON GREEDY??\n",
    "    for s in non_term_states:\n",
    "        a = max(Q[s], key = Q[s].get)\n",
    "        V_star[s]=Q[s][a]\n",
    "    \n",
    "    # Output\n",
    "    return Pi_star, V_star, Q, Returns, N_iter\n",
    "\n",
    "# END DEF MC_Ctrl_EpsGreedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313ecdb",
   "metadata": {},
   "source": [
    "Here's now a test cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d59cc410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running MC_Ctrl_EpsGreedy with N_iter=5000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Discount factor, epsilon-greedy probability\n",
    "eps = 0.1\n",
    "gamma = 0.9\n",
    "#epsilon = 1e-3\n",
    "\n",
    "# Policies\n",
    "Pi_eps_ini = {\n",
    "    (2, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 0)]), 'R':eps/len(grid.adm_actions[(2, 0)])},\n",
    "    (1, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 0)]), 'D':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 0): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 0)]), 'D':eps/len(grid.adm_actions[(0, 0)])},\n",
    "    (0, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 1)]), 'L':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 2): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 2)]), 'D':eps/len(grid.adm_actions[(0, 2)]),\\\n",
    "                                                             'L':eps/len(grid.adm_actions[(0, 2)])},\n",
    "    (1, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 2)]), 'D':eps/len(grid.adm_actions[(1, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(1, 2)])},\n",
    "    (2, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(2, 1)]), 'L':eps/len(grid.adm_actions[(2, 1)])},\n",
    "    (2, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 2)]), 'L':eps/len(grid.adm_actions[(2, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(2, 2)])},\n",
    "    (2, 3): {'L': (1-eps)+eps/len(grid.adm_actions[(2, 3)]), 'U':eps/len(grid.adm_actions[(2, 3)])},\n",
    "  }\n",
    "\n",
    "Pi_unif = {}\n",
    "for s in grid.non_term_states:\n",
    "    Pi_unif[s]={}\n",
    "    for a in grid.adm_actions[s]:\n",
    "        Pi_unif[s][a] = 1/len(grid.adm_actions[s])\n",
    "\n",
    "Pi_opt = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "# Max. episode length:\n",
    "T_max = 200\n",
    "N_samples = 5000\n",
    "eps = 0.05\n",
    "\n",
    "# Search for Pi_star:\n",
    "Pi_star, V_star, Q, Returns, N_iter = MC_Ctrl_EpsGreedy(Pi_unif, eps, grid, gamma, N_samples, T_max,\\\n",
    "                                                        all_visits_MC = False)\n",
    "\n",
    "print(f\"Finished running MC_Ctrl_EpsGreedy with N_iter={N_iter} samples.\")\n",
    "\n",
    "# MC policy evaluation needs a stochastic version too"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599e11c",
   "metadata": {},
   "source": [
    "Executing the above for the uniform policy, we see that we find a policy that is rather close to the optimal one indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2509bc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): {'L': 0.025, 'R': 0.975},\n",
       " (1, 2): {'U': 0.9666666666666667,\n",
       "  'D': 0.016666666666666666,\n",
       "  'R': 0.016666666666666666},\n",
       " (2, 1): {'L': 0.975, 'R': 0.025},\n",
       " (0, 0): {'R': 0.975, 'D': 0.025},\n",
       " (2, 0): {'U': 0.975, 'R': 0.025},\n",
       " (2, 3): {'L': 0.975, 'U': 0.025},\n",
       " (0, 2): {'R': 0.9666666666666667,\n",
       "  'L': 0.016666666666666666,\n",
       "  'D': 0.016666666666666666},\n",
       " (2, 2): {'U': 0.016666666666666666,\n",
       "  'R': 0.016666666666666666,\n",
       "  'L': 0.9666666666666667},\n",
       " (1, 0): {'U': 0.975, 'D': 0.025}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cfb9ed",
   "metadata": {},
   "source": [
    "As with the exploring starts however, the computation of the value function is also off. MC_Ctrl_EpsGreedy() gives the following value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f0a9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.14| 0.25| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.05| 0.00| 0.27| 0.00|\n",
      "------------------------\n",
      "-0.01|-0.09|-0.19|-0.34|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_star, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9066fa10",
   "metadata": {},
   "source": [
    "Using MC policy evaluation however, we obtain the following value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8f92bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.80| 0.86| 0.99| 0.00|\n",
      "------------------------\n",
      " 0.72| 0.00| 0.87| 0.00|\n",
      "------------------------\n",
      " 0.64| 0.57| 0.64| 0.47|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "V = MC_Stochastic_Policy_Eval(Pi_star, grid, gamma, N_samples, 200, all_visits_MC=False)\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12040b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed967450",
   "metadata": {},
   "source": [
    "#### Comments on epsilon-greedy strategies:\n",
    "\n",
    "* A greedy strategy can be described as a short-sighted one, in the sense that we disregard the number of samples collected and the confidence in the accuracy of some estimated expected return. More precisely, we use the available samples and simply take the maximizing action. In principle, this is a heuristic only. The pseudocode for a greedy policy would typically look like the following, where we evaluate have a predicted expectation $J$:\n",
    "\n",
    "            While True:\n",
    "                Compute the expectations J[a] from collected samples\n",
    "                a = argmax_b(J)\n",
    "                execute action a\n",
    "\n",
    "* An epsilon-greedy strategy aims to explore more data instead of only exploiting the collected samples. We fix a small probability $\\varepsilon\\sim 5\\%, 10\\%$ of taking a completely different action from the argmax. In contrast with the previous pseudocode, an epsilon-greedy algorithm is as follows:\n",
    "\n",
    "            While True:\n",
    "                Compute the expectations J[a] from collected samples\n",
    "                Generate p = random number in [0,1]\n",
    "                if p < epsilon:\n",
    "                    a = random action\n",
    "                else:\n",
    "                    a = argmax_b(J)\n",
    "                execute action a\n",
    "\n",
    "* In order to stop the \"exploration\" at some point, one typically decreases epsilon as samples are collected. Some researchers call this \"cooling\" in the literature, and implement epsilon as a function that decays fast as t goes to infinity (or as the number of sample grows if I understood properly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc89072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f84094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df1a72fc",
   "metadata": {},
   "source": [
    "## Appendix A: Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d90b0e",
   "metadata": {},
   "source": [
    "### a) Non windy GridWorld\n",
    "\n",
    "The helper function only. Make sure the class has been compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce9e0ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_standard_grid():\n",
    "    # Start at bottom left (randomize later)\n",
    "    ini_state = (2,0)\n",
    "    # Action space \n",
    "    ACTION_SPACE = {\"U\", \"D\", \"L\", \"R\"}\n",
    "    # Non terminal states\n",
    "    NON_TERMINAL_STATES = {(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2), (2,3)}\n",
    "    # Terminal states\n",
    "    TERMINAL_STATES = {(0,3), (1,3)}\n",
    "    \n",
    "    # Instantiate:\n",
    "    # \n",
    "    env = GridWorld_Windy_small(3, 4, ini_state, NON_TERMINAL_STATES, TERMINAL_STATES, ACTION_SPACE)\n",
    "\n",
    "    \n",
    "    # Dictionary of rewards\n",
    "    # Not storing 0s\n",
    "    rewards = {(0,3):1, (1,3): -1}\n",
    "    \n",
    "    # Dictionary of admissible actions per state\n",
    "    adm_actions = {\n",
    "        (0,0): (\"D\", \"R\"),\n",
    "        (0,1): (\"L\", \"R\"),\n",
    "        (0,2): (\"L\", \"R\", \"D\"),\n",
    "        (1,0): (\"D\", \"U\"),\n",
    "        (1,2): (\"U\", \"D\", \"R\"),\n",
    "        (2,0): (\"U\", \"R\"),\n",
    "        (2,1): (\"L\", \"R\"),\n",
    "        (2,2): (\"U\", \"R\", \"L\"),\n",
    "        (2,3): (\"U\", \"L\"),\n",
    "    }\n",
    "    \n",
    "    # Dictionary of deterministic transitions:\n",
    "    transition_probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        \n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        \n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        \n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        \n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        \n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        \n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((1, 2), 'U'): {(0, 2): 1.0},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "    }\n",
    "    \n",
    "    # Assign missing environment attributes\n",
    "    env.set(rewards, adm_actions, transition_probs)\n",
    "    \n",
    "    # Output line\n",
    "    return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0162bd91",
   "metadata": {},
   "source": [
    "### b) Generate Episode - First version\n",
    "\n",
    "This first version was inefficient. Storing the episode as a list of tuples requires the generation of new lists of states and returns for the computation of the cumulative returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a8d5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "### GENERATE EPISODE - V.1 ###\n",
    "##############################\n",
    "# 2022/04/25, AJ Zerouali\n",
    "# Format: Epsd = [(0, s_0, a_0), (r_1, s_1, a_1), ..., (r_T, s_T, '')]\n",
    "\n",
    "def generate_episode(s_0, a_0, Pi, env, T_max):\n",
    "    # ARGUMENTS: Initial state and action; policy; environment; max episode length.\n",
    "    # OUTPUT: Epsd: List of tuples (r_t, s_t, a_t)\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Step t=0\n",
    "    s_new = s_0\n",
    "    a_new = a_0\n",
    "    r_new = 0\n",
    "    \n",
    "    # Init. episode list of tuples (format: step_t = (r_t, s_t, a_t))\n",
    "    Epsd = []\n",
    "    # Store step 0\n",
    "    Epsd.append( (r_new, s_new, a_new) )\n",
    "        \n",
    "    # Init. episode length\n",
    "    T = 0\n",
    "    \n",
    "    ##### 0<t\n",
    "    while (s_new not in term_states) and (T<T_max):\n",
    "        \n",
    "        # Init. old step\n",
    "        r_old = r_new\n",
    "        s_old = s_new\n",
    "        a_old = a_new\n",
    "        \n",
    "        # Compute new state\n",
    "        if a_old == 'U':\n",
    "            s_new = (s_old[0]-1, s_old[1])\n",
    "        elif a_old == 'D':\n",
    "            s_new = (s_old[0]+1, s_old[1])\n",
    "        elif a_old == 'L':\n",
    "            s_new = (s_old[0], s_old[1]-1)\n",
    "        elif a_old == 'R':\n",
    "            s_new = (s_old[0], s_old[1]+1)\n",
    "        \n",
    "        # Compute new action\n",
    "        if s_new in non_term_states:\n",
    "            a_new = list(Pi[s_new].keys())[0]\n",
    "        elif s_new in term_states:\n",
    "            a_new = ''\n",
    "        \n",
    "        # Compute new reward\n",
    "        r_new = Rwds.get(s_new, 0)\n",
    "        \n",
    "        # Add step to episode\n",
    "        Epsd.append( (r_new, s_new, a_new) )\n",
    "        \n",
    "        # Update\n",
    "        T += 1\n",
    "        \n",
    "    # END WHILE\n",
    "    \n",
    "    # Output line\n",
    "    return Epsd, T\n",
    "    \n",
    "# END DEF generate_episode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2e8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "### GENERATE EPISODE TEST ###\n",
    "#############################\n",
    "\n",
    "# a_random = actions_list[np.random.randint(len(actions_list))]\n",
    "\n",
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Grid attributes\n",
    "non_term_states = grid.non_term_states\n",
    "term_states = grid.term_states\n",
    "adm_actions = grid.adm_actions\n",
    "Rwds = grid.rewards\n",
    "\n",
    "Pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "N_samples = 0\n",
    "T_max = 50\n",
    "while N_samples<1:\n",
    "    ##### t=0\n",
    "    # Count no. of non_term_states\n",
    "    N_non_term_states = len(non_term_states)\n",
    "    # Generate s_0 randomly from non_term_states, get 1st step of episode\n",
    "    #r_new = 0\n",
    "    s_0 = list(non_term_states)[np.random.randint(N_non_term_states)]\n",
    "    a_0 = list(Pi[s_0].keys())[0]\n",
    "    \n",
    "    # Signature: Epsd, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "    Epsd, T = generate_episode(s_0, a_0, Pi, grid, T_max)\n",
    "    \n",
    "     \n",
    "    N_samples += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78135293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecfac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce6cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a88bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41735bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c11db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
