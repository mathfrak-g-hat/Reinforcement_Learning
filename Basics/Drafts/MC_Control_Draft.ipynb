{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f43c12",
   "metadata": {},
   "source": [
    "# **Monte Carlo Control (Draft)**\n",
    "### 2022/05/02, A. J. Zerouali\n",
    "\n",
    " \n",
    "\n",
    "## **1 - Introduction**\n",
    "\n",
    "### a) Contents\n",
    "\n",
    "This notebook is a debug file for my Monte Carlo impelementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4444eace",
   "metadata": {},
   "source": [
    "### b) Import cells\n",
    "\n",
    "We'll continue working with GridWorld. I'll also include some helper functions written previously for DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa18e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### IMPORTANT: ALWAYS EXECUTE THIS CELL FIRST #####\n",
    "#####################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed61960",
   "metadata": {},
   "source": [
    "Import the Windy GridWorld class and helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24ba169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Windy_GridWorld import GridWorld_Windy_small, windy_standard_grid, test_standard_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca32ab7",
   "metadata": {},
   "source": [
    "Import printing functions for value fn and policy, random episode generator, and value fn comparing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "490677c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RL_Fns_Windy_GridWorld import print_values, print_policy, gen_random_policy, compare_value_fns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f23b302",
   "metadata": {},
   "source": [
    "Import episode generating function and Monte Carlo evaluation of policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766bd1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Monte_Carlo_Windy_GridWorld import generate_episode, MC_Policy_Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa106a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function generate_episode in module Monte_Carlo_Windy_GridWorld:\n",
      "\n",
      "generate_episode(s_0, a_0, Pi, env, T_max)\n",
      "    Generates a random episode of max length (T_max + 1), given an initial state-action.\n",
      "    ARGUMENTS: Initial state and action; policy; environment; max episode length (in this order).\n",
      "    OUTPUT: episode_rewards: Rewards list\n",
      "            episode_states: State list\n",
      "            episode_actions: Actions list\n",
      "            T: Episode length minus one.\n",
      "            \n",
      "    NOTE: The function below is taylored to GridWorld. The correct way\n",
      "         to implement it is to use methods of the environment class.\n",
      "         I'm not using the methods of the Windy_GridWorld class because\n",
      "         the instructor's implementation is a little too clunky and the\n",
      "         entire design should be redone from scratch.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(generate_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbdbdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63094339",
   "metadata": {},
   "source": [
    "#### IMPORTANT:\n",
    "\n",
    "Execute all cells before the one above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7886a60",
   "metadata": {},
   "source": [
    "## 2 - Monte Carlo control - Exploring starts\n",
    "\n",
    "MC with exploring starts pseudocode\n",
    "\n",
    "            Require: Discount factor gamma.\n",
    "            Initialize: Random policy Pi, Q(s,a)=0 for all state-actions (s,a),\n",
    "                        create returns dict. and lists returns[(s,a)]=[]\n",
    "            Loop until convergence:\n",
    "                Generate initial state-action (s_0, a_0) randomly.\n",
    "                Generate episode sample of length T following Pi: (s_0, a_0, r_1, ... a_(T-1), r_T, s_T)\n",
    "                G = 0                                              # Init. cumulative return of generated episode\n",
    "                for t in {(T-1), (T-2), ..., 1, 0}:\n",
    "                    G = gamma*G + r_(t+1)                          # Evaluate sample return\n",
    "                    if (s_t, a_t) not in {(s_i,a_i)}_{i=0,...,t-1}:# If this is the first visit of (s_t,a_t)\n",
    "                        Append G to returns[s_t,a_t]               # Store sample cum. return\n",
    "                        Q[s_t, a_t] = average(returns[s_t, a_t])   # Update Q(s_t, a_t)\n",
    "                        Pi[s_t] = argmax_a {Q[s_t, a]}             # Update Pi*(s_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad83fd7",
   "metadata": {},
   "source": [
    "### a) Version 2\n",
    "\n",
    "In version 2, I computed the value function and the optimal policy with 2 approaches:\n",
    "\n",
    "- One is a modification of Version 1 (which had bugs).\n",
    "\n",
    "- The second is a modification of Lazy Programmer's version of MC control with exploring starts.\n",
    "\n",
    "Both give the same result, and both have the issue of the values $V(2,2)$ and $V(2,3)$ being different from those obtained by MC policy evaluation. More details below. I plotted the same graph as the instructor with the learning rate (1/no. samples), but the results were very erratic. I then tried to execute the code with 30000 samples, a random initial policy, and max length of episode of 51. This crashed Jupyter, and I had to erase the file because I corrupted it.\n",
    "\n",
    "That execution crashed because I was printing-out an infinite loop. The random policy was alternating between 3 states ad infinitum, thus generating samples of maximal length, and since I was printing out contents from the for loop over time, I ended up overflowing the Jupyter memory allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295d29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83378e75",
   "metadata": {},
   "source": [
    "### b) Version 3\n",
    "\n",
    "In version 3, I just fixed version 1. I think the discrepancy in $V(2,2)$ and $V(2,3)$ between the control function and policy evaluation function comes from a sampling issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "434eb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "###  MONTE CARLO CONTROL WITH EXPLORING STARTS - VERSION 3 ###\n",
    "##############################################################\n",
    "## 2022/05/02, AJ Zerouali\n",
    "# Monte Carlo with exploring starts (Lect. 64-65)\n",
    "# We are not using convergence thresholds here.\n",
    "# This function below generates a specified number of sample\n",
    "# episodes and averages returns to find the optimal policy.\n",
    "# Can choose first visit MC or all visits MC with Boolean.\n",
    "# This function also calls generate_episode() of previous sec.\n",
    "\n",
    "def MC_Ctrl_ExpStarts(Pi, env, gamma, N_samples, T_max, all_visits_MC):\n",
    "    '''\n",
    "     Monte Carlo control with exploring starts\n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - N_samples: No. of samples for Monte Carlo expectation.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - all_visits_MC: Boolean for all visits or first visit MC.\n",
    "     OUTPUT:    - Pi_star: Optimal policy\n",
    "                - V_star: Optimal value function\n",
    "                - Q: State-action values from samples\n",
    "                - Returns: Dict. of return samples (by init. state-action)\n",
    "                - N_iter: No. of randomly generated episodes (<= N_samples)\n",
    "     NOTE: This function calls generate_episode().\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    \n",
    "    # Init. Q, returns, Pi_star, and V_star dictionaries\n",
    "    Pi_star = {}\n",
    "    Q = {}\n",
    "    Returns = {}\n",
    "    V_star = {}\n",
    "    for s in non_term_states:\n",
    "        Pi_star[s] = {}\n",
    "        Q[s]={}\n",
    "        Returns[s] = {}\n",
    "        V_star[s] = 0.0\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            Returns[s][a]=[]\n",
    "            \n",
    "    for s in term_states:\n",
    "        V_star[s] = 0.0\n",
    "            \n",
    "    \n",
    "    # Init. counter\n",
    "    N_iter = 0\n",
    "    \n",
    "    # Main MC loop\n",
    "    while (N_iter<N_samples):\n",
    "                \n",
    "        ##########################\n",
    "        ##### Step t=0 setup #####\n",
    "        ##########################\n",
    "\n",
    "        # Generate (s_0, a_0) randomly from non_term_states and adm_actions\n",
    "        # np.random.choice() works only for 1-dim'l arrays\n",
    "        s_0 = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        a_0 = np.random.choice(adm_actions[s_0])\n",
    "\n",
    "        ########################\n",
    "        ##### Steps 1 to T #####\n",
    "        ########################\n",
    "        \n",
    "        #print(f\"Generating sample episode no. {N_iter+1}...\\n\") # Debug\n",
    "        # Generate episode\n",
    "        # Signature: episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, grid, T_max)\n",
    "        # Step t of episode is (r_t, s_t, a_t) = (episode_rewards[t], episode_states[t], episode_actions[t])\n",
    "        #rint(f\"Sample episode N_iter={N_iter} has T={T} steps after t=0.\\n\") # Debug\n",
    " \n",
    "        #####################################################\n",
    "        ### COMPUTE CUMULATIVE RETURNS AND OPTIMAL ACTION ###\n",
    "        #####################################################\n",
    "\n",
    "        # First visit only MC\n",
    "        if not all_visits_MC:\n",
    "            # State-action iterable\n",
    "            episode_state_actions = list(zip(episode_states, episode_actions))\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            # Loop over episode\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the return\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                if (s_t, a_t) not in episode_state_actions[:t]:\n",
    "                    \n",
    "                    # Update sample returns and Q-function\n",
    "                    Returns[s_t][a_t].append(G)\n",
    "                    Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                    \n",
    "                    \n",
    "                    # Get a_star and update Pi_star\n",
    "                    a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                    Pi_star[s_t] = {a_star:1.0} \n",
    "                    \n",
    "                    \n",
    "        # END IF first visit MC\n",
    "        \n",
    "        # All visits MC\n",
    "        elif all_visits_MC:\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the returns\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                # Update sample returns and Q-function\n",
    "                Returns[s_t][a_t].append(G)\n",
    "                Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                \n",
    "                \n",
    "                # Get a_star and update Pi_star\n",
    "                a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                Pi_star[s_t] = {a_star:1.0} \n",
    "                \n",
    "                              \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "        \n",
    "        '''\n",
    "        # DEBUG:\n",
    "        print(f\"Completed episode N_iter={N_iter} with R={G}.\")\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        '''\n",
    "       \n",
    "    # END WHILE of MC loop\n",
    "    \n",
    "    # COMPUTE V\n",
    "    for s in non_term_states:\n",
    "        a = max(Q[s], key = Q[s].get)\n",
    "        V_star[s]=Q[s][a]\n",
    "    \n",
    "    # Output\n",
    "    return Pi_star, V_star, Q, Returns, N_iter\n",
    "\n",
    "# END DEF MC_Ctrl_ExpStarts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092c233",
   "metadata": {},
   "source": [
    "#### Test with non-windy GridWorld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b6d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Policy to be evaluated\n",
    "Pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "#epsilon = 1e-3\n",
    "\n",
    "# Max. episode length:\n",
    "T_max = 50\n",
    "N_samples = 1000\n",
    "\n",
    "# Evaluate V_Pi:\n",
    "# SIGNATURE: Pi_star, V_star, Q, Returns = MC_Ctrl_ExpStarts(Pi, env, gamma, N_samples, T_max, all_visits_MC)\n",
    "#V = MC_Policy_Eval(Pi, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "Pi_star, V_star, Q, Returns, N_iter = MC_Ctrl_ExpStarts(Pi_lect, grid, gamma, N_samples, T_max, all_visits_MC = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf136c",
   "metadata": {},
   "source": [
    "The policy obtained looks optimal (although that depends on the number of samples collected):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55b4f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_policy(Pi_star, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6163f1",
   "metadata": {},
   "source": [
    "The value function is off though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f13352f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59|-0.73|-0.81|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_star, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e114ff8a",
   "metadata": {},
   "source": [
    "If we evaluate the optimal policy with MC, we get a different value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5839379",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_eval = MC_Policy_Eval(Pi_star, grid, gamma, 50, T_max, all_visits_MC = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6bb51fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_eval, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ee67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17ff52d9",
   "metadata": {},
   "source": [
    "## 3 - Monte Carlo control - Epsilon greedy policy\n",
    "\n",
    "Turning to MC without exploring starts. Main points:\n",
    "\n",
    "* When performing control with a given policy, it is possible to miss certain state-action pairs that aren't visited by the policy.\n",
    "\n",
    "* In principle, would need to fill the dictionary $Q_pi(s,a)$ for all $(s,a)\\in\\mathcal{S}\\times \\mathcal{A}_s$.\n",
    "\n",
    "* First new change is that the policy built is $\\varepsilon$-soft. In particular, we will build an $\\varepsilon$-greedy policy.\n",
    "\n",
    "* **Definition:** (Sutton-Barto, p.100) A policy is said to be soft if $\\pi(a|s)>0$ for all $(s,a)\\in\\mathcal{S}\\times\\mathcal{A}_s$, and gradually shifts closer to an optimal deterministic policy.\n",
    "\n",
    "* **Question:** Why is he resetting to one initial state $s_0$?\n",
    "\n",
    "* The new part in the implementation is how $a^\\ast$ is chosen in the policy update at time $t$. If $a^\\ast=\\arg\\max_a Q(s_t,a)$, choose \\$pi(a|s_t)=(1-\\varepsilon)+(\\varepsilon/|\\mathcal{A}|)$ if $a=a^\\ast$ and $\\pi(a|s_t)=\\varepsilon/|\\mathcal{A}|$ otherwise.\n",
    "\n",
    "* Lazy Programmer proposes a function that chooses an epsilon-greedy action.\n",
    "\n",
    "            def epsilon_greedy(Q,s,eps):\n",
    "                if random() < eps:\n",
    "                    return random action\n",
    "                else:\n",
    "                    return argmax_a Q(s,a)\n",
    "\n",
    "* Whereas exploring starts was an on-policy algorithm, the version without exploring starts is off-policy.\n",
    "\n",
    "* Will write an $\\varepsilon$-soft policy generator. For the execution however, will have to modify the policy to avoid infinite loops when testing MC control.\n",
    "\n",
    "* Will also have to write an episode generator following a stochastic policy. print_policy() will be useless then.\n",
    "\n",
    "\n",
    "\n",
    "The \"epsilon-greedy\" Monte Carlo pseudocode is as follows:\n",
    "\n",
    "            Require: Discount factor gamma.\n",
    "            Initialize: Pi = arbitrary eps-soft policy with Pi(a|s)>0 for all s and all a in adm_actions[s],\n",
    "                        Q(s,a)=0 for all state-actions (s,a),\n",
    "                        create returns dict. and lists returns[(s,a)]=[].\n",
    "            Loop until convergence:\n",
    "                Reset to initial state s_0.\n",
    "                Generate episode sample of length T following Pi: (s_0, a_0, r_1, ... a_(T-1), r_T, s_T)\n",
    "                G = 0                                              # Init. cumulative return of generated episode\n",
    "                for t in {(T-1), (T-2), ..., 1, 0}:\n",
    "                    G = gamma*G + r_(t+1)                          # Evaluate sample return\n",
    "                    if (s_t, a_t) not in {(s_i,a_i)}_{i=0,...,t-1}:# If this is the first visit of (s_t,a_t)\n",
    "                        Append G to returns[s_t,a_t]               # Store sample cum. return\n",
    "                        Q[s_t, a_t] = average(returns[s_t, a_t])   # Update Q(s_t, a_t)\n",
    "                        a* = argmax_a Q[s_t,a]\n",
    "                        for a in adm_actions[s_t]:                 # Update Pi*[a|s_t], eps-greedy policy\n",
    "                            if a==a*:\n",
    "                                Pi[a|s_t] = 1-eps+(eps/len(adm_actions[s_t]))\n",
    "                            else:\n",
    "                                Pi[a|s_t] = eps/len(adm_actions[s_t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cdcb4",
   "metadata": {},
   "source": [
    "#### Random eps-soft policy generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea79d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "##### RANDOM EPSILON-SOFT POLICY GENERATOR #####\n",
    "################################################\n",
    "## 2022/05/03, AJ Zerouali\n",
    "# Recall: rand_ind = np.random.choice(a = len(next_states), p = next_states_probs)\n",
    "\n",
    "\n",
    "def gen_random_epslnsoft_policy(eps, env):\n",
    "    '''\n",
    "      Generates a random epsilon-soft policy for Windy GridWorld.\n",
    "      ARGUMENTS: - eps, the epsilon float;\n",
    "                 - env, Windy_GridWorld_simple object (environment).\n",
    "      OUTPUT: Pi, an epsilon-soft policy dictionary.\n",
    "      Note: eps should be between 5% and 10%.\n",
    "    '''\n",
    "    non_term_states = env.non_term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Pi = {}\n",
    "    \n",
    "    for s in non_term_states:\n",
    "        Pi[s]={}\n",
    "        actions_list_s = list(adm_actions[s])\n",
    "        a_rand = np.random.choice(actions_list_s)\n",
    "        for a in actions_list_s:\n",
    "            if a==a_rand:\n",
    "                Pi[s][a] = 1-eps+(eps/len(actions_list_s))\n",
    "            else:\n",
    "                Pi[s][a] = eps/len(actions_list_s)\n",
    "    \n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654e08f3",
   "metadata": {},
   "source": [
    "#### Random episode generator with stochastic policy\n",
    "\n",
    "Will use np.random.choice. Given a 1-D array-like object and a list of corresponding probabilities, this function picks one element at random (assuming size has no value). The main issue is this 1-D array object which can only be numbers, not characters or other objects. The 1-D array should therefore be a list of indices, and we need to specify the list of probabilities with p= list of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1dccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "### GENERATE EPISODE FROM STOCHASTIC POLICY ###\n",
    "###############################################\n",
    "# 2022/05/03, AJ Zerouali\n",
    "# Format: Epsd = [(0, s_0, a_0), (r_1, s_1, a_1), ..., (r_T, s_T, '')]\n",
    "# \n",
    "\n",
    "def generate_episode_stochastic(s_0, a_0, Pi, env, T_max):\n",
    "    '''\n",
    "     Generates a random episode of max length (T_max + 1), given an initial state-action.\n",
    "     ARGUMENTS: Initial state and action; stochastic policy; environment; max episode length (in this order).\n",
    "     OUTPUT: episode_rewards: Rewards list\n",
    "             episode_states: State list\n",
    "             episode_actions: Actions list\n",
    "             T: Episode length minus one.\n",
    "             \n",
    "     NOTE: The function below is taylored to GridWorld. The correct way\n",
    "          to implement it is to use methods of the environment class.\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Step t=0\n",
    "    s_new = s_0\n",
    "    a_new = a_0\n",
    "    r_new = 0\n",
    "    \n",
    "    # Init. episode lists (format: step_t = (r_t, s_t, a_t)) and store Step 0\n",
    "    episode_rewards = [r_new]\n",
    "    episode_states = [s_new]\n",
    "    episode_actions = [a_new]\n",
    "    \n",
    "        \n",
    "    # Init. episode length\n",
    "    T = 0\n",
    "    \n",
    "    ##### 0<t\n",
    "    while (s_new not in term_states) and (T<T_max):\n",
    "        \n",
    "        # Init. old step\n",
    "        r_old = r_new\n",
    "        s_old = s_new\n",
    "        a_old = a_new\n",
    "        \n",
    "        # WARNING: Modify for other environments\n",
    "        # Compute new state\n",
    "        if a_old == 'U':\n",
    "            s_new = (s_old[0]-1, s_old[1])\n",
    "        elif a_old == 'D':\n",
    "            s_new = (s_old[0]+1, s_old[1])\n",
    "        elif a_old == 'L':\n",
    "            s_new = (s_old[0], s_old[1]-1)\n",
    "        elif a_old == 'R':\n",
    "            s_new = (s_old[0], s_old[1]+1)\n",
    "        \n",
    "        # Compute new action\n",
    "        ###################################\n",
    "        # CHANGE THIS FOR STOCHASTIC POLICY\n",
    "        ###################################\n",
    "        if s_new in non_term_states:\n",
    "            # Here we are assuming that Pi[s].keys() is the same as adm_actions[s]\n",
    "            actions_list_s_new = list(Pi[s_new].keys())\n",
    "            probs_list_s_new = list(Pi[s_new].values())\n",
    "            rand_ind = np.random.choice(len(actions_list_s_new), p = probs_list_s_new)\n",
    "            # Pick action from random index\n",
    "            a_new = actions_list_s_new[rand_ind]\n",
    "        elif s_new in term_states:\n",
    "            a_new = ''\n",
    "        \n",
    "        # Compute new reward\n",
    "        r_new = Rwds.get(s_new, 0)\n",
    "        \n",
    "        # Add step to episode\n",
    "        episode_rewards.append(r_new)\n",
    "        episode_states.append(s_new)\n",
    "        episode_actions.append(a_new)\n",
    "        \n",
    "        # Update\n",
    "        T += 1\n",
    "        \n",
    "    # END WHILE\n",
    "    \n",
    "    # Output line\n",
    "    return episode_rewards, episode_states, episode_actions, T\n",
    "    \n",
    "# END DEF generate_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d16573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c3af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2091b525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "###  MONTE CARLO CONTROL WITH EPSILON-GREEDY POLICY ###\n",
    "#######################################################\n",
    "## 2022/05/02, AJ Zerouali\n",
    "# Monte Carlo without exploring starts (Lect. 64-65), following\n",
    "# an epsilon greedy scheme.\n",
    "# We are not using convergence thresholds here.\n",
    "# This function below generates a specified number of sample\n",
    "# episodes and averages returns to find the optimal policy.\n",
    "# Can choose first visit MC or all visits MC with Boolean.\n",
    "# This function also calls generate_episode() of previous sec.\n",
    "\n",
    "def MC_Ctrl_EpsGreedy(Pi, eps, env, gamma, N_samples, T_max, all_visits_MC):\n",
    "    '''\n",
    "     Epsilon-greedy Monte Carlo control algorithm.\n",
    "     \n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - eps: Epsilon float for output policy.\n",
    "                - N_samples: No. of samples for Monte Carlo expectation.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - all_visits_MC: Boolean for all visits or first visit MC.\n",
    "     OUTPUT:    - Pi_star: Optimal policy\n",
    "                - V_star: Optimal value function\n",
    "                - Q: State-action values from samples\n",
    "                - Returns: Dict. of return samples (by init. state-action)\n",
    "                - N_iter: No. of randomly generated episodes (<= N_samples)\n",
    "     NOTE: This function calls generate_episode().\n",
    "           Should take an eps-soft policy as input.\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    \n",
    "    # Init. Q, returns, Pi_star, and V_star dictionaries\n",
    "    Pi_star = {}\n",
    "    Q = {}\n",
    "    Returns = {}\n",
    "    V_star = {}\n",
    "    for s in non_term_states:\n",
    "        Pi_star[s] = {}\n",
    "        Q[s]={}\n",
    "        Returns[s] = {}\n",
    "        V_star[s] = 0.0\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            Returns[s][a]=[]\n",
    "            \n",
    "    for s in term_states:\n",
    "        V_star[s] = 0.0\n",
    "            \n",
    "    \n",
    "    # Init. counter\n",
    "    N_iter = 0\n",
    "    \n",
    "    # Main MC loop\n",
    "    while (N_iter<N_samples):\n",
    "                \n",
    "        ##########################\n",
    "        ##### Step t=0 setup #####\n",
    "        ##########################\n",
    "\n",
    "        # Generate (s_0, a_0) randomly from non_term_states and adm_actions\n",
    "        # np.random.choice() works only for 1-dim'l arrays\n",
    "        s_0 = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        a_0 = np.random.choice(adm_actions[s_0])\n",
    "\n",
    "        ########################\n",
    "        ##### Steps 1 to T #####\n",
    "        ########################\n",
    "        \n",
    "        #print(f\"Generating sample episode no. {N_iter+1}...\\n\") # Debug\n",
    "        # Generate episode\n",
    "        # Signature: episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        # Step t of episode is (r_t, s_t, a_t) = (episode_rewards[t], episode_states[t], episode_actions[t])\n",
    "        episode_rewards, episode_states, episode_actions, T = generate_episode_stochastic(s_0, a_0, Pi, grid, T_max)\n",
    "        #print(f\"Sample episode N_iter={N_iter} has T={T} steps after t=0.\\n\") # Debug\n",
    " \n",
    "        #####################################################\n",
    "        ### COMPUTE CUMULATIVE RETURNS AND OPTIMAL ACTION ###\n",
    "        #####################################################\n",
    "\n",
    "        # First visit only MC\n",
    "        if not all_visits_MC:\n",
    "            # State-action iterable\n",
    "            episode_state_actions = list(zip(episode_states, episode_actions))\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            # Loop over episode\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the return\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                if (s_t, a_t) not in episode_state_actions[:t]:\n",
    "                    \n",
    "                    # Update sample returns and Q-function\n",
    "                    Returns[s_t][a_t].append(G)\n",
    "                    Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                    \n",
    "                    # Get a_star and update Pi_star\n",
    "                    a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                    Pi_star[s_t][a_star] = 1-eps+eps/len(adm_actions[s_t])\n",
    "                    for a in adm_actions[s_t]:\n",
    "                        if a != a_star:\n",
    "                            Pi_star[s_t][a] = eps/len(adm_actions[s_t])\n",
    "                \n",
    "                    \n",
    "        # END IF first visit MC\n",
    "        \n",
    "        # All visits MC\n",
    "        elif all_visits_MC:\n",
    "            \n",
    "            # Init. storing variable\n",
    "            G = 0.0\n",
    "            \n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                \n",
    "                # Sum the returns\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                # Update sample returns and Q-function\n",
    "                Returns[s_t][a_t].append(G)\n",
    "                Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                \n",
    "                # Get a_star and update Pi_star\n",
    "                a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                Pi_star[s_t][a_star] = 1-eps+eps/len(adm_actions[s_t])\n",
    "                for a in adm_actions[s_t]:\n",
    "                    if a != a_star:\n",
    "                        Pi_star[s_t][a] = eps/len(adm_actions[s_t])\n",
    "                \n",
    "                              \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "        \n",
    "        '''\n",
    "        # DEBUG:\n",
    "        print(f\"Completed episode N_iter={N_iter} with R={G}.\")\n",
    "        print(\"--------------------------------------------------------------------\")\n",
    "        '''\n",
    "       \n",
    "    # END WHILE of MC loop\n",
    "    \n",
    "    # COMPUTE V\n",
    "    ## CHANGE IN EPSILON GREEDY??\n",
    "    for s in non_term_states:\n",
    "        a = max(Q[s], key = Q[s].get)\n",
    "        V_star[s]=Q[s][a]\n",
    "    \n",
    "    # Output\n",
    "    return Pi_star, V_star, Q, Returns, N_iter\n",
    "\n",
    "# END DEF MC_Ctrl_EpsGreedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac144c4a",
   "metadata": {},
   "source": [
    "#### Test with non-windy GridWorld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b495e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished running MC_Ctrl_EpsGreedy with N_iter=5000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Discount factor, epsilon-greedy probability\n",
    "eps = 0.1\n",
    "gamma = 0.9\n",
    "#epsilon = 1e-3\n",
    "\n",
    "# Policies\n",
    "Pi_eps_ini = {\n",
    "    (2, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 0)]), 'R':eps/len(grid.adm_actions[(2, 0)])},\n",
    "    (1, 0): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 0)]), 'D':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 0): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 0)]), 'D':eps/len(grid.adm_actions[(0, 0)])},\n",
    "    (0, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 1)]), 'L':eps/len(grid.adm_actions[(1, 0)])},\n",
    "    (0, 2): {'R': (1-eps)+eps/len(grid.adm_actions[(0, 2)]), 'D':eps/len(grid.adm_actions[(0, 2)]),\\\n",
    "                                                             'L':eps/len(grid.adm_actions[(0, 2)])},\n",
    "    (1, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(1, 2)]), 'D':eps/len(grid.adm_actions[(1, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(1, 2)])},\n",
    "    (2, 1): {'R': (1-eps)+eps/len(grid.adm_actions[(2, 1)]), 'L':eps/len(grid.adm_actions[(2, 1)])},\n",
    "    (2, 2): {'U': (1-eps)+eps/len(grid.adm_actions[(2, 2)]), 'L':eps/len(grid.adm_actions[(2, 2)]),\\\n",
    "                                                             'R':eps/len(grid.adm_actions[(2, 2)])},\n",
    "    (2, 3): {'L': (1-eps)+eps/len(grid.adm_actions[(2, 3)]), 'U':eps/len(grid.adm_actions[(2, 3)])},\n",
    "  }\n",
    "\n",
    "Pi_unif = {}\n",
    "for s in grid.non_term_states:\n",
    "    Pi_unif[s]={}\n",
    "    for a in grid.adm_actions[s]:\n",
    "        Pi_unif[s][a] = 1/len(grid.adm_actions[s])\n",
    "\n",
    "Pi_opt = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "# Max. episode length:\n",
    "T_max = 200\n",
    "N_samples = 5000\n",
    "eps_2 = 0.05\n",
    "\n",
    "# Evaluate V_Pi:\n",
    "Pi_star, V_star, Q, Returns, N_iter = MC_Ctrl_EpsGreedy(Pi_unif, eps_2, grid, gamma, N_samples, T_max,\\\n",
    "                                                        all_visits_MC = False)\n",
    "\n",
    "print(f\"Finished running MC_Ctrl_EpsGreedy with N_iter={N_iter} samples.\")\n",
    "\n",
    "# MC policy evaluation needs a stochastic version too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dc89072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): 0.2653558492526721,\n",
       " (1, 2): 0.26344629585996776,\n",
       " (2, 1): -0.09710571099198803,\n",
       " (0, 0): 0.1538743819135216,\n",
       " (2, 0): -0.02086175870654055,\n",
       " (2, 3): -0.31735544576054125,\n",
       " (0, 2): 1.0,\n",
       " (2, 2): -0.1879232051953183,\n",
       " (1, 0): 0.05649572956105423,\n",
       " (0, 3): 0.0,\n",
       " (1, 3): 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a3ba08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.12| 0.22| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.04| 0.00| 0.23| 0.00|\n",
      "------------------------\n",
      "-0.02|-0.10|-0.19|-0.34|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_star,grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12adc496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): {'R': 0.975, 'L': 0.025},\n",
       " (1, 2): {'U': 0.9666666666666667,\n",
       "  'D': 0.016666666666666666,\n",
       "  'R': 0.016666666666666666},\n",
       " (2, 1): {'R': 0.025, 'L': 0.975},\n",
       " (0, 0): {'R': 0.975, 'D': 0.025},\n",
       " (2, 0): {'R': 0.025, 'U': 0.975},\n",
       " (2, 3): {'U': 0.025, 'L': 0.975},\n",
       " (0, 2): {'R': 0.9666666666666667,\n",
       "  'L': 0.016666666666666666,\n",
       "  'D': 0.016666666666666666},\n",
       " (2, 2): {'R': 0.016666666666666666,\n",
       "  'U': 0.016666666666666666,\n",
       "  'L': 0.9666666666666667},\n",
       " (1, 0): {'U': 0.975, 'D': 0.025}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e15627fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): {'L': 0.5, 'R': 0.5},\n",
       " (1, 2): {'U': 0.3333333333333333,\n",
       "  'D': 0.3333333333333333,\n",
       "  'R': 0.3333333333333333},\n",
       " (2, 1): {'L': 0.5, 'R': 0.5},\n",
       " (0, 0): {'D': 0.5, 'R': 0.5},\n",
       " (2, 0): {'U': 0.5, 'R': 0.5},\n",
       " (2, 3): {'U': 0.5, 'L': 0.5},\n",
       " (0, 2): {'L': 0.3333333333333333,\n",
       "  'R': 0.3333333333333333,\n",
       "  'D': 0.3333333333333333},\n",
       " (2, 2): {'U': 0.3333333333333333,\n",
       "  'R': 0.3333333333333333,\n",
       "  'L': 0.3333333333333333},\n",
       " (1, 0): {'D': 0.5, 'U': 0.5}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi_unif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eee68",
   "metadata": {},
   "source": [
    "#### MC stochastic policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5202e8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "###  MONTE CARLO POLICY EVALUATION ###\n",
    "######################################\n",
    "## 2022/05/03, AJ Zerouali\n",
    "\n",
    "def MC_Stochastic_Policy_Eval(Pi, env, gamma, N_samples, T_max, all_visits_MC):\n",
    "    '''\n",
    "    Function implementing Monte Carlo policy evaluation. Generates \n",
    "    a specified number of sample episodes and averages returns to \n",
    "    evaluate a given stochastic policy. \n",
    "    Can choose first visit MC or all visits MC with Boolean.\n",
    "    \n",
    "     ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "                - N_samples: No. of samples for Monte Carlo expectation.\n",
    "                - T_max: Max. episode length minus 1.\n",
    "                - all_visits_MC: Boolean for all visits or first visit MC.\n",
    "     OUTPUT:    - V:=V_Pi, value function obtained.\n",
    "     NOTE: This function calls generate_episode_stochastic().\n",
    "    '''\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    # Init output and returns\n",
    "    V = {}\n",
    "    Returns = {}\n",
    "    for s in term_states:\n",
    "        V[s] = 0.0\n",
    "    for s in non_term_states:\n",
    "        V[s] = 0.0\n",
    "        Returns[s]=[]\n",
    "        # Init. counter\n",
    "        N_iter = 0\n",
    "    \n",
    "    # Main MC loop\n",
    "    while N_iter < N_samples:\n",
    "\n",
    "        ##########################\n",
    "        ##### Step t=0 setup #####\n",
    "        ##########################\n",
    "\n",
    "        # Count no. of non_term_states\n",
    "        N_non_term_states = len(non_term_states)\n",
    "\n",
    "        # Generate s_0 randomly from non_term_states, get a_0 from policy\n",
    "        s_0 = list(non_term_states)[np.random.randint(N_non_term_states)]\n",
    "        a_0 = list(Pi[s_0].keys())[0]\n",
    "\n",
    "        ########################\n",
    "        ##### Steps 1 to T #####\n",
    "        ########################\n",
    "\n",
    "        # Generate episode\n",
    "        # Signature: episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        episode_rewards, episode_states, episode_actions, T = generate_episode_stochastic(s_0, a_0, Pi, env, T_max)\n",
    "        # Step t of episode is (r_t, s_t, a_t) = (episode_rewards[t], episode_states[t], episode_actions[t])\n",
    "\n",
    "        ##################################\n",
    "        ### COMPUTE CUMULATIVE RETURNS ###\n",
    "        ##################################\n",
    "\n",
    "        # Init. storing variable\n",
    "        G = 0.0\n",
    "        \n",
    "        # First visit only MC\n",
    "        if not all_visits_MC:\n",
    "            # Loop over episode\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                \n",
    "                if s_t not in episode_states[:t]:\n",
    "                    Returns[s_t].append(G)\n",
    "                    V[s_t] = np.average(Returns[s_t])\n",
    "                    \n",
    "        # All visits MC\n",
    "        elif all_visits_MC:\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                Returns[s_t].append(G)\n",
    "                V[s_t] = np.average(Returns[s_t])\n",
    "        \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "\n",
    "    # END WHILE of MC loop\n",
    "    \n",
    "    # Output\n",
    "    return V\n",
    "\n",
    "# END DEF MC_Stochastic_Policy_Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50415838",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = MC_Stochastic_Policy_Eval(Pi_star, grid, gamma, N_samples, 200, all_visits_MC=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90111fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.12| 0.22| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.04| 0.00| 0.23| 0.00|\n",
      "------------------------\n",
      "-0.02|-0.10|-0.19|-0.34|\n",
      "------------------------\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.80| 0.89| 0.99| 0.00|\n",
      "------------------------\n",
      " 0.72| 0.00| 0.89| 0.00|\n",
      "------------------------\n",
      " 0.60| 0.54| 0.44|-0.28|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "print_values(V_star, grid)\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7961599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d7edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d516ede9",
   "metadata": {},
   "source": [
    "#### Comments on epsilon-greedy strategies:\n",
    "\n",
    "* A greedy strategy can be described as a short-sighted one, in the sense that we disregard the number of samples collected and the confidence in the accuracy of some estimated expected return. More precisely, we use the available samples and simply take the maximizing action. In principle, this is a heuristic only. The pseudocode for a greedy policy would typically look like the following, where we evaluate have a predicted expectation $J$:\n",
    "\n",
    "            While True:\n",
    "                Compute the expectations J[a] from collected samples\n",
    "                a = argmax_b(J)\n",
    "                execute action a\n",
    "\n",
    "* An epsilon-greedy strategy aims to explore more data instead of only exploiting the collected samples. We fix a small probability $\\varepsilon\\sim 5\\%, 10\\%$ of taking a completely different action from the argmax. In contrast with the previous pseudocode, an epsilon-greedy algorithm is as follows:\n",
    "\n",
    "            While True:\n",
    "                Compute the expectations J[a] from collected samples\n",
    "                Generate p = random number in [0,1]\n",
    "                if p < epsilon:\n",
    "                    a = random action\n",
    "                else:\n",
    "                    a = argmax_b(J)\n",
    "                execute action a\n",
    "\n",
    "* In order to stop the \"exploration\" at some point, one typically decreases epsilon as samples are collected. Some researchers call this \"cooling\" in the literature, and implement epsilon as a function that decays fast as t goes to infinity (or as the number of sample grows if I understood properly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1a72fc",
   "metadata": {},
   "source": [
    "## Appendix A: Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d90b0e",
   "metadata": {},
   "source": [
    "### a) MC Control with exploring starts - Initial version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b18acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "###  MONTE CARLO CONTROL WITH EXPLORING STARTS ###\n",
    "##################################################\n",
    "## 2022/04/27, AJ Zerouali\n",
    "# Monte Carlo with exploring starts (Lect. 64-65)\n",
    "# We are not using convergence thresholds here.\n",
    "# This function below generates a specified number of sample\n",
    "# episodes and averages returns to find the optimal policy.\n",
    "# Can choose first visit MC or all visits MC with Boolean.\n",
    "# This function also calls generate_episode() of previous sec.\n",
    "\n",
    "def MC_Ctrl_ExpStarts(Pi, env, gamma, N_samples, T_max, all_visits_MC):\n",
    "    # ARGUMENTS: - Pi, env, gamma: Policy, environment, discount factor.\n",
    "    #            - N_samples: No. of samples for Monte Carlo expectation.\n",
    "    #            - T_max: Max. episode length minus 1.\n",
    "    #            - all_visits_MC: Boolean for all visits or first visit MC.\n",
    "    # OUTPUT:    - Pi_star: Optimal policy\n",
    "    #            - V_star: Optimal value function\n",
    "    # NOTE: This function calls generate_episode().\n",
    "    \n",
    "    # Environment attributes\n",
    "    non_term_states = env.non_term_states\n",
    "    term_states = env.term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Rwds = env.rewards\n",
    "    \n",
    "    \n",
    "    # Init. Q, returns, Pi_star, and V_star dictionaries\n",
    "    Pi_star = {}\n",
    "    Q = {}\n",
    "    Returns = {}\n",
    "    V_star = {}\n",
    "    for s in non_term_states:\n",
    "        Pi_star[s] = {}\n",
    "        Q[s]={}\n",
    "        Returns[s] = {}\n",
    "        V_star[s] = 0.0\n",
    "        for a in adm_actions[s]:\n",
    "            Q[s][a] = 0.0\n",
    "            Returns[s][a]=[]\n",
    "            \n",
    "    for s in term_states:\n",
    "        V_star[s] = 0.0\n",
    "            \n",
    "    \n",
    "    # Init. counter\n",
    "    N_iter = 0\n",
    "    \n",
    "    # Main MC loop\n",
    "    while N_iter < N_samples:\n",
    "\n",
    "        ##########################\n",
    "        ##### Step t=0 setup #####\n",
    "        ##########################\n",
    "\n",
    "        # Generate (s_0, a_0) randomly from non_term_states and adm_actions\n",
    "        # np.random.choice() works only for 1-dim'l arrays\n",
    "        s_0 = list(non_term_states)[np.random.randint(len(non_term_states))]\n",
    "        a_0 = np.random.choice(adm_actions[s_0])\n",
    "\n",
    "        ########################\n",
    "        ##### Steps 1 to T #####\n",
    "        ########################\n",
    "\n",
    "        # Generate episode\n",
    "        # Signature: episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, env, T_max)\n",
    "        episode_rewards, episode_states, episode_actions, T = generate_episode(s_0, a_0, Pi, grid, T_max)\n",
    "        # Step t of episode is (r_t, s_t, a_t) = (episode_rewards[t], episode_states[t], episode_actions[t])\n",
    "\n",
    "        #####################################################\n",
    "        ### COMPUTE CUMULATIVE RETURNS AND OPTIMAL ACTION ###\n",
    "        #####################################################\n",
    "\n",
    "        # Init. storing variable\n",
    "        G = 0.0\n",
    "        \n",
    "        # First visit only MC\n",
    "        if not all_visits_MC:\n",
    "            # State-action iterable\n",
    "            episode_state_actions = list(zip(episode_states, episode_actions))\n",
    "            \n",
    "            # Loop over episode\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                if (s_t, a_t) not in episode_state_actions[:t]:\n",
    "                    \n",
    "                    # Update sample returns and Q-function\n",
    "                    # WARNING: THERE SEEMS TO BE A PROBLEM HERE\n",
    "                    Returns[s_t][a_t].append(G)\n",
    "                    Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                    \n",
    "                    # Get a_star and update Pi_star\n",
    "                    a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                    Pi_star[s_t] = {a_star:1.0} \n",
    "                    \n",
    "                    # Update V_star\n",
    "                    V_star[s_t] = Q[s_t][a_star]\n",
    "                    \n",
    "                    \n",
    "        # All visits MC\n",
    "        elif all_visits_MC:\n",
    "            for t in range(T-1, -1, -1): # Loop goes backwards from (T-1) to 0\n",
    "                G = gamma*G + episode_rewards[t+1]\n",
    "                s_t = episode_states[t]\n",
    "                a_t = episode_actions[t]\n",
    "                \n",
    "                # Update sample returns and Q-function\n",
    "                Returns[s_t][a_t].append(G)\n",
    "                Q[s_t][a_t] = np.average(Returns[s_t][a_t])\n",
    "                \n",
    "                # Get a_star and update Pi_star\n",
    "                a_star = max(Q[s_t], key = Q[s_t].get)\n",
    "                Pi_star[s_t] = {a_star:1.0} \n",
    "                \n",
    "                # Update V_star\n",
    "                V_star[s_t] = Q[s_t][a_star]\n",
    "        \n",
    "        # Update N_iter\n",
    "        N_iter += 1\n",
    "\n",
    "    # END WHILE of MC loop\n",
    "    \n",
    "    # Output\n",
    "    return Pi_star, V_star, Q, Returns\n",
    "\n",
    "# END DEF MC_Ctrl_ExpStarts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e3d343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the optimal policy obtained from MC_Ctrl_ExpStarts:\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "Printing the value fn obtained from MC_Ctrl_ExpStarts:\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66|-0.66|-0.73|-0.81|\n",
      "------------------------\n",
      "Printing the value fn obtained from MC_Policy_Eval(Pi_star,...):\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "grid = test_standard_grid()\n",
    "\n",
    "# Policy to be evaluated\n",
    "Pi_opt = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_lect = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'R': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'R': 1.0},\n",
    "    (2, 3): {'U': 1.0},\n",
    "  }\n",
    "\n",
    "Pi_rand = gen_random_policy(grid)\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "#epsilon = 1e-3\n",
    "\n",
    "# Max. episode length:\n",
    "T_max = 50\n",
    "N_samples = 10000\n",
    "\n",
    "# Evaluate V_Pi:\n",
    "# SIGNATURE: Pi_star, V_star, Q, Returns = MC_Ctrl_ExpStarts(Pi, env, gamma, N_samples, T_max, all_visits_MC)\n",
    "#V = MC_Policy_Eval(Pi, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "Pi_star, V_star, Q, Returns = MC_Ctrl_ExpStarts(Pi_rand, grid, gamma, N_samples, T_max, all_visits_MC = False)\n",
    "\n",
    "print(f\"Printing the optimal policy obtained from MC_Ctrl_ExpStarts:\")\n",
    "print_policy(Pi_star, grid)\n",
    "\n",
    "print(f\"Printing the value fn obtained from MC_Ctrl_ExpStarts:\")\n",
    "print_values(V_star, grid)\n",
    "\n",
    "V_eval = MC_Policy_Eval(Pi_star, grid, gamma, 50, T_max, all_visits_MC = False)\n",
    "print(f\"Printing the value fn obtained from MC_Policy_Eval(Pi_star,...):\")\n",
    "print_values(V_eval, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78135293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecfac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ce6cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a88bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bc5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6ae56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724aa218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95028434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d9b0215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): {'L': 0.9500000000000001, 'R': 0.05},\n",
       " (1, 2): {'U': 0.03333333333333333,\n",
       "  'D': 0.9333333333333333,\n",
       "  'R': 0.03333333333333333},\n",
       " (2, 1): {'L': 0.9500000000000001, 'R': 0.05},\n",
       " (0, 0): {'D': 0.9500000000000001, 'R': 0.05},\n",
       " (2, 0): {'U': 0.9500000000000001, 'R': 0.05},\n",
       " (2, 3): {'U': 0.9500000000000001, 'L': 0.05},\n",
       " (0, 2): {'L': 0.03333333333333333,\n",
       "  'R': 0.9333333333333333,\n",
       "  'D': 0.03333333333333333},\n",
       " (2, 2): {'U': 0.03333333333333333,\n",
       "  'R': 0.03333333333333333,\n",
       "  'L': 0.9333333333333333},\n",
       " (1, 0): {'D': 0.9500000000000001, 'U': 0.05}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi_eps = gen_random_epslnsoft_policy(0.1, grid)\n",
    "Pi_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90bd0b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "myListIndices = [0,1,2,3,4,5]\n",
    "myProbs = [0.25,0.25,0.25,0.05,0.15,0.05]\n",
    "myList = ['A','B','C','D','E','F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f3af10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random index: ind_rand=np.random.choice(myList, myProbs)=1\n",
      "Random character: myList[1]=B\n"
     ]
    }
   ],
   "source": [
    "ind_rand = np.random.choice(myListIndices, p=myProbs)\n",
    "print(f\"Random index: ind_rand=np.random.choice(myList, myProbs)={ind_rand}\")\n",
    "print(f\"Random character: myList[{ind_rand}]={myList[ind_rand]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ac86a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c29ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'B': 2452, 'C': 2453, 'A': 2615, 'E': 1467, 'F': 530, 'D': 483})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = []\n",
    "for i in range(10000):\n",
    "    ind_rand = np.random.choice(myListIndices, p=myProbs)\n",
    "    samples.append(myList[ind_rand])\n",
    "\n",
    "collections.Counter(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8313ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08a5a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict = {'F':0.1, 'A':0.25, 'D':0.05, 'C':0.15, 'E':3.14 , 'B':0.35}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c54490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here: i=0\n",
      "list(myDict.keys())[0]=F\n",
      "myDict[F]=0.1\n",
      "list(myDict.values())[0]=0.1\n",
      "_________________________\n",
      "Here: i=1\n",
      "list(myDict.keys())[1]=A\n",
      "myDict[A]=0.25\n",
      "list(myDict.values())[1]=0.25\n",
      "_________________________\n",
      "Here: i=2\n",
      "list(myDict.keys())[2]=D\n",
      "myDict[D]=0.05\n",
      "list(myDict.values())[2]=0.05\n",
      "_________________________\n",
      "Here: i=3\n",
      "list(myDict.keys())[3]=C\n",
      "myDict[C]=0.15\n",
      "list(myDict.values())[3]=0.15\n",
      "_________________________\n",
      "Here: i=4\n",
      "list(myDict.keys())[4]=E\n",
      "myDict[E]=3.14\n",
      "list(myDict.values())[4]=3.14\n",
      "_________________________\n",
      "Here: i=5\n",
      "list(myDict.keys())[5]=B\n",
      "myDict[B]=0.35\n",
      "list(myDict.values())[5]=0.35\n",
      "_________________________\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(myDict.keys()))):\n",
    "    print(f\"Here: i={i}\")\n",
    "    print(f\"list(myDict.keys())[{i}]={list(myDict.keys())[i]}\")\n",
    "    print(f\"myDict[{list(myDict.keys())[i]}]={myDict[list(myDict.keys())[i]]}\")\n",
    "    print(f\"list(myDict.values())[{i}]={list(myDict.values())[i]}\")\n",
    "    print(\"_________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79bba4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F', 'A', 'D', 'C', 'E', 'B']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(myDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4997e304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 0.25, 0.05, 0.15, 3.14, 0.35]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(myDict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb70e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = test_standard_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ab28815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  L  |  L  |\n",
      "------------------------\n",
      "  D  |     |  R  |\n",
      "------------------------\n",
      "  R  |  R  |  L  |  U  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "Pi_rand = gen_random_policy(grid)\n",
    "print_policy(Pi_rand, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eec6a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid\n",
    "adm_actions = grid.adm_actions\n",
    "s_0 = (2,0)\n",
    "a_0 = np.random.choice(adm_actions[s_0])\n",
    "T_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "29fd836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards, episode_states, episode_actions, T = generate_episode_stochastic(s_0, a_0, Pi, env, T_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "415797af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (2, 0), 'R')\n",
      "(0, (2, 1), 'L')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'R')\n",
      "(0, (2, 1), 'R')\n",
      "(0, (2, 2), 'L')\n",
      "(0, (2, 1), 'L')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'U')\n",
      "(0, (0, 0), 'D')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'U')\n",
      "(0, (0, 0), 'D')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n",
      "(0, (1, 0), 'D')\n",
      "(0, (2, 0), 'U')\n"
     ]
    }
   ],
   "source": [
    "episode = list(zip(episode_rewards, episode_states, episode_actions))\n",
    "for step in episode:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6172447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): {'L': 0.9500000000000001, 'R': 0.05},\n",
       " (1, 2): {'U': 0.03333333333333333,\n",
       "  'D': 0.9333333333333333,\n",
       "  'R': 0.03333333333333333},\n",
       " (2, 1): {'L': 0.9500000000000001, 'R': 0.05},\n",
       " (0, 0): {'D': 0.9500000000000001, 'R': 0.05},\n",
       " (2, 0): {'U': 0.9500000000000001, 'R': 0.05},\n",
       " (2, 3): {'U': 0.9500000000000001, 'L': 0.05},\n",
       " (0, 2): {'L': 0.03333333333333333,\n",
       "  'R': 0.9333333333333333,\n",
       "  'D': 0.03333333333333333},\n",
       " (2, 2): {'U': 0.03333333333333333,\n",
       "  'R': 0.03333333333333333,\n",
       "  'L': 0.9333333333333333},\n",
       " (1, 0): {'D': 0.9500000000000001, 'U': 0.05}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pi_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d6f6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi = Pi_eps\n",
    "s_new = (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f6a46ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_list_s_new = list(Pi[s_new].keys())\n",
    "probs_list_s_new = list(Pi[s_new].values())\n",
    "rand_ind = np.random.choice(len(actions_list_s_new), p = probs_list_s_new)\n",
    "# Pick action from random index\n",
    "a_new = actions_list_s_new[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72224af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions_list_s_new = ['U', 'D', 'R']\n",
      "probs_list_s_new = [0.03333333333333333, 0.9333333333333333, 0.03333333333333333]\n",
      "rand_ind = 1\n",
      "a_new = D\n"
     ]
    }
   ],
   "source": [
    "print(f\"actions_list_s_new = {actions_list_s_new}\")\n",
    "print(f\"probs_list_s_new = {probs_list_s_new}\")\n",
    "print(f\"rand_ind = {rand_ind}\")\n",
    "print(f\"a_new = {a_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "068a804d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'L': 9351, 'R': 312, 'U': 337})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_new = (2,2)\n",
    "\n",
    "samples = []\n",
    "\n",
    "for i in range(10000):\n",
    "    actions_list_s_new = list(Pi[s_new].keys())\n",
    "    probs_list_s_new = list(Pi[s_new].values())\n",
    "    rand_ind = np.random.choice(len(actions_list_s_new), p = probs_list_s_new)\n",
    "    # Pick action from random index\n",
    "    a_new = actions_list_s_new[rand_ind]\n",
    "    \n",
    "    samples.append(a_new)\n",
    "\n",
    "collections.Counter(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
