{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f43c12",
   "metadata": {},
   "source": [
    "# **Policy Iteration - Draft**\n",
    "### 2022/04/22, A. J. Zerouali\n",
    "\n",
    "This is a debug file. Objective is to implement policy iteration for an environment such as Windy GridWorld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa18e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "##### IMPORTANT: ALWAYS EXECUTE THIS CELL FIRST #####\n",
    "#####################################################\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35775e7e",
   "metadata": {},
   "source": [
    "## I - Windy GridWorld\n",
    "\n",
    "Next cell contains the main test environment class, along with a helper function for building the environment. This section also contains the printing functions and the random policy generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be71b787",
   "metadata": {},
   "source": [
    "### 1) Windy GridWorld Class and helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "669e07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### WINDY GRIDWORLD #####\n",
    "# Updated: 22/04/07, A. J. Zerouali\n",
    "# The Windy GridWorld environment used in Lazy Programmer's course.\n",
    "# This is a 3x4 grid, with wall at (1,1), +1 reward at the terminal \n",
    "# square (0,3), and -1 reward at the terminal square (1,3).\n",
    "# For the \"windy\" variant, the main changes occur in the move() method.\n",
    "# States are (i,j) tuples, actions are characters, containers are dictionaries.\n",
    "\n",
    "\n",
    "# GridWorld_simple with only 3x4 grid. This is the environment.\n",
    "class GridWorld_Windy_small():\n",
    "    def __init__(self, rows, cols, ini_state, non_term_states, term_states, actions):\n",
    "        # Attributes rows and cols are dimensions of the grid\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        # Coordinates of agent\n",
    "        self.i = ini_state[0]\n",
    "        self.j = ini_state[1]\n",
    "        # State and action spaces\n",
    "        self.non_term_states = non_term_states\n",
    "        self.term_states = term_states\n",
    "        self.actions = actions \n",
    "        # The next attributes are populated using the set() method\n",
    "        self.adm_actions = {}\n",
    "        self.rewards = {}\n",
    "        self.transition_probs = {}\n",
    "        \n",
    "    # Method setting up the actions, rewards, and transition probabilities\n",
    "    def set(self, rewards, adm_actions, transition_probs):\n",
    "        # INPUT: adm_actions: Dictionary of (i,j):[a_i] = (row,col):[action list]\n",
    "        #        rewards: Dictionary of (i,j):r = (row,col):reward\n",
    "        #        transition_probs: Dictionary of (i,j):{a_i:p_ij}= ...\n",
    "        #                          .. (row,col):{dictionary of probs for each action}\n",
    "        # WARNING: Do not confuse self.adm_actions with self.actions. Latter is the action space,\n",
    "        #          adm_actions are the accessible actions from a state (dict. {s_i:[a_ij]}).\n",
    "        self.rewards = rewards\n",
    "        self.adm_actions = adm_actions\n",
    "        self.transition_probs = transition_probs\n",
    "    \n",
    "    # Method that sets current state of agent\n",
    "    def set_state(self, s):\n",
    "        # INPUT: s: (i,j)=(row,col), coord. of agent\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    # Method to return current state of agent\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "    \n",
    "    # Method to check if current agent state is terminal\n",
    "    # Note: Lazy Prog not explciting terminal states\n",
    "    def is_terminal(self, s):\n",
    "        return (s in self.term_states)\n",
    "    \n",
    "    # HAS TO BE MODIFIED FOR WINDY GRIDWORLD\n",
    "    # Method to perform action in environment\n",
    "    def move(self, action):\n",
    "        # Input:  action: New action to execute\n",
    "        # Output: reward\n",
    "        # Comments: - Requires transition probabilities. \n",
    "        #           - Calls numpy.random.choice(), doesn't work with dictionaries.\n",
    "        \n",
    "        # Check if action is admissible in current state\n",
    "        if action in adm_actions[self.current_state()]:\n",
    "            \n",
    "            # Convert transition_probs to lists compatible with np.random.choice().\n",
    "            # Recall self.transition_probs[(self.current_state(), action)] is a dictionary,\n",
    "            # while np.random.choice() works with ints or ndarrays.\n",
    "            next_states = list(self.transition_probs[(self.current_state(), action)].keys())\n",
    "            next_states_probs = list(self.transition_probs[(self.current_state(), action)].values())\n",
    "            \n",
    "            # Generate a random index (this Numpy function is tricky)\n",
    "            rand_ind = np.random.choice(a = len(next_states), p = next_states_probs)\n",
    "            # Set new state of agent\n",
    "            s_new = next_states[rand_ind] # Not necessary, for debug\n",
    "            self.set_state(s_new)\n",
    "            \n",
    "        # END IF\n",
    "   \n",
    "        # Return reward. If not in given dictionary, return 0\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    # Method to check if agent is currently in terminal state\n",
    "    def game_over(self):\n",
    "        # Output true if agent is in terminal states (0,3) or (1,3)\n",
    "        return ( (self.i, self.j) in self.term_states)\n",
    "    \n",
    "    # Method returnning all admissible states, i.e. not in the wall (1,1)\n",
    "    def all_states(self):\n",
    "        return (self.non_term_states | self.term_states )\n",
    "# END CLASS\n",
    "\n",
    "# Helper function to construct an environment.\n",
    "# Consists mainly of initializations.\n",
    "def windy_standard_grid(penalty=0):\n",
    "    # Input: penalty: Float. Penalty for moving to non terminal state.\n",
    "    # Output: env. Windy_GridWorld_small() object (the environment).\n",
    "    \n",
    "    # Start at bottom left (randomize later)\n",
    "    ini_state = (2,0)\n",
    "    # Action space \n",
    "    ACTION_SPACE = {\"U\", \"D\", \"L\", \"R\"}\n",
    "    # Non terminal states\n",
    "    NON_TERMINAL_STATES = {(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2), (2,3)}\n",
    "    # Terminal states\n",
    "    TERMINAL_STATES = {(0,3), (1,3)}\n",
    "    \n",
    "    # Instantiate:\n",
    "    env = GridWorld_Windy_small(3, 4, ini_state, NON_TERMINAL_STATES, TERMINAL_STATES, ACTION_SPACE)\n",
    "\n",
    "    \n",
    "    # Dictionary of rewards\n",
    "    # Not storing 0s if penalty=0\n",
    "    rewards = {(0,3):1, (1,3): -1}\n",
    "    # Poplate non terminal states for penalty != 0\n",
    "    if penalty != 0:\n",
    "        for s in NON_TERMINAL_STATES:\n",
    "            rewards[s] = penalty\n",
    "    \n",
    "    # Dictionary of admissible actions per state\n",
    "    adm_actions = {\n",
    "        (0,0): (\"D\", \"R\"),\n",
    "        (0,1): (\"L\", \"R\"),\n",
    "        (0,2): (\"L\", \"R\", \"D\"),\n",
    "        (1,0): (\"D\", \"U\"),\n",
    "        (1,2): (\"U\", \"D\", \"R\"),\n",
    "        (2,0): (\"U\", \"R\"),\n",
    "        (2,1): (\"L\", \"R\"),\n",
    "        (2,2): (\"U\", \"R\", \"L\"),\n",
    "        (2,3): (\"U\", \"L\"),\n",
    "    }\n",
    "    \n",
    "    # Dictionary of transition probabilities\n",
    "    # NOTE: I've modified the instructor's implementation.\n",
    "    #       I've removed all tautologies (agent doesn't stay in current state).\n",
    "    transition_probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        \n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        \n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        \n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        \n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        \n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        \n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((1, 2), 'U'): {(0, 2): 0.5, (1, 3): 0.5},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "    }\n",
    "    \n",
    "    # Assign missing environment attributes\n",
    "    env.set(rewards, adm_actions, transition_probs)\n",
    "    \n",
    "    # Output line\n",
    "    return env\n",
    "\n",
    "# END DEF windy_standard_grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c4050",
   "metadata": {},
   "source": [
    "### 2) Printing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9283909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe policy looks like this:\\n\\npi = {\\n    (2, 0): {'U': 1.0},\\n    (1, 0): {'U': 1.0},\\n    (0, 0): {'R': 1.0},\\n    (0, 1): {'R': 1.0},\\n    (0, 2): {'R': 1.0},\\n    (1, 2): {'U': 1.0},\\n    (2, 1): {'R': 1.0},\\n    (2, 2): {'U': 1.0},\\n    (2, 3): {'L': 1.0},\\n  }\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### PRINTING FUNCTIONS #####\n",
    "# 2022/04/06, AJ Zerouali\n",
    "# Modified from Lazy Prog's GitHub\n",
    "\n",
    "def print_values(Val_fn, env):\n",
    "    print(f\"## VALUE FUNCTION ##\")\n",
    "    for i in range(env.rows):\n",
    "        print(\"------------------------\")\n",
    "        for j in range(env.cols):\n",
    "            v = Val_fn.get((i,j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\") # -ve sign takes up an extra space\n",
    "        print(\"\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "def print_policy(Pi_fn, env):\n",
    "    # REMARK: WILL ONLY PRINT A DETERMINISTIC POLICY WITH {(i,j):{\"action\":1.0}}\n",
    "    print(f\"##  POLICY  ##\")\n",
    "    for i in range(env.rows):\n",
    "        print(\"------------------------\")\n",
    "        for j in range(env.cols):\n",
    "            if (i,j) not in [(1,1), (0,3), (1,3)]:\n",
    "                # WARNING: Will only work if there's one and only one element\n",
    "                a = list(Pi_fn[(i,j)].keys())[0]\n",
    "                print(\"  %s  |\" % a, end=\"\")\n",
    "            elif (i,j) == (1,1):\n",
    "                print(\"  %s  |\" % \" \", end=\"\")\n",
    "        print(\"\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "'''\n",
    "The policy looks like this:\n",
    "\n",
    "pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d5d6a",
   "metadata": {},
   "source": [
    "### 3) Random policy generator\n",
    "\n",
    "The following generates a random deterministic policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b93fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RANDOM DETERMINISTIC POLICY GENERATOR #####\n",
    "## 2022/04/08, AJ Zerouali\n",
    "# Recall: rand_ind = np.random.choice(a = len(next_states), p = next_states_probs)\n",
    "\n",
    "def gen_random_policy(env):\n",
    "    # Input: env, Windy_GridWorld_simple object (environment).\n",
    "    # Output: Pi, a (deterministic) policy dictionary.\n",
    "    non_term_states = env.non_term_states\n",
    "    adm_actions = env.adm_actions\n",
    "    Pi = {}\n",
    "    \n",
    "    for s in non_term_states:\n",
    "        actions_list = list(adm_actions[s])\n",
    "        a_random = actions_list[np.random.randint(len(actions_list))]\n",
    "        Pi[s] = {a_random:1.0}\n",
    "    \n",
    "    return Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933889f",
   "metadata": {},
   "source": [
    "### 4) Value function comparison\n",
    "\n",
    "**22/04/22**\n",
    "Let's write a function that compares two value functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9baf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two value functions #\n",
    "\n",
    "def compare_value_fns(V_old, V_new, non_term_states):\n",
    "    # ARGUMENTS: - V_old and V_new: Dictionaries of 2 value functions to compare\n",
    "    #            - non_term_states: Set of non-terminal states in the environment\n",
    "    # OUTPUT: delta_V = sup_{s in S} |V_old(s)- V_new(s)|\n",
    "    delta_V = 0\n",
    "    for s in non_term_states:\n",
    "        delta_V = max(delta_V, abs(V_old[s]-V_new[s]))\n",
    "        \n",
    "    return delta_V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f74d0",
   "metadata": {},
   "source": [
    "## II - Policy Iteration \n",
    "\n",
    "This section is divided into the following parts (implemented as functions below):\n",
    "\n",
    "1) Iterative policy evaluation\n",
    "\n",
    "2) Policy improvement\n",
    "\n",
    "3) Policy iteration\n",
    "\n",
    "The \"main\" is in the next section. Each of the parts above is implemented in a distinct cell. There is an appendix below with a test case for the iterative policy eval function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163434f1",
   "metadata": {},
   "source": [
    "### 1) Iterative policy evaluation\n",
    "\n",
    "A more general function for stochastic policies and non-trivial transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69a2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ITERATIVE POLICY EVALUATION #####\n",
    "## 2022/04/08, AJ Zerouali\n",
    "## REMARK: This function takes into account s\n",
    "\n",
    "def iter_policy_eval(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma):\n",
    "    # ARGUMENTS:\n",
    "    #  Pi: Dict. Policy function to be evaluated, from main() function.\n",
    "    #  V_ini: Dict. Initial value fn, from main() function.\n",
    "    #  P_trans: Dict. Transition probabilities of MDP, from main() function.\n",
    "    #  Rwds: Dict. Rewards by (state, action, state_new), from main() function.\n",
    "    #  adm_actions: Dict. Admissible actions in a given state, from grid attributes.\n",
    "    #  non_term_states: Set. Non terminal states, from grid attributes.\n",
    "    #  term_states: Set. Terminal states only, from grid attributes.\n",
    "    #  epsilon: Float. Convergence threshold (for sup norm of value function), from main() function.\n",
    "    #  gamma: Float. Discount factor, from main() function.\n",
    "    \n",
    "    # OUTPUT:\n",
    "    #  V_pi: Dict. Value function corresp. to Pi\n",
    "    #  k: Number of iterations for convergence of policy eval.\n",
    "    \n",
    "    \n",
    "    # INITIALIZATIONS\n",
    "    # V_k and V_(k+1) ini. (get switched in while loop)\n",
    "    V_new = V_ini\n",
    "    for s in term_states:\n",
    "        V_new[s] = 0\n",
    "    V_old = {}\n",
    "    # Iteration counter ini\n",
    "    k = 0\n",
    "    # Stopping Boolean ini\n",
    "    V_is_stable = False\n",
    "    \n",
    "    \n",
    "    # MAIN LOOP\n",
    "    # Iterates over k\n",
    "    while not V_is_stable:\n",
    "        \n",
    "        # Initialize V_k and V_(k+1)\n",
    "        V_old = V_new\n",
    "        V_new = {}\n",
    "        for s in term_states:\n",
    "            V_new[s] = 0\n",
    "        # Initialize sup|V_(k+1) - V_k|\n",
    "        Delta_V = 0\n",
    "        \n",
    "        # EVALUATE V_(k+1)=V_new\n",
    "        # Loop over non terminal states\n",
    "        for s in non_term_states:  \n",
    "            \n",
    "            # COMPUTE V_(k+1)(s)\n",
    "            \n",
    "            # Initialize\n",
    "            V_s_new = 0\n",
    "            \n",
    "            # Loop over admissible actions in state s\n",
    "            for a in adm_actions[s]:\n",
    "                \n",
    "                # Add sum over s_ind only if pi(a|s) is non-zero:\n",
    "                if (Pi[s].get(a,0) != 0):\n",
    "                \n",
    "                    # This loop is only over non-trivial transitions\n",
    "                    for s_ind in P_trans[(s,a)].keys(): \n",
    "                        # UPDATE V_s_new\n",
    "                        V_s_new += Pi[s].get(a,0)*P_trans[(s,a)].get(s_ind,0) \\\n",
    "                                    *( Rwds.get(s_ind,0) + gamma*V_old[s_ind] )  \n",
    "                    # END FOR OVER s_ind\n",
    "                    \n",
    "                # END IF\n",
    "\n",
    "            # END FOR OVER a\n",
    "            \n",
    "            # Assign V_(k+1)(s)\n",
    "            V_new[s] = V_s_new\n",
    "            \n",
    "            # Update sup|V_(k+1) - V_k|\n",
    "            Delta_V = max(Delta_V, abs(V_s_new-V_old.get(s,0)) )\n",
    "            \n",
    "        # END FOR OVER s     \n",
    "        \n",
    "        # Update stopping Boolean\n",
    "        V_is_stable = (Delta_V < epsilon)\n",
    "        \n",
    "        # Update iteration counter\n",
    "        k += 1\n",
    "    # END WHILE\n",
    "    \n",
    "    # Return V_pi and number of iterations\n",
    "    return V_new, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2aebe",
   "metadata": {},
   "source": [
    "**22/04/22 - 19:11**\n",
    "So I finally got this to work and recovered the first result (zero penalty) of Lazy Programmer. I had to change the way I broke the main while loop in policy iteration. I have to clarify where my mistake was. \n",
    "\n",
    "Secondly, I need to modify the code so as to compare two value functions and use the epsilon threshold to break the main loop. On the first successful try, I limited *N_iter* to 31. \n",
    "\n",
    "**22/04/22 - 21:01**\n",
    "The cells below reproduce the results of Lazy Programmer. I made a mistake in the update line of the policy improvement step, it was the wrong value function variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17600da",
   "metadata": {},
   "source": [
    "### 2) Policy improvement by policy iteration\n",
    "\n",
    "\n",
    "**Functions that we'll need:**\n",
    "1) Need a function that will implement policy improvement.\n",
    "\n",
    "2) Policy iteration will call iterative policy evaluation and policy improvement function in (1).\n",
    "\n",
    "3) Need a function that generates a random initial policy.\n",
    "\n",
    "\n",
    "For policy improvement function, keep the following in mind:\n",
    "* Create a dictionary for $Q_\\pi$.\n",
    "* Use the *max()* function to extract the argmax from $Q_\\pi(s,\\cdot)$ for each $s$. Syntax is as follows:\n",
    "\n",
    "                {argmax in dict} = max(dict, key = dict.get)\n",
    "* Major flaw/complication of present design: The algorithm is valid for a deterministic policy which is implemented as a stochastic policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1c8194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "## POLICY IMPROVEMENT - improve_policy() ##\n",
    "###########################################\n",
    "# 2022/04/22 - A. J. Zerouali\n",
    "# This function is called in the main loop of the policy iteration algorithm.\n",
    "\n",
    "def improve_policy(Pi, V_pi, P_trans, Rwds, adm_actions, non_term_states, term_states, gamma):\n",
    "    \n",
    "    \n",
    "    # Initialize policy_is_stable\n",
    "    policy_is_stable = True\n",
    "    \n",
    "    for s in non_term_states:\n",
    "        \n",
    "        # Store old action\n",
    "        a_old = list(Pi[s].keys())[0]\n",
    "        \n",
    "        # Initialize Vs_dict (dictionary for Q_pi(s,-))\n",
    "        Vs_dict = {}\n",
    "        \n",
    "        # Loop over admissible actions\n",
    "        for a in adm_actions[s]:\n",
    "            \n",
    "            V_temp = 0\n",
    "            \n",
    "            # Loop over non-zero probability transitions\n",
    "            # Evaluate new V_pi(s)\n",
    "            for s_ind in P_trans[(s,a)].keys(): \n",
    "                V_temp += P_trans[(s,a)].get(s_ind,0)*\\\n",
    "                            ( Rwds.get(s_ind,0) + gamma*V_pi[s_ind] )\n",
    "            # END FOR over s_ind\n",
    "            \n",
    "            # Store V_temp in Vs_dict\n",
    "            Vs_dict[a] = V_temp     \n",
    "            \n",
    "        # END FOR over a in adm_actions[s]\n",
    "        \n",
    "        # Get argmax\n",
    "        a_new = max(Vs_dict, key = Vs_dict.get)\n",
    "        \n",
    "        # Update policy with argmax:\n",
    "        Pi[s] = {a_new:1.0}\n",
    "        # Update V? Not necessary, gets evaluated again at beginning of loop\n",
    "        \n",
    "        # CLARIFY WHY THIS IS THE WAY\n",
    "        if a_old != a_new:\n",
    "            policy_is_stable = False\n",
    "        \n",
    "    # END FOR s in non_term_states\n",
    "        \n",
    "    return Pi, policy_is_stable\n",
    "###########################################\n",
    "## END OF improve_policy()               ##\n",
    "###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ef3a9",
   "metadata": {},
   "source": [
    "### 3) Policy Iteration Algorithm\n",
    "\n",
    "This function is the genesis of all the above. It implements the famous \"Policy Iteration Algorithm\" of dynamic programming, and calls both the policy evaluation and policy improvement functions.\n",
    "\n",
    "**Remarks:** \n",
    "1) The first execution was around 2:30 on 2022/04/09 and crashed.\n",
    "\n",
    "2) This function is called in the \"main\" section below. Didn't work\n",
    "\n",
    "3) 2022/04/22 - 21:25 - Finally debugged. Gives correct result with penalty 0. See test below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bec325fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## POLICY ITERATION ALGORITHM ##\n",
    "################################\n",
    "\n",
    "def Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma):\n",
    "\n",
    "    # Initialize counter and looping Boolean\n",
    "    N_iter = 0\n",
    "    policy_is_stable = True #Necessary?\n",
    "    \n",
    "    # Init. V_old\n",
    "    V_old = V_ini\n",
    "\n",
    "    # Loop until policy_is_stable = True\n",
    "    while True:\n",
    "        #######################\n",
    "        ## POLICY EVALUATION ##\n",
    "        #######################\n",
    "\n",
    "        # Execute policy eval function\n",
    "        V_new, k = iter_policy_eval(Pi, V_old, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "\n",
    "        # DEBUG:\n",
    "        print(f\"Policy evaluation fn iter_policy_eval() converged after {k} iterations.\")\n",
    "\n",
    "        ###########################################\n",
    "        ## POLICY IMPROVEMENT - improve_policy() ##\n",
    "        ###########################################\n",
    "\n",
    "        Pi, policy_is_stable = improve_policy(Pi, V_new, P_trans, Rwds, adm_actions, non_term_states, term_states, gamma)\n",
    "\n",
    "        # Break condition (Tricky)####\n",
    "        # Update policy iteration counter\n",
    "        N_iter += 1\n",
    "\n",
    "        # Compare value functions:\n",
    "        delta_V = compare_value_fns(V_old, V_new, non_term_states)\n",
    "        # Update value function\n",
    "        V_old = V_new\n",
    "\n",
    "        # BREAK WHILE condition\n",
    "        #if policy_is_stable or N_iter>30:\n",
    "        #    break\n",
    "        if policy_is_stable:\n",
    "            break\n",
    "        elif delta_V<=epsilon:\n",
    "            break\n",
    "\n",
    "    # END WHILE not policy_is_stable\n",
    "\n",
    "    # DEBUG/REMINDER: In function, should finish with\n",
    "    return V_new, Pi, N_iter\n",
    "\n",
    "# END DEF Policy_Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239b8feb",
   "metadata": {},
   "source": [
    "#### Test with zero penalty. \n",
    "\n",
    "The optimal policy is (Lecture 57):\n",
    "\n",
    "        ##  POLICY  ##\n",
    "        ------------------------\n",
    "          R  |  R  |  R  |\n",
    "        ------------------------\n",
    "          U  |     |  D  |\n",
    "        ------------------------\n",
    "          U  |  L  |  L  |  L  |\n",
    "        ------------------------\n",
    "        \n",
    "The optimal value function is:\n",
    "\n",
    "        ## VALUE FUNCTION ##\n",
    "        ------------------------\n",
    "         0.81| 0.90| 1.00| 0.00|\n",
    "        ------------------------\n",
    "         0.73| 0.00| 0.48| 0.00|\n",
    "        ------------------------\n",
    "         0.66| 0.59| 0.53| 0.48|\n",
    "        ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77d1d91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del grid, Pi_star, V_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "212d5ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windy GridWorld environment with penalty = -2 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  R  |  U  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 74 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 3 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-2.99|-1.10| 1.00| 0.00|\n",
      "------------------------\n",
      "-4.69| 0.00|-1.00| 0.00|\n",
      "------------------------\n",
      "-6.15|-4.61|-2.90|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "## INIT ENVIRONMENT ##\n",
    "######################\n",
    "\n",
    "# Penalty\n",
    "pen = -2\n",
    "# Discount factor and error threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Create environment\n",
    "grid = windy_standard_grid(penalty=pen)\n",
    "print(f\"Windy GridWorld environment with penalty = {pen} created ... \\n\")\n",
    "\n",
    "# Initialize policy\n",
    "Pi = gen_random_policy(grid)\n",
    "\n",
    "# Print optimal (deterministic) policy\n",
    "print(f\"Printing initial policy ...\")\n",
    "print_policy(Pi, grid)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Initialize value function\n",
    "V_ini = {}\n",
    "for s in (grid.non_term_states | grid.term_states):\n",
    "    V_ini[s] = 0\n",
    "\n",
    "##############################\n",
    "## EXECUTE POLICY ITERATION ##\n",
    "##############################\n",
    "\n",
    "print(f\"Executing policy iteration algorithm ...\")\n",
    "# SIGNATURE: Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "(V_star, Pi_star, N_iter) = Policy_Iteration(Pi, V_ini, grid.transition_probs, grid.rewards, grid.adm_actions, \\\n",
    "                                            grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "\n",
    "###################\n",
    "## PRINT RESULTS ##\n",
    "###################\n",
    "\n",
    "# Print N_iter\n",
    "# Print optimal value function\n",
    "print(f\"Policy_Iteration() converged after {N_iter} iterations ...\\n\")\n",
    "\n",
    "# Print optimal value function\n",
    "print(f\"Printing optimal value function ...\")\n",
    "print_values(V_star, grid)\n",
    "\n",
    "# Print optimal (deterministic) policy\n",
    "print(f\"Printing optimal policy ...\")\n",
    "print_policy(Pi_star, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1d69b",
   "metadata": {},
   "source": [
    "### Windy GridWorld with various penalties\n",
    "\n",
    "In this part I'm attempting to reproduce the results of Lecture 57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08a3956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windy GridWorld environment with penalty = 0.0 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  L  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  L  |  U  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 4 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy_Iteration() converged after 5 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.48| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.59| 0.53| 0.48|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.1 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  L  |  R  |\n",
      "------------------------\n",
      "  D  |     |  R  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 45 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 7 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.46| 0.00|-0.04| 0.00|\n",
      "------------------------\n",
      " 0.31| 0.18| 0.06|-0.04|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.2 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  L  |  L  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 52 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 8 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 7 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 7 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.43| 0.70| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.19| 0.00|-0.15| 0.00|\n",
      "------------------------\n",
      "-0.03|-0.23|-0.34|-0.50|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.4 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  D  |\n",
      "------------------------\n",
      "  D  |     |  R  |\n",
      "------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 6 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 4 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 5 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 4 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.05| 0.50| 1.00| 0.00|\n",
      "------------------------\n",
      "-0.36| 0.00|-0.25| 0.00|\n",
      "------------------------\n",
      "-0.72|-0.96|-0.62|-0.96|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -0.5 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  L  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  R  |  R  |  R  |  L  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 60 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 5 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-0.14| 0.40| 1.00| 0.00|\n",
      "------------------------\n",
      "-0.63| 0.00|-0.30| 0.00|\n",
      "------------------------\n",
      "-1.06|-1.19|-0.77|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "Windy GridWorld environment with penalty = -2 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  R  |  L  |\n",
      "------------------------\n",
      "  U  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  L  |  L  |\n",
      "------------------------\n",
      "\n",
      "\n",
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 74 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy_Iteration() converged after 5 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-2.99|-1.10| 1.00| 0.00|\n",
      "------------------------\n",
      "-4.69| 0.00|-1.00| 0.00|\n",
      "------------------------\n",
      "-6.15|-4.61|-2.90|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  R  |\n",
      "------------------------\n",
      "  R  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Windy GridWorld with various penalties\n",
    "# 2022/04/22\n",
    "### SIGNATURES:\n",
    "# windy_standard_grid(penalty=0)\n",
    "# Policy_Iteration(Pi_ini, V_ini, ---grid attributes---)\n",
    "# print_values(Val_fn, env)\n",
    "# print_policy(Val_fn, env)\n",
    "\n",
    "# Discount factor and error threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Penalty list\n",
    "penalties = [0.0, -0.1, -0.2, -0.4, -0.5, -2]\n",
    "\n",
    "# Loop over penalties\n",
    "for pen in penalties:\n",
    "    \n",
    "    # Create environment\n",
    "    grid = windy_standard_grid(penalty=pen)\n",
    "    print(f\"Windy GridWorld environment with penalty = {pen} created ... \\n\")\n",
    "\n",
    "    # Initialize policy\n",
    "    Pi = gen_random_policy(grid)\n",
    "\n",
    "    # Print optimal (deterministic) policy\n",
    "    print(f\"Printing initial policy ...\")\n",
    "    print_policy(Pi, grid)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Initialize value function\n",
    "    V_ini = {}\n",
    "    for s in (grid.non_term_states | grid.term_states):\n",
    "        V_ini[s] = 0\n",
    "\n",
    "    ##############################\n",
    "    ## EXECUTE POLICY ITERATION ##\n",
    "    ##############################\n",
    "\n",
    "    print(f\"Executing policy iteration algorithm ...\")\n",
    "    # SIGNATURE: Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "    (V_star, Pi_star, N_iter) = Policy_Iteration(Pi, V_ini, grid.transition_probs, grid.rewards, grid.adm_actions, \\\n",
    "                                                grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "\n",
    "    ###################\n",
    "    ## PRINT RESULTS ##\n",
    "    ###################\n",
    "\n",
    "    # Print N_iter\n",
    "    # Print optimal value function\n",
    "    print(f\"Policy_Iteration() converged after {N_iter} iterations ...\\n\")\n",
    "\n",
    "    # Print optimal value function\n",
    "    print(f\"Printing optimal value function ...\")\n",
    "    #print_values(V_star, grid) #\n",
    "    print_values(V_star, grid)\n",
    "\n",
    "    # Print optimal (deterministic) policy\n",
    "    print(f\"Printing optimal policy ...\")\n",
    "    #print_policy(Pi_star, grid)\n",
    "    print_policy(Pi_star, grid)\n",
    "    \n",
    "    # Separator\n",
    "    print(\"_____________________________________________\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900b96e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdd05740",
   "metadata": {},
   "source": [
    "## Policy iteration debug\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cb223",
   "metadata": {},
   "source": [
    "### Policy improvement and policy iteration - Scrap\n",
    "\n",
    "This is getting more and more complex. \n",
    "\n",
    "The next cell sets up a test case with \"wind\" in state (1,2) and with zero penalty (Lecture 57). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e142a043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windy GridWorld environment with penalty = -0.5 created ... \n",
      "\n",
      "Printing initial policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  L  |\n",
      "------------------------\n",
      "  D  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  L  |  L  |\n",
      "------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Penalty\n",
    "pen = -0.5\n",
    "# Discount factor and error threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Create environment\n",
    "grid = windy_standard_grid(penalty=pen)\n",
    "print(f\"Windy GridWorld environment with penalty = {pen} created ... \\n\")\n",
    "\n",
    "# Initialize policy\n",
    "Pi = gen_random_policy(grid)\n",
    "\n",
    "# Print optimal (deterministic) policy\n",
    "print(f\"Printing initial policy ...\")\n",
    "print_policy(Pi, grid)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Initialize value function\n",
    "V_ini = {}\n",
    "for s in (grid.non_term_states | grid.term_states):\n",
    "    V_ini[s] = 0\n",
    "\n",
    "# DEBUG: Initialize arguments of iter_policy_eval(), improve_policy() and Policy_Iteration()\n",
    "# P_trans, Rwds, adm_actions, non_term_states, term_states\n",
    "P_trans = grid.transition_probs\n",
    "Rwds = grid.rewards\n",
    "adm_actions = grid.adm_actions\n",
    "non_term_states = grid.non_term_states\n",
    "term_states = grid.term_states\n",
    "\n",
    "# Might have to suppress this later\n",
    "V_old = V_ini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0e92",
   "metadata": {},
   "source": [
    "#### Policy iteration\n",
    "\n",
    "Next comes the main loop of policy iteration. Upon convergence, we're supposed to get that the optimal policy is (Lecture 57):\n",
    "\n",
    "        ##  POLICY  ##\n",
    "        ------------------------\n",
    "          R  |  R  |  R  |\n",
    "        ------------------------\n",
    "          U  |     |  D  |\n",
    "        ------------------------\n",
    "          U  |  L  |  L  |  L  |\n",
    "        ------------------------\n",
    "        \n",
    "The optimal value function is:\n",
    "\n",
    "        ## VALUE FUNCTION ##\n",
    "        ------------------------\n",
    "         0.81| 0.90| 1.00| 0.00|\n",
    "        ------------------------\n",
    "         0.73| 0.00| 0.48| 0.00|\n",
    "        ------------------------\n",
    "         0.66| 0.59| 0.53| 0.48|\n",
    "        ------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a04df75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 60 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 4 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 3 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### POLICY ITERATION ###\n",
    "########################\n",
    "    \n",
    "print(f\"Executing policy iteration algorithm ...\")\n",
    "# SIGNATURE: Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "#(V_star, Pi_star, N_iter) = Policy_Iteration(Pi, V_ini, grid.transition_probs, grid.rewards, grid.adm_actions, \\\n",
    "#                                            grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "# REMARK: V_ini becomes V_old\n",
    "\n",
    "# Initialize counter and looping Boolean\n",
    "N_iter = 0\n",
    "# policy_is_stable = False\n",
    "\n",
    "# Loop until policy_is_stable = True\n",
    "while True:\n",
    "    #######################\n",
    "    ## POLICY EVALUATION ##\n",
    "    #######################\n",
    "    \n",
    "    # Execute policy eval function\n",
    "    V_new, k = iter_policy_eval(Pi, V_old, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "\n",
    "    # DEBUG:\n",
    "    print(f\"Policy evaluation fn iter_policy_eval() converged after {k} iterations.\")\n",
    "    \n",
    "    ###########################################\n",
    "    ## POLICY IMPROVEMENT - improve_policy() ##\n",
    "    ###########################################\n",
    "    # DEBUG/REMINDER This block normally starts with the following line\n",
    "    # def improve_policy(Pi, V_pi, P_trans, Rwds, adm_actions, non_term_states, term_states, gamma):\n",
    "    \n",
    "    # Initialize policy_is_stable\n",
    "    policy_is_stable = True\n",
    "    \n",
    "    for s in non_term_states:\n",
    "        \n",
    "        # Store old action\n",
    "        a_old = list(Pi[s].keys())[0]\n",
    "        \n",
    "        # Initialize Vs_dict (dictionary for Q_pi(s,-))\n",
    "        Vs_dict = {}\n",
    "        \n",
    "        # Loop over admissible actions\n",
    "        for a in adm_actions[s]:\n",
    "            \n",
    "            V_temp = 0\n",
    "            \n",
    "            # Loop over non-zero probability transitions\n",
    "            # Evaluate new V_pi(s)\n",
    "            for s_ind in P_trans[(s,a)].keys(): \n",
    "                V_temp += P_trans[(s,a)].get(s_ind,0)*\\\n",
    "                            ( Rwds.get(s_ind,0) + gamma*V_new[s_ind] )\n",
    "            # END FOR over s_ind\n",
    "            \n",
    "            # Store V_temp in Vs_dict\n",
    "            Vs_dict[a] = V_temp     \n",
    "            \n",
    "        # END FOR over a in adm_actions[s]\n",
    "        \n",
    "        # Get argmax\n",
    "        a_new = max(Vs_dict, key = Vs_dict.get)\n",
    "        \n",
    "        # Update policy with argmax:\n",
    "        Pi[s] = {a_new:1.0}\n",
    "        # Update V? Not necessary, gets evaluated again at beginning of loop\n",
    "        \n",
    "        # CLARIFY WHY THIS IS THE WAY\n",
    "        if a_old != a_new:\n",
    "            policy_is_stable = False\n",
    "        \n",
    "    # END FOR s in non_term_states\n",
    "        \n",
    "    # DEBUG/REMINDER: This block ends with\n",
    "    #return Pi, policy_is_stable\n",
    "    ###########################################\n",
    "    ## END OF improve_policy()               ##\n",
    "    ###########################################\n",
    "    \n",
    "    # Break condition (Tricky)####\n",
    "    # Update policy iteration counter\n",
    "    N_iter += 1\n",
    "\n",
    "    # Compare value functions:\n",
    "    delta_V = compare_value_fns(V_old, V_new, non_term_states)\n",
    "    # Update value function\n",
    "    V_old = V_new\n",
    "    \n",
    "    # BREAK WHILE condition\n",
    "    #if policy_is_stable or N_iter>30:\n",
    "    #    break\n",
    "    if policy_is_stable:\n",
    "        break\n",
    "    elif delta_V<=epsilon:\n",
    "        break\n",
    "    \n",
    "# END WHILE not policy_is_stable\n",
    "\n",
    "# DEBUG/REMINDER: In function, should finish with\n",
    "#return V_pi, Pi, N_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de16c06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb502136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy_Iteration() converged after 6 iterations ...\n",
      "\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-0.14| 0.40| 1.00| 0.00|\n",
      "------------------------\n",
      "-0.63| 0.00|-0.30| 0.00|\n",
      "------------------------\n",
      "-1.06|-1.19|-0.77|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "## PRINT RESULTS ##\n",
    "###################\n",
    "\n",
    "# Print N_iter\n",
    "# Print optimal value function\n",
    "print(f\"Policy_Iteration() converged after {N_iter} iterations ...\\n\")\n",
    "\n",
    "# Print optimal value function\n",
    "print(f\"Printing optimal value function ...\")\n",
    "#print_values(V_star, grid) #\n",
    "print_values(V_new, grid)\n",
    "\n",
    "# Print optimal (deterministic) policy\n",
    "print(f\"Printing optimal policy ...\")\n",
    "#print_policy(Pi_star, grid)\n",
    "print_policy(Pi, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf4e6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca814da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c71d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffa4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab3e71b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Pi_star[(0,1)].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68909b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6f109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361996e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6aa1725c",
   "metadata": {},
   "source": [
    "## III - Policy Iteration in Windy GridWorld\n",
    "\n",
    "In this part I'm attempting to reproduce the results of Lecture 57 of Lazy Programmer's 1st RL course. Idea is to visualize value function and deterministic policy for several penalties. First execution on 22/04/09 failed (miserably).\n",
    "\n",
    "\n",
    "**Comments:**\n",
    "1) First bug: The line \n",
    "\n",
    "            V_pi, k = iter_policy_eval(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "was initially:\n",
    "            \n",
    "            V_pi = iter_policy_eval(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "\n",
    "C++ would've never overlooked this...\n",
    "\n",
    "2) The output variables *V_star* and *Pi_star* in *Policy_Iteration()* should actually be ***V_pi*** and ***Pi***.\n",
    "\n",
    "3) First successful execution of the cell below was on 22/04/21 at 19:20. The final policy and value function obtained are completely wrong. I generate random initial policies, which could be causing some issue, so I'll start with a well defined policy. The fact that the value function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06bd7866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windy GridWorld environment with penalty = 0.0 created ...\n",
      "Executing policy iteration algorithm ...\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n",
      "------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  L  |  R  |\n",
      "------------------------\n",
      "  D  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "Windy GridWorld environment with penalty = -0.1 created ...\n",
      "Executing policy iteration algorithm ...\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-0.99|-0.99|-0.99| 0.00|\n",
      "------------------------\n",
      "-0.99| 0.00|-1.00| 0.00|\n",
      "------------------------\n",
      "-0.99|-0.99|-0.99|-0.99|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  L  |  R  |\n",
      "------------------------\n",
      "  D  |     |  D  |\n",
      "------------------------\n",
      "  U  |  L  |  R  |  L  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "Windy GridWorld environment with penalty = -0.2 created ...\n",
      "Executing policy iteration algorithm ...\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-1.99|-1.99|-1.99| 0.00|\n",
      "------------------------\n",
      "-1.99| 0.00|-1.99| 0.00|\n",
      "------------------------\n",
      "-1.99|-1.99|-1.99|-1.99|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  L  |  R  |\n",
      "------------------------\n",
      "  D  |     |  R  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "Windy GridWorld environment with penalty = -0.4 created ...\n",
      "Executing policy iteration algorithm ...\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-3.99|-3.99|-1.73| 0.00|\n",
      "------------------------\n",
      "-3.99| 0.00|-1.48| 0.00|\n",
      "------------------------\n",
      "-3.99|-3.99|-1.73|-1.96|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  R  |  R  |\n",
      "------------------------\n",
      "  D  |     |  R  |\n",
      "------------------------\n",
      "  U  |  R  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "Windy GridWorld environment with penalty = -0.5 created ...\n",
      "Executing policy iteration algorithm ...\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-0.14| 0.40| 1.00| 0.00|\n",
      "------------------------\n",
      "-4.99| 0.00|-1.76| 0.00|\n",
      "------------------------\n",
      "-4.99|-4.99|-1.40|-1.00|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  R  |  R  |  R  |\n",
      "------------------------\n",
      "  U  |     |  U  |\n",
      "------------------------\n",
      "  U  |  R  |  R  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n",
      "Windy GridWorld environment with penalty = -2 created ...\n",
      "Executing policy iteration algorithm ...\n",
      "Printing optimal value function ...\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      "-19.99|-19.99| 1.00| 0.00|\n",
      "------------------------\n",
      "-19.99| 0.00|-19.99| 0.00|\n",
      "------------------------\n",
      "-19.99|-19.99|-19.99|-19.99|\n",
      "------------------------\n",
      "Printing optimal policy ...\n",
      "##  POLICY  ##\n",
      "------------------------\n",
      "  D  |  R  |  R  |\n",
      "------------------------\n",
      "  D  |     |  R  |\n",
      "------------------------\n",
      "  U  |  L  |  U  |  U  |\n",
      "------------------------\n",
      "_____________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Windy GridWorld with various penalties\n",
    "# 2022/04/21\n",
    "### SIGNATURES:\n",
    "# windy_standard_grid(penalty=0)\n",
    "# Policy_Iteration(Pi_ini, V_ini, grid)\n",
    "# print_values(Val_fn, env)\n",
    "# print_policy(Val_fn, env)\n",
    "\n",
    "# Penalty list\n",
    "penalties = [0.0, -0.1, -0.2, -0.4, -0.5, -2]\n",
    "\n",
    "# Discount factor and error threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Loop over penalties\n",
    "for pen in penalties:\n",
    "    \n",
    "    # Create environment\n",
    "    grid = windy_standard_grid(penalty=pen)\n",
    "    print(f\"Windy GridWorld environment with penalty = {pen} created ...\")\n",
    "    \n",
    "    # Initialize policy and value function\n",
    "    Pi = gen_random_policy(grid)\n",
    "    V_ini = {}\n",
    "    for s in (grid.non_term_states | grid.term_states):\n",
    "        V_ini[s] = 0\n",
    "    \n",
    "    # Execute policy iteration\n",
    "    print(f\"Executing policy iteration algorithm ...\")\n",
    "    # SIGNATURE: Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "    (V_star, Pi_star, N_ter) = Policy_Iteration(Pi, V_ini, grid.transition_probs, grid.rewards, grid.adm_actions, \\\n",
    "                                            grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "    \n",
    "    \n",
    "    # Print optimal value function\n",
    "    print(f\"Printing optimal value function ...\")\n",
    "    print_values(V_star, grid)\n",
    "    \n",
    "    # Print optimal (deterministic) policy\n",
    "    print(f\"Printing optimal policy ...\")\n",
    "    print_policy(Pi_star, grid)\n",
    "    \n",
    "    # Separator\n",
    "    print(\"_____________________________________________\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaad7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62707f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a01f776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4bdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fc197af",
   "metadata": {},
   "source": [
    "## Appendix A: Test case for iterative policy evaluation\n",
    "\n",
    "Below is a test case with deterministic policy and non-windy GridWorld. It's a reality check from the first few lectures on DP. The test should give the following table:\n",
    "\n",
    "        Value function\n",
    "        \n",
    "        ------------------------\n",
    "         0.81| 0.90| 1.00| 0.00|\n",
    "        ------------------------\n",
    "         0.73| 0.00| 0.90| 0.00|\n",
    "        ------------------------\n",
    "         0.66| 0.73| 0.81| 0.73|\n",
    "        ------------------------\n",
    "        \n",
    "**Remark:** Execute all cells before and including *iter_policy_eval()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef77d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Windy GridWorld deterministic test converged after N_iter=6 iterations. V_pi is:\n",
      "## VALUE FUNCTION ##\n",
      "------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "##### (NON)WINDY GRIDWORLD (TEST) #####\n",
    "# Updated: 22/04/08, A. J. Zerouali\n",
    "# Test with non-windy case and deterministic policy\n",
    "\n",
    "def test_standard_grid():\n",
    "    # Start at bottom left (randomize later)\n",
    "    ini_state = (2,0)\n",
    "    # Action space \n",
    "    ACTION_SPACE = {\"U\", \"D\", \"L\", \"R\"}\n",
    "    # Non terminal states\n",
    "    NON_TERMINAL_STATES = {(0,0), (0,1), (0,2), (1,0), (1,2), (2,0), (2,1), (2,2), (2,3)}\n",
    "    # Terminal states\n",
    "    TERMINAL_STATES = {(0,3), (1,3)}\n",
    "    \n",
    "    # Instantiate:\n",
    "    # \n",
    "    env = GridWorld_Windy_small(3, 4, ini_state, NON_TERMINAL_STATES, TERMINAL_STATES, ACTION_SPACE)\n",
    "\n",
    "    \n",
    "    # Dictionary of rewards\n",
    "    # Not storing 0s\n",
    "    rewards = {(0,3):1, (1,3): -1}\n",
    "    \n",
    "    # Dictionary of admissible actions per state\n",
    "    adm_actions = {\n",
    "        (0,0): (\"D\", \"R\"),\n",
    "        (0,1): (\"L\", \"R\"),\n",
    "        (0,2): (\"L\", \"R\", \"D\"),\n",
    "        (1,0): (\"D\", \"U\"),\n",
    "        (1,2): (\"U\", \"D\", \"R\"),\n",
    "        (2,0): (\"U\", \"R\"),\n",
    "        (2,1): (\"L\", \"R\"),\n",
    "        (2,2): (\"U\", \"R\", \"L\"),\n",
    "        (2,3): (\"U\", \"L\"),\n",
    "    }\n",
    "    \n",
    "    # Dictionary of deterministic transitions:\n",
    "    transition_probs = {\n",
    "        ((2, 0), 'U'): {(1, 0): 1.0},\n",
    "        ((2, 0), 'R'): {(2, 1): 1.0},\n",
    "        \n",
    "        ((1, 0), 'U'): {(0, 0): 1.0},\n",
    "        ((1, 0), 'D'): {(2, 0): 1.0},\n",
    "        \n",
    "        ((0, 0), 'D'): {(1, 0): 1.0},\n",
    "        ((0, 0), 'R'): {(0, 1): 1.0},\n",
    "        \n",
    "        ((0, 1), 'L'): {(0, 0): 1.0},\n",
    "        ((0, 1), 'R'): {(0, 2): 1.0},\n",
    "        \n",
    "        ((0, 2), 'D'): {(1, 2): 1.0},\n",
    "        ((0, 2), 'L'): {(0, 1): 1.0},\n",
    "        ((0, 2), 'R'): {(0, 3): 1.0},\n",
    "        \n",
    "        ((2, 1), 'L'): {(2, 0): 1.0},\n",
    "        ((2, 1), 'R'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((2, 2), 'U'): {(1, 2): 1.0},\n",
    "        ((2, 2), 'L'): {(2, 1): 1.0},\n",
    "        ((2, 2), 'R'): {(2, 3): 1.0},\n",
    "        \n",
    "        ((2, 3), 'U'): {(1, 3): 1.0},\n",
    "        ((2, 3), 'L'): {(2, 2): 1.0},\n",
    "        \n",
    "        ((1, 2), 'U'): {(0, 2): 1.0},\n",
    "        ((1, 2), 'D'): {(2, 2): 1.0},\n",
    "        ((1, 2), 'R'): {(1, 3): 1.0},\n",
    "    }\n",
    "    \n",
    "    # Assign missing environment attributes\n",
    "    env.set(rewards, adm_actions, transition_probs)\n",
    "    \n",
    "    # Output line\n",
    "    return env\n",
    "\n",
    "# END DEF test_standard_grid()\n",
    "\n",
    "# Create environment\n",
    "# adm_actions, rewards and transition_probs are attributes of grid\n",
    "\n",
    "grid = test_standard_grid()\n",
    "\n",
    "\n",
    "### The policy dictionary ###\n",
    "pi = {\n",
    "    (2, 0): {'U': 1.0},\n",
    "    (1, 0): {'U': 1.0},\n",
    "    (0, 0): {'R': 1.0},\n",
    "    (0, 1): {'R': 1.0},\n",
    "    (0, 2): {'R': 1.0},\n",
    "    (1, 2): {'U': 1.0},\n",
    "    (2, 1): {'R': 1.0},\n",
    "    (2, 2): {'U': 1.0},\n",
    "    (2, 3): {'L': 1.0},\n",
    "  }\n",
    "\n",
    "### Initial value function ###\n",
    "# Just a dictionary of 0s\n",
    "V = {}\n",
    "for s in grid.all_states():\n",
    "    V[s] = 0\n",
    "\n",
    "# Discount factor and convergence threshold\n",
    "gamma = 0.9\n",
    "epsilon = 1e-3\n",
    "\n",
    "# Compute V_pi\n",
    "# Signature: iter_policy_eval(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "V_pi, N_iter = iter_policy_eval(pi, V, grid.transition_probs, grid.rewards, grid.adm_actions,\\\n",
    "                                grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "# Print the value function function obtained\n",
    "print(f\"The Windy GridWorld deterministic test converged after N_iter={N_iter} iterations. V_pi is:\")\n",
    "print_values(V_pi, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a24b4",
   "metadata": {},
   "source": [
    "## Appendix B: Some bug documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894faba",
   "metadata": {},
   "source": [
    "**22/04/21 - 19:20 **\n",
    "\n",
    "I got the following results:\n",
    "\n",
    "            Windy GridWorld environment with penalty = 0.0 created ...\n",
    "            Executing policy iteration algorithm ...\n",
    "            Printing optimal value function ...\n",
    "            ## VALUE FUNCTION ##\n",
    "            ------------------------\n",
    "             0.00| 0.00| 0.00| 0.00|\n",
    "            ------------------------\n",
    "             0.00| 0.00| 0.00| 0.00|\n",
    "            ------------------------\n",
    "             0.00| 0.00| 0.00|-1.00|\n",
    "            ------------------------\n",
    "            Printing optimal policy ...\n",
    "            ##  POLICY  ##\n",
    "            ------------------------\n",
    "              D  |  L  |  R  |\n",
    "            ------------------------\n",
    "              D  |     |  D  |\n",
    "            ------------------------\n",
    "              U  |  L  |  U  |  L  |\n",
    "            ------------------------\n",
    "            _____________________________________________\n",
    "            Windy GridWorld environment with penalty = -0.1 created ...\n",
    "            Executing policy iteration algorithm ...\n",
    "            Printing optimal value function ...\n",
    "            ## VALUE FUNCTION ##\n",
    "            ------------------------\n",
    "            -0.99|-0.99|-0.99| 0.00|\n",
    "            ------------------------\n",
    "            -0.99| 0.00|-1.00| 0.00|\n",
    "            ------------------------\n",
    "            -0.99|-0.99|-0.99|-0.99|\n",
    "            ------------------------\n",
    "            Printing optimal policy ...\n",
    "            ##  POLICY  ##\n",
    "            ------------------------\n",
    "              D  |  L  |  R  |\n",
    "            ------------------------\n",
    "              D  |     |  D  |\n",
    "            ------------------------\n",
    "              U  |  L  |  R  |  L  |\n",
    "            ------------------------\n",
    "            _____________________________________________\n",
    "            Windy GridWorld environment with penalty = -0.2 created ...\n",
    "            Executing policy iteration algorithm ...\n",
    "            Printing optimal value function ...\n",
    "            ## VALUE FUNCTION ##\n",
    "            ------------------------\n",
    "            -1.99|-1.99|-1.99| 0.00|\n",
    "            ------------------------\n",
    "            -1.99| 0.00|-1.99| 0.00|\n",
    "            ------------------------\n",
    "            -1.99|-1.99|-1.99|-1.99|\n",
    "            ------------------------\n",
    "            Printing optimal policy ...\n",
    "            ##  POLICY  ##\n",
    "            ------------------------\n",
    "              D  |  L  |  R  |\n",
    "            ------------------------\n",
    "              D  |     |  R  |\n",
    "            ------------------------\n",
    "              U  |  L  |  U  |  U  |\n",
    "            ------------------------\n",
    "            _____________________________________________\n",
    "            Windy GridWorld environment with penalty = -0.4 created ...\n",
    "            Executing policy iteration algorithm ...\n",
    "            Printing optimal value function ...\n",
    "            ## VALUE FUNCTION ##\n",
    "            ------------------------\n",
    "            -3.99|-3.99|-1.73| 0.00|\n",
    "            ------------------------\n",
    "            -3.99| 0.00|-1.48| 0.00|\n",
    "            ------------------------\n",
    "            -3.99|-3.99|-1.73|-1.96|\n",
    "            ------------------------\n",
    "            Printing optimal policy ...\n",
    "            ##  POLICY  ##\n",
    "            ------------------------\n",
    "              D  |  R  |  R  |\n",
    "            ------------------------\n",
    "              D  |     |  R  |\n",
    "            ------------------------\n",
    "              U  |  R  |  U  |  U  |\n",
    "            ------------------------\n",
    "            _____________________________________________\n",
    "            Windy GridWorld environment with penalty = -0.5 created ...\n",
    "            Executing policy iteration algorithm ...\n",
    "            Printing optimal value function ...\n",
    "            ## VALUE FUNCTION ##\n",
    "            ------------------------\n",
    "            -0.14| 0.40| 1.00| 0.00|\n",
    "            ------------------------\n",
    "            -4.99| 0.00|-1.76| 0.00|\n",
    "            ------------------------\n",
    "            -4.99|-4.99|-1.40|-1.00|\n",
    "            ------------------------\n",
    "            Printing optimal policy ...\n",
    "            ##  POLICY  ##\n",
    "            ------------------------\n",
    "              R  |  R  |  R  |\n",
    "            ------------------------\n",
    "              U  |     |  U  |\n",
    "            ------------------------\n",
    "              U  |  R  |  R  |  U  |\n",
    "            ------------------------\n",
    "            _____________________________________________\n",
    "            Windy GridWorld environment with penalty = -2 created ...\n",
    "            Executing policy iteration algorithm ...\n",
    "            Printing optimal value function ...\n",
    "            ## VALUE FUNCTION ##\n",
    "            ------------------------\n",
    "            -19.99|-19.99| 1.00| 0.00|\n",
    "            ------------------------\n",
    "            -19.99| 0.00|-19.99| 0.00|\n",
    "            ------------------------\n",
    "            -19.99|-19.99|-19.99|-19.99|\n",
    "            ------------------------\n",
    "            Printing optimal policy ...\n",
    "            ##  POLICY  ##\n",
    "            ------------------------\n",
    "              D  |  R  |  R  |\n",
    "            ------------------------\n",
    "              D  |     |  R  |\n",
    "            ------------------------\n",
    "              U  |  L  |  U  |  U  |\n",
    "            ------------------------\n",
    "            _____________________________________________\n",
    "            ​\n",
    "            ​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c17365",
   "metadata": {},
   "source": [
    "#### Backup..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3660eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing policy iteration algorithm ...\n",
      "Policy evaluation fn iter_policy_eval() converged after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### POLICY ITERATION ###\n",
    "########################\n",
    "    \n",
    "print(f\"Executing policy iteration algorithm ...\")\n",
    "# SIGNATURE: Policy_Iteration(Pi, V_ini, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "#(V_star, Pi_star, N_iter) = Policy_Iteration(Pi, V_ini, grid.transition_probs, grid.rewards, grid.adm_actions, \\\n",
    "#                                            grid.non_term_states, grid.term_states, epsilon, gamma)\n",
    "\n",
    "# REMARK: V_ini becomes V_old\n",
    "\n",
    "# Initialize counter and looping Boolean\n",
    "N_iter = 0\n",
    "# policy_is_stable = False\n",
    "\n",
    "# Loop until policy_is_stable = True\n",
    "while True:\n",
    "    #######################\n",
    "    ## POLICY EVALUATION ##\n",
    "    #######################\n",
    "    \n",
    "    V_new, k = iter_policy_eval(Pi, V_old, P_trans, Rwds, adm_actions, non_term_states, term_states, epsilon, gamma)\n",
    "    # NEW: 22/04/22\n",
    "    # Update value function\n",
    "    #V_ini = V_pi\n",
    "    # I've changed the above. Now it's below, after the compare_value_fns() call (for break condition)\n",
    "    # DEBUG:\n",
    "    print(f\"Policy evaluation fn iter_policy_eval() converged after {k} iterations.\")\n",
    "    \n",
    "    ###########################################\n",
    "    ## POLICY IMPROVEMENT - improve_policy() ##\n",
    "    ###########################################\n",
    "    # DEBUG/REMINDER This block normally starts with the following line\n",
    "    # def improve_policy(Pi, V_pi, P_trans, Rwds, adm_actions, non_term_states, term_states, gamma):\n",
    "    \n",
    "    # Initialize policy_is_stable\n",
    "    policy_is_stable = True\n",
    "    \n",
    "    for s in non_term_states:\n",
    "        \n",
    "        # Store old action\n",
    "        a_old = list(Pi[s].keys())[0]\n",
    "        \n",
    "        # Initialize Q_pi(s,-)\n",
    "        Q_pi_s = {}\n",
    "        \n",
    "        # Loop over admissible actions\n",
    "        for a in adm_actions[s]:\n",
    "            Q_sa = 0\n",
    "            \n",
    "            \n",
    "            # Loop over non-zero probability transitions\n",
    "            for s_ind in P_trans[(s,a)].keys(): \n",
    "                # UPDATE Q_sa\n",
    "                Q_sa += P_trans[(s,a)].get(s_ind,0) \\\n",
    "                        *( Rwds.get(s_ind,0) + gamma*V_pi[s_ind] )\n",
    "            # END FOR over s_ind\n",
    "            \n",
    "            # Store Q_pi(s,a)\n",
    "            Q_pi_s[a] = Q_sa\n",
    "            \n",
    "        # END FOR over a\n",
    "        \n",
    "        # Get argmax\n",
    "        ### Getting the argmax in a dictionary is done with max(dict, key = dict.get).\n",
    "        a_new = max(Q_pi_s, key = Q_pi_s.get)\n",
    "       \n",
    "        # Assign new value:\n",
    "        Pi[s] = {a_new:1.0}\n",
    "        \n",
    "        # DEBUG\n",
    "        # This condition is not good. It should not even be in this loop\n",
    "        if a_old != a_new:\n",
    "            policy_is_stable = False\n",
    "        \n",
    "        delta_V = compare_value_fns(V_old, V_new, non_term_states)\n",
    "        V_old = V_new\n",
    "\n",
    "    # END FOR s in non_term_states\n",
    "    \n",
    "    \n",
    "    # DEBUG/REMINDER: This block ends with\n",
    "    #return Pi, policy_is_stable\n",
    "    \n",
    "    ###########################################\n",
    "    ## END OF improve_policy()               ##\n",
    "    ###########################################\n",
    "    \n",
    "    \n",
    "    # Break condition (Tricky)####\n",
    "    # Update policy iteration counter\n",
    "    N_iter += 1\n",
    "    \n",
    "    # DEBUG/REMINDER: In function, should finish with\n",
    "    #return V_pi, Pi, N_iter\n",
    "    \n",
    "    # BREAK WHILE condition\n",
    "    #if policy_is_stable or N_iter>30:\n",
    "    #    break\n",
    "    \n",
    "    if policy_is_stable:\n",
    "        break\n",
    "    elif delta_V<=epsilon:\n",
    "        break\n",
    "    \n",
    "# END WHILE not policy_is_stable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
