{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a17764-0ac2-4068-b8ae-800b3e78dadf",
   "metadata": {
    "id": "f0a17764-0ac2-4068-b8ae-800b3e78dadf"
   },
   "source": [
    "# Notes on Stable Baselines 3 - v1.0\n",
    "#### 2022/09/14, AJ Zerouali\n",
    "#### Updated: 22/10/17\n",
    "\n",
    "**<span style='color:red'> To do (22/10/03) </span>**:\n",
    "\n",
    "* Order of function calls for training.\n",
    "* Order of function calls for prediction (backtesting).\n",
    "* The probability distributions submodule.\n",
    "* The replay buffer submodule.\n",
    "* Stochastic policies.\n",
    "* Deterministic policies.\n",
    "* Saving and loading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8ac3b-4936-4935-94e2-5e82a20238e7",
   "metadata": {
    "id": "ecd8ac3b-4936-4935-94e2-5e82a20238e7"
   },
   "source": [
    "### Contents:\n",
    "1) [The FinRL algorithmic trading pipeline](#1)\n",
    "    * a) [Stock trading pipeline](#1.a)\n",
    "    * b) [Portfolio optimization pipeline](#1.b)\n",
    "    * c) [Paper trading with Alpaca](#1.c)\n",
    "2) [Stable Baselines 3](#2)\n",
    "    * a) [Environment classes](#2.a)\n",
    "    * b) [Models I - Algorithm classes](#2.b)\n",
    "    * c) [Models II - Policy classes](#2.c)\n",
    "    * d) [Loggers](#2.d)\n",
    "    * e) [TensorBoard and Callbacks](#2.e)\n",
    "3) [The portfolio optimization environment](#3)\n",
    "    * a) [FinRL's updated environment](#3.a)\n",
    "    * b) [Attributes](#3.b)\n",
    "    * c) [The *step()* and *reset()* methods](#3.c)\n",
    "4) [Deep RL Agents](#4)\n",
    "    * a) [The *DRLAgent* class](#4.a)\n",
    "    * b) [Training - The *DRLAgent.train_model()* method](#4.b)\n",
    "    * c) [Backtesting - The *DRLAgent.DRL_prediction()* method](#4.c)\n",
    "    * d) [Backtest plots and statistics](#4.d)\n",
    "5) [Financial data - Acquisition and Preprocessing](#5)\n",
    "    * a) [Downloader](#5.a)\n",
    "    * b) [The *FeatureEngineer()* function](#5.b)\n",
    "    * c) [Return covariances](#5.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bec1f-b91d-4639-bb3b-ba89333f8ceb",
   "metadata": {
    "id": "919bec1f-b91d-4639-bb3b-ba89333f8ceb"
   },
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Stable Baselines 3\n",
    "\n",
    "FinRL is mainly a library of wrappers for financial applications, so when it comes to RL implementations, this section is maybe the most important part of these notes. We address the following topics.\n",
    "\n",
    "   * a) [Environment classes](#2.a)\n",
    "   * b) [Models I - Algorithm classes](#2.b)\n",
    "   * c) [Models II - Policy classes](#2.c)\n",
    "   * d) [Loggers](#2.d)\n",
    "   * e) [TensorBoard and Callbacks](#2.e)\n",
    "\n",
    "    \n",
    "The main references for the discussion below are the following pages:\n",
    "* SB3 official docs: https://stable-baselines3.readthedocs.io/en/master/index.html\n",
    "* SB3 GitHub: https://github.com/DLR-RM/stable-baselines3\n",
    "* Tutorial repo: https://github.com/araffin/rl-tutorial-jnrr19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4802c-15c4-4725-a43d-ebd4e08c30cd",
   "metadata": {
    "id": "7cd4802c-15c4-4725-a43d-ebd4e08c30cd"
   },
   "source": [
    "<a id='2.a'></a>\n",
    "### a) Environment classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf4647-2512-40a6-be28-05eb657266b6",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "source": [
    "#### The *VecEnv* and *DummyVecEnv* classes\n",
    "\n",
    "The main abstract environment class of SB3 is the vectorized environment *VecEnv*.\n",
    "* A description of vectorized envs is given here: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html;\n",
    "* The class is implemented at: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/base_vec_env.py.\n",
    "\n",
    "The idea is to allow the possibility of having multiple agents evolving in stacked environments (particularly important when dealing with Atari frames for states), and thus get vectorized actions, rewards and state spaces with various shapes. Just as with Gym, there is a plethora of wrappers that the library uses, which are briefly described at the first link above. \n",
    "\n",
    "There are two subclasses of *VecEnv* that are called in practice. From the docs: \n",
    "* ***SubprocEnv***: Creates a multiprocess vectorized wrapper for multiple environments, distributing each environment to its own process, allowing significant speed up when the environment is computationally complex.\n",
    "* ***DummyVecEnv***: Creates a simple vectorized wrapper for multiple environments, calling each environment in sequence on the current Python process. This is useful for computationally simple environment such as cartpole-v1, as the overhead of multiprocess or multithread outweighs the environment computation time. This can also be used for RL methods that require a vectorized environment, but that you want **a single environment** to train with. \n",
    "\n",
    "The latter is the one used by FinRL for SB3 environments. The implementation is here:\n",
    "* https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/dummy_vec_env.py\n",
    "\n",
    "Comments and facts on *DummyVecEnv* and *VecEnv*:\n",
    "* *VecEnv* is quite similar to the *VectorEnv* class of Gym (https://github.com/openai/gym/blob/master/gym/vector/vector_env.py), but in contrast to the latter, *VecEnv* is an **abstract base class** (https://docs.python.org/3/library/abc.html) in SB3.\n",
    "* Concerning the classes representing state and action spaces, *VecEnv* declares these as *gym.spaces.Space* objects in the constructor. \n",
    "* Here is an important note from SB3's Vectorized Envs. docs: When using vectorized environments, the environments are automatically reset at the end of each episode. Thus, the observation returned for the i-th environment when *done[i]* is true will in fact be the first observation of the next episode, **not** the last observation of the episode that has just terminated. You can access the “real” final observation of the terminated episode—that is, the one that accompanied the *done* event provided by the underlying environment—using the *terminal_observation* keys in the info dicts returned by the *VecEnv*.\n",
    "* The *step()* method itself is not implemented in *DummyVecEnv*, it is only inherited from *VecEnv*, where it is implemented in a very abstract manner using the *typing* module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6c5f4-edce-4b75-bd14-51dbb5dae4ea",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "outputs": [],
   "source": [
    "            # Typing imports\n",
    "            from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Type, Union\n",
    "            \n",
    "            # VecEnvStepReturn is what is returned by the step() method\n",
    "            # it contains the observation, reward, done, info for each env\n",
    "            VecEnvStepReturn = Tuple[VecEnvObs, np.ndarray, np.ndarray, List[Dict]]\n",
    "            \n",
    "            # Class def\n",
    "            class VecEnv(ABC):\n",
    "                ....\n",
    "                 def step(self, actions: np.ndarray) -> VecEnvStepReturn:\n",
    "                    \"\"\"\n",
    "                    Step the environments with the given action\n",
    "                    :param actions: the action\n",
    "                    :return: observation, reward, done, information\n",
    "                    \"\"\"\n",
    "                    self.step_async(actions)\n",
    "                    return self.step_wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18537382-24b3-47b4-ae30-28a6e59c881a",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "source": [
    "   Similarly, the usual *reset()*, *close()*, and *render()* are all abstract methods of *VecEnv* that wrap helper methods implemented in *DummyVecEnv*. For instance, *DummyVecEnv* class implements the *step_wait()* method called by *VecEnv*'s *step()* method. \n",
    "* In the end, a *DummyVecEnv* instance is constructed from stacked *gym.Env* objects and other environment functions, as one can see from its constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78b3e6-9dc9-4280-a52b-31d9e339cf94",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "outputs": [],
   "source": [
    "            def __init__(self, env_fns: List[Callable[[], gym.Env]]):\n",
    "                self.envs = [fn() for fn in env_fns]\n",
    "                env = self.envs[0]\n",
    "                VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n",
    "                obs_space = env.observation_space\n",
    "                self.keys, shapes, dtypes = obs_space_info(obs_space)\n",
    "                ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb43d29-df7a-4325-882e-89165a601d18",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "source": [
    "In summary, and in relation to our use in FinRL, the *DummyVecEnv* class is simply a wrapper for the portfolio optimization environment that allows us to use SB3 agents. Concretely, this means that the state/action spaces and rewards constituting our MDP are specified in the *StockPortfolioEnv* class, which subclasses the foundational *gym.Env* class. In the next part we give some reminders on the main moving parts of this class.\n",
    "\n",
    "<span style=\"color:red\">**To do (22/09/30):**</span> Add a comment on how **custom** *step()*/*reset()*/*init()* methods from a gym environment are adopted by the *VecEnv*/*DummyVecEnv* classes. This is particularly important for the discussion of FinRL's *StockPortFolioEnv* below, since:\n",
    "\n",
    "* At first glance, this is not obvious from the code or the docs. What's the mechanism? Check:\n",
    "    * https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/dummy_vec_env.py\n",
    "    * https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/base_vec_env.py\n",
    "* Unclear at the moment on what the argument \n",
    "    \n",
    "        env_fns: List[Callable[[], gym.Env]]\n",
    "  means.\n",
    "* Comment on the *DummyVecEnv.env_method(method_name, *method_args)* method. For instance, it is used for non-gym methods proper to *StockPortfolioEnv*.\n",
    "* Might be useful to give a sequence of method calls here.\n",
    "* There's more to say about environments in the *BaseAlgorithm* section. Algorithms have an *env* parameter that is wrapped by SB3 before being assigned to *BaseAlgorithm.env*. \n",
    "* In FinRL, *StockPortfolioEnv* is converted to a *DummyVecEnv* before being passed to the SB3 algorithm. This closes the loop between FinRL's *DRLAgent* class and SB3 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c11e0e-f73c-4009-af33-8c545097207e",
   "metadata": {
    "id": "dcb2eda3-7ab4-4dd7-899c-7122db66d540"
   },
   "source": [
    "#### Custom environments\n",
    "**Note (22/09/19):** See: https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2a727-fbba-47d6-ae27-5e1705bb093f",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "source": [
    "#### The *gym.Env* class\n",
    "\n",
    "To summarize, this class consists of:\n",
    "\n",
    "* ***action_space:*** This attribute encodes the actions in the environment, and is typically a subclass of *Space* discussed below.\n",
    "* ***observation_space:*** Same as above, but this attribute encodes the state space.\n",
    "* ***reset():*** This method puts resets the agent back to the initial state in the environment. It should always be called after instantiating the *Env* object to obtain the initial observation of the episode.\n",
    "* ***step():*** This method is used as follows:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa00584-2c7a-442c-8ad8-b9256fe8bf7d",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "outputs": [],
   "source": [
    "        next_state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97909674-c6ab-4261-956d-953f92a08f40",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "source": [
    "  The input is the action of the agent, and the output consists of the new state (NumPy array), the corresponding reward (float), and the boolean *done* saying whether or not *next_state* is terminal. The *info* output contains information relevant for debugging and learning.\n",
    "        \n",
    "* ***render():*** This method allows to visualize the evolution of the agent. It calls the PyGame package which in turn calls upon some lower-level C functions (one of the prerequisite packages of Gym is *SWIG*, which calls a C++ compiler). One of the args of *render()* is *mode*, which equals *'human'* by default and displays the game screen. Once done with rendering, one must call ***Env.close()*** to shutdown PyGame.\n",
    "\n",
    "The action space and the state space are represented by the abstract Gym class entitled ***Space***, whose two most relevant methods are:\n",
    "* ***sample()***, which returns a random sample from the space. This method is typically called when performing actions.\n",
    "* ***contains(x)***, which checks whether the state $x$ belongs to the space's domain.\n",
    "\n",
    "These abstract methods are reimplemented in the 3 usual subclasses of *Space*:\n",
    "* ***Discrete:*** This space class models finite spaces, with elements labelled from $0$ to $n$. The values assigned to each label are described in the environment subclass (see example below).\n",
    "\n",
    "* ***Box:*** Boxes represent $n$-dimensional tensor of rational numbers in intervals $[x_\\min, x_\\max]$, and have a *dtype* parameter in their constructor. The first use of this class is to define the bounds of a rectangular region that will be discretized in the background, and the *dtype* specifies the desired accuracy. There is also a *shape* argument in the constructor, which for example is used when the states are represented by screenshots of a game (think of the Atari environments). In the case of images of size 210x160 pixels, one calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e1199-0b21-4b63-82b9-67bb83fd79b1",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "outputs": [],
   "source": [
    "            Box(low = 0, high=255, size = (210, 160, 3)),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9014c-2a28-4ea6-a798-9016fb1540db",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "source": [
    "          \n",
    "    where 3 stands for the RGB channels.\n",
    "    \n",
    "* ***Tuple:*** Some spaces could be of various complexity, such as having discrete and continuous components. The *Tuple* class allows to define such spaces in a nested way, by combining the previous classes for instance (see example of car controls).\n",
    "\n",
    "In terms of RL algorithms, the *step()* method is the crucial one, as it implements the environment dynamics: This method is typically where the state transitions and rewards are computed. This remark will be of particular importance when returning to FinRL.\n",
    "\n",
    "**Some remarks on *gym.Env*:**\n",
    "- When calling *gym.make('Env_name')*, Gym actually calls a wrapper to create the environment, and not exactly the class itself.\n",
    "- Gym's pre-built environments do not readily give access to the possible actions in a given state or the rewards corresponding to a new state. To access these, one can use the ***env.unwrapped*** attribute, such as for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c4985-425c-4e0f-8636-21e0994f4978",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "outputs": [],
   "source": [
    "        env = gym.make('CartPole-v1')\n",
    "        # Get action space\n",
    "        cartpole_action_space = env.unwrapped.action_space\n",
    "        # Get no. of possible actions\n",
    "        cartpole_n_actions = env.unwrapped.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee41f37-09d6-4fe6-ad0c-e4537999bf98",
   "metadata": {
    "id": "1ee41f37-09d6-4fe6-ad0c-e4537999bf98"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aa150d4-c515-450d-b978-7acc5981a3a5",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "<a id='2.b'></a>\n",
    "### b) Models I - Algorithm classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e50b09c-58f0-43fb-9ed1-b6272e7d6249",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "#### b.1 - Basics\n",
    "\n",
    "The main documentation can be found at the following links: \n",
    "\n",
    "* Getting started: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html\n",
    "* Developer guide: https://stable-baselines3.readthedocs.io/en/master/guide/developer.html#\n",
    "* Algos: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
    "* Base RL class: https://stable-baselines3.readthedocs.io/en/master/modules/base.html\n",
    "* Saving and loading: https://stable-baselines3.readthedocs.io/en/master/guide/save_format.html\n",
    "* More algorithms: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "One of the most interesting features of SB3 is the stable implementation of policy gradient and actor-critic methods, along with the intuitive interface for their training and deployment.\n",
    "\n",
    "Before delving into details, let's start with the algorithms structure discussed in the developer guide above: \n",
    "* The DRL algorithms that train agents are stored in individual submodules of stable_baselines3. \n",
    "* Each of these submodules contains two files: policy.py and algo_name.py. The former implements the policy, while the latter implements the training algorithm for the agent.\n",
    "* In SB3, \"policy\" refers to all neural nets involved in the algorithm, and not only the learned policy network.\n",
    "* Each algorithm has **two main methods**. First is *collect_rollouts()* that defines how the samples are collected, and stores them in a *RolloutBuffer* (discarded after grad. update) or a *ReplayBuffer*. Second is the *train()* method that updates the parameters using samples from the buffer.\n",
    "* All the environments handled by agents are inherited from *VecEnv*.\n",
    "* At present, the algorithms that SB3 supports (on- and off-policy versions) are A2C, DDPG, DQN, HER, PPO, SAC and TD3.\n",
    "\n",
    "On top of the algorithms implemented in the main SB3 library, there is a second library called *stable_baselines3-contrib* (sb3-contrib), where there are more experimental algorithms/agents implemented. Among the presently supported algorithms are ARS (Aug. Rand. Search), TRPO, and Recurrent PPO. The GitHub repo is here:\n",
    "\n",
    "https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "At this point, it's helpful to give an example of how to use SB3 models. Say we would like to train and deploy a PPO agent in the cartpole environment. The code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c4206-01b1-40cc-9c69-efc504e2cf77",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "outputs": [],
   "source": [
    "        import gym\n",
    "        from stable_baselines3 import PPO\n",
    "        from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "        # Create 4 parallel environments\n",
    "        env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "        \n",
    "        # Instantiate agent\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "        \n",
    "        # Train\n",
    "        model.learn(total_timesteps=25000)\n",
    "        \n",
    "        # Save\n",
    "        model.save(\"ppo_cartpole\")\n",
    "\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, dones, info = env.step(action)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc1b9c3-2cad-47c1-ace7-632f01c79d10",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "Loading a trained agent is done as follows:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7f055-c8d1-446e-8eac-867850337ed8",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "outputs": [],
   "source": [
    "        from stable_baselines3 import PPO\n",
    "        model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d86316-c1c4-4261-b6d1-768ac49d1130",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "As mentioned above, any DRL algorithm consists of two parts: \n",
    "* (A) An algorithm class inherited from **BaseAlgorithm(ABC)**, which implements the sample collection (*collect_rollouts()*) and the corresponding learning algorithm (*train()* and *learn()*).\n",
    "* (B) A policy class inherited from **BasePolicy(torch.nn.Module)**, which gathers all neural nets used in the *collect_rollouts()* and *learn()* methods of the algorithm class. Any instance of a *BaseAlgorithm* subclass has a *policy* attribute that is a *BasePolicy* object.\n",
    "\n",
    "The *BasePolicy* class is discussed in Section (2.c). For the remainder of Section (2.b) we focus on the specifics of the subclasses of *BaseAlgorithm*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1b1b44-4065-4a8d-83ae-5208e324c57b",
   "metadata": {
    "id": "53c98835-09a9-49d6-8ea3-ab71db561f4e"
   },
   "source": [
    "#### b.2 - Abstract classes - *BaseAlgorithm*, *On/OffPolicyAlgorithm*\n",
    "\n",
    "The abstract base class underlying all SB3 algorithms is the *BaseAlgorithm* class in *stable_baselines3.common.base_class*. The implementation can be found here:\n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/base_class.py\n",
    "\n",
    "Stable Baselines 3 separates its DRL implementations into on-policy algorithms and off-policy ones. As such, any algorithm implemented in this library is a subclass of either *OnPolicyAlgorithm* or *OffPolicyAlgorithm*, (inherited from *BaseAlgorithm*). These are also in the submodule *stable_baselines3.common*, and their implementations are at:\n",
    "\n",
    "* On: https://github.com/DLR-RM/stable-baselines3/blob/88e1be9ff5e04b7688efa44951f845b7daf5717f/stable_baselines3/common/on_policy_algorithm.py\n",
    "* Off: https://github.com/DLR-RM/stable-baselines3/blob/88e1be9ff5e04b7688efa44951f845b7daf5717f/stable_baselines3/common/off_policy_algorithm.py\n",
    "\n",
    "We have the following class diagram for the algorithms implemented at the time of writing:\n",
    "\n",
    "<h3 align=\"center\"><img src = \"Figures/SB3_BaseAlgorithm_Class_Diagram.png\"  height= 340 width = 340></h3>\n",
    "\n",
    "For the sake of completeness and for future reference, we close this part by listing some of the interface components of SB3 on/off-policy classes.\n",
    "\n",
    "**Attributes:**\n",
    "* *policy*: *Policy* object, discussed in more detail below. Can be built at instantiation if *_init_setup_model*=True, and customized by passing the attribute *policy_kwargs* to the constructor.\n",
    "* *env*: The *gym.Env* environment to learn from. <span style=\"color:red\">**(Requires comments)**</span>\n",
    "* *learning_rate*: Learning rate for the optimizer, can be a function of the current progress remaining.\n",
    "* *gamma*: Discount factor.\n",
    "* *action_noise*: Action noise type (*None* by default) for hard exploration problems. See *common.noise* for the different action noise types.\n",
    "* *seed*: Seed for the pseudo-random generators.\n",
    "* *use_sde*: Whether to use generalized State Dependent Exploration (gSDE) instead of action noise exploration (default: False). The sample frequency can be customized with *sde_sample_freq*.\n",
    "* *device*: Specify CPU or GPU. Will try to use a Cuda compatible device by default and fallback to CPU otherwise.\n",
    "* For *OnPolicyAlgorithm*, one has attributes *ent_coef* (entropy coeff.), *vf_coef* (value f'n coeff.), and *max_grad_norm*.\n",
    "* For *OffPolicyAlgorithm*, one has attributes *batch_size*, *buffer_size*, *learning_starts*, *tau* (soft update coeff.), *train_freq*, *gradient_steps* etc.\n",
    "* *supported_action_spaces*: The action spaces supported by the algorithm.\n",
    "* In addition to the above, we also have the following attributes:\n",
    "    * For training monitoring: *monitor_wrapper*, *tensorboard_log*, and *verbose*.\n",
    "    * More environment attributes: e.g. *support_multi_env*, *create_eval_env*.\n",
    "\n",
    "The policies are handled using the *stable_baselines3.common.policies* submodule, and the replay buffer classes are in the *stable_baselines3.common.buffers* submodule.\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "* *\\_\\_init\\_\\_()*: The specific parameters of the constructor depend on the algorithm type (on/off-policy).\n",
    "* ***collect_rollouts()***: Collects experiences using the current policy and fills a *RolloutBuffer*. \"Rollout\" here refers to the model-free RL notion and **should not be confused** with the rollout concept of model-based RL or planning. This method is implemented in the *On/OffPolicyAlgorithm* classes (not in the Base), and calls the *step()* method of the environment, .\n",
    "* ***train()***: This is an abstract method of *On/OffPolicyAlgorithm*, and is **implemented for each deep RL algorithm individually**. In particular, this is the method containing the usual PyTorch training loop with *loss.backward()* and *optimizer.step()* (for a concrete example, see implementation at https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/dqn/dqn.py).\n",
    "* ***learn()***: Returns a trained model, included in the Base class and implemented differently in *On/OffPolicyAlgorithm*. The purpose of this method is to manage the *logger* and *callback* objects during training. It first calls *_setup_learn* to initialize training parameters, and next loops over time steps to collect rollouts using the method above. Once all the callback/logger instructions are done, the method *train* is called to update network weights.\n",
    "* ***predict()***: This is a method of *BaseAlgorithm* that wraps the *predict()* method of the *policy* attribute (see below).\n",
    "* *get_parameters()*: Returns the parameters of the agent. This includes parameters from different networks, e.g. critics (value functions) and policies (pi functions).\n",
    "* The remaining methods manage several other aspects of the interface. For instance:\n",
    "    * More deep learning functions: *_setup_model*, *_setup_lr_schedule*, *_update_current_progress_remaining*, *_update_learning_rate*, *_get_torch_save_params*, *_setup_learn*, *set_random_seed*,  (loads files), \n",
    "    * Saving and loading: *set_parameters*, *load*, *save*\n",
    "    * Environment methods: *_wrap_env*, *_get_eval_env*, *get_env*, *set_env*, *get_vec_normalize_env*.\n",
    "    * Memory buffer methods: *_update_info_buffer*, \n",
    "    * Callback methods: *_init_callback*, \n",
    "    * Logger methods: *set_logger*, *logger*, *_dump_logs*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952d8a2-1943-4123-accf-79d9fac3ad44",
   "metadata": {},
   "source": [
    "#### b.3 - The *learn()* and *train()* methods\n",
    "\n",
    "Regarding these two methods, here are the points to keep in mind:\n",
    "\n",
    "* The first notable difference between the *On/OffPolicyAlgorithm* classes is their *learn()* methods. Both call the methods *_setup_learn()*, *callback.on_training_start()*, *callback.on_training_end()*, *collect_rollouts()*, and both contain the main training loop where the *train()* method is called. The main difference is that *OnPolicyAlgorithm.learn()* records the training process in the *logger* attribute.\n",
    "\n",
    "* The *train()* method is where the deep reinforcement learning algorithms are effectively implemented. Indeed, *train()* is an abstract method of *On/OffPolicyAlgorithm.learn()*, and it is only implemented in their subclasses (A2C, PPO, TD3, DDPG, SAC and DQN).\n",
    "\n",
    "* The *collect_rollouts()* method is discussed in Sec. 2.b.4 below.\n",
    "\n",
    "<span style=\"color:red\">**(Requires more comments)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b0251-c555-4f4d-b1b8-481c4f935dd7",
   "metadata": {},
   "source": [
    "#### b.4 - The *collect_rollouts()* method\n",
    "\n",
    "This method is implemented in the *On/OffPolicyAlgorithm* classes. <span style=\"color:red\">**(how different are they)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3b580c-5af0-4221-86d1-bc50181955a5",
   "metadata": {},
   "source": [
    "#### b.5 - Saving and loading models\n",
    "\n",
    "* When loading a trained model, you do not need to have an environment argument.\n",
    "\n",
    "* To train a model you need an environment attribute.\n",
    "\n",
    "<span style=\"color:red\">**(Finish this)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e902e5a-70ac-46b2-822a-3ad8302da84c",
   "metadata": {
    "id": "53c98835-09a9-49d6-8ea3-ab71db561f4e"
   },
   "source": [
    "    \n",
    "Our next topic is the handling of neural networks and policies in *stable_baselines3*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1542a-6997-4c83-90f1-620087001e55",
   "metadata": {
    "id": "226b5d7f-448d-484c-8c7b-795a6eb1318c"
   },
   "source": [
    "<a id='2.c'></a>\n",
    "### c) Models II - Policy classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b336b-598f-47c1-b00b-97168a4f4241",
   "metadata": {
    "id": "226b5d7f-448d-484c-8c7b-795a6eb1318c"
   },
   "source": [
    "\n",
    "\n",
    "The submodule of interest here is *stable_baselines3.common.policies*, which can be found at:\n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py.\n",
    "\n",
    "More information on the structure of SB3's neural nets can be found at the article on customization of policies:\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
    "\n",
    "Here we look at the classes from which the *policy* attribute of *BaseAlgorithm* inherits. As previously stated, this attribute stores and manages **all** of the neural networks involved in a given algorithm, not only the policy network. Here is the diagram class of the submodule above:\n",
    "\n",
    "<h3 align=\"center\"><img src = \"Figures/SB3_BaseModel_Class_Diagram.png\" height= 340 width = 340></h3>\n",
    "\n",
    "The next paragraphs discuss the following specifics: \n",
    "\n",
    "1) How SB3 subdivides neural nets.\n",
    "2) Probabilistic policies of actor-critic algorithms\n",
    "3) The critic network of a deterministic policy algorithm\n",
    "4) The training process of SB3 policies\n",
    "\n",
    "<span style=\"color:red\">**Note (22/09/30): This section to be re-written. Each of the points in the above list should in fact be a subsection**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae82fe0-a310-43f0-baa0-53a231275a51",
   "metadata": {
    "id": "dae82fe0-a310-43f0-baa0-53a231275a51"
   },
   "source": [
    "**(1) Neural networks** in SB3 are separated into two main parts:\n",
    "* A **features extractor** network that is usually shared between the actor and the critic (for computational efficiency). This could be a CNN for instance when dealing with images. This part of the model is controlled by the ***features_extractor***, ***features_extractor_class***, ***share_features_extractor*** and ***features_extractor_kwargs*** attributes.\n",
    "* A **fully-connected network** mapping features to the policy network (actions) or to the Q-function (values). The attribute specifying this DNN is the ***net_arch*** parameter.\n",
    "\n",
    "The feature extractor of an SB3 \"policy\" is implemented as a separate class in the *common.torch_layers* submodule:\n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/torch_layers.py.\n",
    "\n",
    "As discussed at the \"Custom Policy Network\" page, writing a custom policy could involve writing a custom feature extractor class. For a concrete example of CNNs as feature extractors, one can look at the *ActorCriticCNNPolicy* class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca7508-99ce-4ea0-8dbe-d043600adf77",
   "metadata": {
    "id": "d9ca7508-99ce-4ea0-8dbe-d043600adf77"
   },
   "source": [
    "**(2)** For algorithms with **probabilistic policies**, \n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/distributions.py\n",
    "\n",
    "**finish me...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d3e9a-02ed-47aa-b63c-cc048efc6e53",
   "metadata": {
    "id": "808d3e9a-02ed-47aa-b63c-cc048efc6e53"
   },
   "source": [
    "(3) The ContinuousCritic class\n",
    "\n",
    "**finish me...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebfd528-5112-4a10-bb48-5349512f7e98",
   "metadata": {
    "id": "1ebfd528-5112-4a10-bb48-5349512f7e98"
   },
   "source": [
    "(4) Training\n",
    "\n",
    "**finish me...**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49374b-fdf5-4675-ae08-f1869523864b",
   "metadata": {
    "id": "2d49374b-fdf5-4675-ae08-f1869523864b"
   },
   "source": [
    "**Attributes:**\n",
    "\n",
    "BaseModel/BasePolicy:\n",
    "\n",
    "* *observation_space*, *action_space*\n",
    "* *features_extractor*, *features_extractor_class*, *features_extractor_kwargs*\n",
    "* *optimizer*, *optimizer_class*, *optimizer_kwargs*\n",
    "* *normalize_images*\n",
    "\n",
    "ActorCriticAlgorithm:\n",
    "* *net_arch*, *activation_fn*, *ortho_init*\n",
    "* *dist_kwargs*, *action_dist*\n",
    "\n",
    "ContinuousCritic:\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "BaseModel/BasePolicy:\n",
    "* *_get_constructor_parameters()*\n",
    "* *_update_features_extractor()*, *make_features_extractor()*, *extract_features()*\n",
    "* *set_training_mode()* (policy, calls nn.Module.train())\n",
    "* *device()*\n",
    "* *save()*, *load()*, \n",
    "* *init_weights()*, *load_from_vector*\n",
    "* *_predict()* (abstract), *predict()*\n",
    "* \n",
    "\n",
    "ActorCriticAlgorithm:\n",
    "* *_build_mlp_extractor()*: Creates the feature extractor layers for the policy and value networks. Part of the layers can be shared.\n",
    "* *_build()*: Creates the policy network (**), the critic network (*value_net*), and the optimizer (*torch.optimizer*). Calls *_build_mlp_extractor()*.\n",
    "* *reset_noise()* gSDE, Sample new weights for the exploration matrix.\n",
    "* *forward()*: Forward pass in all the networks (actor and critic)\n",
    "* *_predict*: Get the action according to the policy for a given observation.\n",
    "* *_get_action_dist_from_latent()*: Retrieve action distribution given the latent codes.\n",
    "* *evaluate_actions()*: Evaluate actions according to the current policy, given the observations.\n",
    "* *get_distributions()*: Get the current policy distribution given the observations.\n",
    "\n",
    "ContinuousCritic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12856b-57e8-475c-acae-8eec7387d5c0",
   "metadata": {
    "id": "6d12856b-57e8-475c-acae-8eec7387d5c0"
   },
   "source": [
    "#### Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eeaa43-079f-4676-969e-f9e025c4f9c3",
   "metadata": {
    "id": "a6eeaa43-079f-4676-969e-f9e025c4f9c3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a796e5-5a8b-4c99-bb7a-1bee2784572a",
   "metadata": {
    "id": "92a796e5-5a8b-4c99-bb7a-1bee2784572a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41b03ec2-df57-4ad5-a135-d27aab1831be",
   "metadata": {
    "id": "41b03ec2-df57-4ad5-a135-d27aab1831be"
   },
   "source": [
    "<a id='2.d'></a>\n",
    "### d) Loggers\n",
    "**Note (22/09/19):** This is about the information logged during model training. In *BaseAlgorithm*, can find the methods *logger()* and *set_logger()* for instance See: https://stable-baselines3.readthedocs.io/en/master/common/logger.html#logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e987a5-1a80-4f0a-9b88-8d296e1e981e",
   "metadata": {
    "id": "a2e987a5-1a80-4f0a-9b88-8d296e1e981e"
   },
   "source": [
    "<a id='2.e'></a>\n",
    "### e) TensorBoard and Callbacks\n",
    "**Note (22/09/19):** This is mostly about Tensorboard integration. See:\n",
    "\n",
    "* https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n",
    "* https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d85fd-e2ea-40fa-be3a-1e07e32a3c98",
   "metadata": {
    "id": "3d7d85fd-e2ea-40fa-be3a-1e07e32a3c98"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c352dc9-497b-4fae-af89-ffd6802e5442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d1347-15d3-4867-9f0f-502c38095872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b42cf757-1ce9-4590-9368-29a9b4d08f6f",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe83aa-2ed0-49b2-b819-0664f5d6293c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1692c-ee73-44c0-96ab-e0fb894be69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24534c68-fb06-4923-bfd8-2bfdfd41e41d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97920c3b-fac3-404f-b2d6-f157519310dc",
   "metadata": {
    "id": "3e65235e-425b-43b1-b8fa-95c5ee2bdc9f"
   },
   "source": [
    "# ActorCriticPolicy\n",
    "##### 22/10/02\n",
    "\n",
    "Focus for now on stochastic policies. Subdivide discussion into:\n",
    "- Feature extraction layer (torch_layers)\n",
    "- Probability distributions (distributions)\n",
    "- Attributes (feature_extr-actor-critic)\n",
    "- Methods (initializations-training-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e8649-1a20-44bb-8803-22f8d5356dcb",
   "metadata": {
    "id": "9f723e7f-0d6f-41d0-90b7-887b338c75a6"
   },
   "source": [
    "## Probability distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63067ef-cfad-4eb4-a1f0-4ccb968a56dd",
   "metadata": {
    "id": "ef73be56-7d64-4948-b706-3fbf2ff42eb7"
   },
   "source": [
    "## Attributes\n",
    "\n",
    "These will be organized by neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec016714-9550-4fd0-9517-3b2d62231367",
   "metadata": {
    "id": "28a32c87-e20b-413c-af2f-cbb9e3b2cd89",
    "tags": []
   },
   "source": [
    "### Environment, training, and general attributes:\n",
    "* Env.: *observation_space*, *action_space*\n",
    "* Training params: *lr_schedule*, *ortho_init*, *use_sde*, *use_expln*, *normalize_images*.\n",
    "* Optimizer: *optimizer_class*, *optimizer_kwargs*\n",
    "\n",
    "### Feature extractor\n",
    "* *features_extractor_class*, *features_extractor_kwargs*, *features_dim*, *net_arch* and *activation_fn*: Feature extractor parameters. These are modified in the subclasses of *ActorCriticPolicy* (namely *ActorCriticCnnPolicy* and *MultiInputActorCriticCnnPolicy*). Obviously, this imp\n",
    "* *mlp_extractor*: The feature extractor for the policy. Instantiated by *_build_mlp_extractor()* (called by constructor and *_build()*).\n",
    "\n",
    "\n",
    "### Actor\n",
    "* \n",
    "* *action_dist*: Action distribution. Instantiated in the constructor using *common.distributions.make_proba_distribution*. See *common.distributions* for more details.\n",
    "* *action_net* (and *log_std_init*): Instantiated in the *_build()* method, which is called by the constructor. **Clarify what this network computes (mean? variance?)**\n",
    "\n",
    "### Critic\n",
    "\n",
    "* *value_net*: Instantiated in the *_build()* method (and hence by the constructor). Set to be a scalar output of the feature extractor, by adding an *nn.Linear* layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4912ee-0d89-49a8-ae94-e6300b8542d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d85c6f4-d5a9-4fc5-98af-338623c4a669",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "These will be organized by task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f684d-bcef-46a2-992c-82cde32b45cd",
   "metadata": {},
   "source": [
    "### Initializations\n",
    "\n",
    "* *\\_\\_init\\_\\_()*:\n",
    "* *_build_mlp_extractor()*: Create the feature extraction layers of actor and critic network.\n",
    "* *_build()*: Instantiate the actor and critic networks and learning optimizer.\n",
    "\n",
    "* *_get_constructor_parameters()*. Self-explanatory.\n",
    "\n",
    "* *reset_noise()*: Resamples noise for exploration matrix of state-dependent exploration.\n",
    "\n",
    "### Training\n",
    "\n",
    "Finding the methods used for training requires tracking function calls.\n",
    "\n",
    "* *evaluate_actions()*: Given an observation and actions, get corresponding values and log probabilities.  ***PPO.train()*** for concrete example.\n",
    "\n",
    "* *predict_values()*: Get estimated Q-value according to current policy, and given an observation. (See ***OnPolicyAlgorithm.collect_rollouts()***)\n",
    "\n",
    "### Prediction\n",
    "\n",
    "Strictly speaking, evaluation refers to *forward()* methods and the like, and are also used during training. The following methods are in the *common.policies* submodule.\n",
    "\n",
    "* *extract_features()*: Inherited from *BaseModel*. Preprocess observation if needed and call *forward()* method of *feature_extractor*.\n",
    "\n",
    "* *get_distribution()*: Get current policy distribution corresponding to observation.\n",
    "\n",
    "* *_get_action_dist_from_latent()*: Cryptic description. Has latent_pi argument. Computes the action distribution given current data? What's the meaning of latent here?\n",
    "\n",
    "* *forward()*: Forward pass in all networks: feature extractor(s), value_net, and action_net. Calls *extract_features()*, then *self.mlp_extractor.forward()*, then *self.value_net.forward()*, then actions from the actor distribution. (See ***OnPolicyAlgorithm.collect_rollouts()***)\n",
    "\n",
    "* *_predict()*: Given an observation, get action from the policy distribution. Wraps the instruction self.get_distribution(observation).get_actions(deterministic=deterministic).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3195f-e962-4304-948c-1fafa76c1da4",
   "metadata": {},
   "source": [
    "# PPO algorithm class\n",
    "##### 22/10/03\n",
    "\n",
    "Subclass of *OnPolicyAlgorithm*. Will have to comment on:\n",
    "\n",
    "* collect_rollouts() and where it is called.\n",
    "* Environment argument for training.\n",
    "* Environment call for prediction and *step()*.\n",
    "\n",
    "\n",
    "**Notes - PPO train()/learn():**\n",
    "\n",
    "*PPO* is an *OnPolicyAlgorithm* algorithm. **Crucial:** *learn()* is implemented in *OnPolicyAlgorithm*, and calls the abstract method *train()* at the very end. The latter is implemented in the *PPO* class, where *train()* calls the usual PyTorch training functions. \n",
    "\n",
    "The *PPO.learn()* method just returns the output of *OnPolicyAlgorithm.learn()*.\n",
    "\n",
    "**Notes - PPO train() and policy calls**\n",
    "* Reference: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py\n",
    "* The PPO algorithm is really implemented in the *train()* method.\n",
    "* There are two losses that are computed in the main loop.\n",
    "    \n",
    "    (i) The value loss (line 243)\n",
    "        \n",
    "        value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
    "        \n",
    "    (ii) The PPO objective function loss (255):\n",
    "    \n",
    "        loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
    "        \n",
    "* The optimization step has the following calls:\n",
    "\n",
    "        # Optimization step\n",
    "        self.policy.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Clip grad norm\n",
    "        th.nn.utils.clip_grad_norm_(self.policy.parameters(),\n",
    "                                    self.max_grad_norm)\n",
    "        self.policy.optimizer.step()\n",
    "        \n",
    "    Note here that the optimization is also done on the critic network, since its parameters are used for the computation of the loss function.\n",
    "        \n",
    "* Additional *PPO* class attributes: *target_kl*, *ent_coef*, *vf_coef*, *max_grad_norm*, *policy_kwargs*, *_init_setup_model*\n",
    "\n",
    "* *PPO.train()* calls the *policy.evaluate_actions()* method of the *ActorCriticPolicy*. Recall:\n",
    "\n",
    "        def evaluate_actions(self, obs, actions)]:\n",
    "            # Preprocess the observation if needed\n",
    "            features = self.extract_features(obs)\n",
    "            latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "            distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "            log_prob = distribution.log_prob(actions)\n",
    "            values = self.value_net(latent_vf)\n",
    "            return values, log_prob, distribution.entropy()\n",
    "    \n",
    "    This looks like the evaluation of a Q-function...\n",
    "* The above is inside a loop on sampled transitions from the rollout buffer of *PPO.rollout_buffer*.\n",
    "\n",
    "**Notes - OnPolicyAlgorithm.collect_rollouts():**\n",
    "* Ref: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/on_policy_algorithm.py.\n",
    "* This method calls *policy.forward()* (line 170) and *policy.predict_values()* (lines 205 and 214), where *policy*=*ActorCriticPolicy*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84473ddc-ca19-41f7-9bd6-351742382349",
   "metadata": {},
   "source": [
    "## Environment and step() - FinRL\n",
    "##### 22/10/03\n",
    "\n",
    "Here is the order of calls during model deployment (backtesting in my case). If *model* is an SB3 algorithm, before calling *env.step(actions)*:\n",
    "\n",
    "1) *actions* is an output of *model.predict(obs, deterministic)*.\n",
    "2) *BaseAlgorithm.predict()* wraps *BaseAlgorithm.policy.predict()*. *BaseAlgorithm.policy* = *BasePolicy*.\n",
    "3) *BasePolicy.predict()* in turn calls *BasePolicy._predict()*, which is an abstract method of *BasePolicy*.\n",
    "4) Several cases here:\n",
    "    \n",
    "    * a - Stochastic policy: *policy* = *ActorCriticPolicy*. Here: *ActorCriticPolicy._predict()* wraps *ActorCriticPolicy.get_distribution(obs).get_actions(deterministic)*.\n",
    "    \n",
    "    * b - TD3 deterministic policy: *policy* = *TD3Policy*. Here *TD3Policy._predict()* wraps *TD3Policy.actor.forward()*. (Note also that *TD3Policy.forward()* wraps *TD3Policy._predict()*.)  \n",
    "    \n",
    "    * c - SAC policy: *policy* = *SACPolicy*. Here *SACPolicy._predict()* wraps *SACPolicy.actor.forward()*. (Note also that *SACPolicy.forward()* wraps *SACPolicy._predict()*.)  \n",
    "    \n",
    "    * d - DQN case...\n",
    "    \n",
    "5) With the same order of cases above:\n",
    "    \n",
    "    * a - *ActorCriticPolicy.get_distribution(obs).get_actions(deterministic)* wraps ....\n",
    "    \n",
    "    * b - TD3 deterministic policy: *TD3Policy.actor* = *Actor*. Here *Actor.forward()* wraps *Actor.mu.forward()*. The *Actor.mu* attribute is an *nn.Sequential* object (see TD3.policies).\n",
    "    \n",
    "    * c - SAC policy...\n",
    "    \n",
    "    * d - DQN case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad31eb79-c826-43c8-b6ca-e6130f085295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c6522e-17ea-4d81-8f97-43fd9efcf378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690dca6-a447-4ab1-b8d2-659cbbfe09d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7928b2c-8719-49a9-bd70-fb1b47dd8c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ca94069-ee45-4945-992c-8e775943391e",
   "metadata": {},
   "source": [
    "# ***torch_layers* submodule** \n",
    "##### 22/10/14\n",
    "\n",
    "Source: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/torch_layers.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107007bf-01a8-4096-b6b5-f978b68d5a38",
   "metadata": {},
   "source": [
    "This one contains several classes and helper functions:\n",
    "\n",
    "### Classes:\n",
    "\n",
    "* *BaseFeaturesExtractor(nn.Module)*: Base class for feature extractors.\n",
    "\n",
    "* *FlattenExtractor(BaseFeaturesExtractor)*: Feature extractor flattening input. Used as placeholder when feature extraction not needed.\n",
    "\n",
    "* *NatureCNN(BaseFeaturesExtractor)*: The CNN from Mnih et al 2015 DQN paper.\n",
    "\n",
    "* *CombinedExtractor(BaseFeaturesExtractor)*: Used with *Dict* observation spaces. Builds a feature extractor for each key of the dictionary, so that inputs are fed through separate submodules, and then concatenated through an additional \"combined\" MLP net.\n",
    "\n",
    "* *MlpExtractor(nn.Module)*: Constructs an MLP whose inputs are outputs of a previous feature extractor (e.g. CNN) or environment observations. Outputs a latent representation for the policy and value networks. Has a *net_arch* parameter that specifies the no. and size of hidden layers, as well as the no. of shared layers between the policy and value nets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083a2028-24b2-4b5a-acf5-6cd7b18bead4",
   "metadata": {},
   "source": [
    "### Helper functions:\n",
    "\n",
    "* *create_mlp(input_dim, output_dim, net_arch, activation_fn, squash_output)*: Creates a multi layer perceptron (MLP), which is a collection of fully-connected layers each followed by an activation function. Output is a list of *nn.Module* objects.\n",
    "\n",
    "* *get_actor_critic_arch(net_arch)*: Gets the policy and value network architectures for the off-policy actor-critic algorithms (SAC, DDPG, TD3). Has a *net_arch* argument that specifies the no. and size of hidden layers. Note: Other than the feature extraction layers, no other layers can be shared by the actor and critic networks. This is to avoid evaluation issues with the target networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732d6b1-373a-4933-aec9-59c9381023ff",
   "metadata": {},
   "source": [
    "**To do (22/10/14):** Clarify when/where this submodule is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8016722b-b935-44ef-b961-dcfa6a9ff457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40269897-e848-44c4-969b-0dab51bb980b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02dc767-60a9-4756-b484-96405eb0cf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc771e8-2400-4d76-9dfe-efa04d2d87f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86d42c14-a446-4c36-9731-4dd70fa5add9",
   "metadata": {},
   "source": [
    "**Notes:** Is this *ActorCriticPolicy.forward()* method really important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba9b39-0990-4b51-a868-1d43791b6c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2255a81-dfa6-46e5-b0ca-c93df03a580d",
   "metadata": {},
   "source": [
    "# The *TD3.td3* and *TD3.policies* submodules\n",
    "##### 22/10/17\n",
    "\n",
    "The source code of this part is at the following links:\n",
    "\n",
    "* TD3 algorithm: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/td3/td3.py\n",
    "* TD3 policy: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/td3/policies.py\n",
    "* Off-policy algorithms: https://github.com/DLR-RM/stable-baselines3/blob/88e1be9ff5e04b7688efa44951f845b7daf5717f/stable_baselines3/common/off_policy_algorithm.py\n",
    "\n",
    "We note that our goal here is to give an idea of how the library is organized, and to give the readers a starting point to implement their own policies. As such, the present description of SB3 is in no way exhaustive, as there are several features that are not discussed.\n",
    "\n",
    "In Section (2.b.1), we gave a template of how an SB3 algorithm is instantiated, trained and deployed. We revisit this template below with comments on each step, where we give more details on the objects, functions and submodules involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ebbb9-9c0b-4d89-81e7-d3d329241a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "'''\n",
    "    Same as gym.make(\"env_name\"), except that we are \n",
    "    wrapping the gym.Env object by a SB3 VecEnv class.\n",
    "    Comment: 1) When instantiating SB3\n",
    "    models with an argument env of class gym.Env,\n",
    "    the latter is wrapped by an SB3 VecEnv object.\n",
    "    2) Continuing (1). In BaseAlgorithm.__init__(),\n",
    "    find the instruction:\n",
    "        env = self._wrap_env(env, self.verbose, monitor_wrapper)\n",
    "    The BaseAlgorithm._wrap_env() method looks for appropriate\n",
    "    environment and monitor wrappers and re-orders\n",
    "    image channels.\n",
    "'''\n",
    "\n",
    "# Instantiate TD3 algorithm object\n",
    "model = TD3(policy = \"MlpPolicy\", env=env, verbose=1)\n",
    "'''\n",
    "    This instruction creates a TD3 object, with arguments specifying those of\n",
    "    the corresponding TD3Policy object (TD3.policy attribute).\n",
    "    \n",
    "    I - TD3Policy Constructor arguments:\n",
    "    ------------------------------------\n",
    "    1) For BaseAlgorithm, the policy argument is either \"MlpPolicy\", \"CnnPolicy\",\n",
    "    or \"MultiInputPolicy\". These 3 aliases are always defined in the corresponding \n",
    "    algorithm submodule. In the case of TD3, MlpPolicy = TD3Policy, and the feature\n",
    "    extractor class is FlattenExtractor. \"CnnPolicy\" is a subclass of TD3Policy for \n",
    "    which the feature extractor is a CNN (NatureCNN by default), typically used for\n",
    "    image data. \"MultiInputPolicy\" is a subclass of MlpPolicy for which the \n",
    "    environment has a Dict observation space, and whose feature extractor class\n",
    "    is CombinedFeatureExtractor.\n",
    "    \n",
    "    2) The env parameter could be a gymEnv, VecEnv or DummyVecEnv object, see comments\n",
    "    above. This parameter is not needed to load a trained SB3 model.\n",
    "    \n",
    "    3) To further customize the network architecture, one specifies the following\n",
    "    parameters: net_arch, activation_fn, features_extractor_class, \n",
    "    features_extractor_kwargs, and share_features_extractor.\n",
    "    \n",
    "    4) The optimizer parameters are optimizer_class and optimizer_kwargs. By \n",
    "    default, the optimizer class is torch.optim.Adam with eps = 1e-5.\n",
    "    \n",
    "    5) The training parameters are lr_schedule, optimizer_class, optimizer_kwargs,\n",
    "    and normalize_images.\n",
    "    \n",
    "    II - TD3Policy Neural nets:\n",
    "    ---------------------------\n",
    "    1) The TD3 constructor calls TD3Policy._build() to initialize 4 neural nets, \n",
    "    each of which is an attribute of the class TD3Policy. The helper functions used \n",
    "    in _build() are make_features_extractor(), make_actor(), and make_critic(). \n",
    "    As opposed to the last 2 functions, make_features_extractor() is inherited from \n",
    "    BaseModel (see line 114 of the common.policies submodule).\n",
    "    \n",
    "    2) Actor networks: The TD3Policy.actor and TD3Policy.actor_target attributes are\n",
    "    instances of the Actor[BasePolicy] class from the TD3.policies submodule. \n",
    "    In detail, the constructor of Actor calls common.torch_layers.create_mlp() to \n",
    "    combine the feature extractor layers and the activation function, then compiles\n",
    "    this module with:\n",
    "        (Line 60) self.mu = nn.Sequential(*actor_net) \n",
    "    The function TD3Policy.actor.forward() wraps mu.forward().\n",
    "    \n",
    "    3) Critic: The TD3Policy.critic and TD3Policy.critic_target attributes are\n",
    "    instances of the ContinuousCritic[BaseModel] class from the common.policies submodule.\n",
    "    This class allows to have several critic networks, and to decide whether or not\n",
    "    to share the feature extraction layers with the actor. \n",
    "    The main attribute of ContinuousCritic is the list of q_networks, which ss for the actor,\n",
    "    are created using common.torch_layers.create_mlp() and by setting:\n",
    "        (Line 875) q_net = nn.Sequential(*q_net)\n",
    "    for each critic in (see common.policies).\n",
    "    The ContinuousCritic.forward() function returns a tuple containing the output\n",
    "    of each of these networks, while the ContinuousCritic.q1_forward() function\n",
    "    returns only the output of the first entry in the ContinuousCritic.q_networks list.\n",
    "    \n",
    "    4) Aliases: The TD3 model constructor calls a helper method called _setup_model(),\n",
    "    which initializes the TD3.policy object as a TD3Policy instance, and then calls \n",
    "    _make_aliases() that initialized the TD3.actor, TD3.actor_target, TD3.critic, and \n",
    "    TD3.critic_target attributes. These are equated to the attributes of the same name \n",
    "    of TD3.policy, and used in the TD3.train() method discussed below.\n",
    "    \n",
    "'''\n",
    "        \n",
    "# Train TD3 model\n",
    "model.learn(total_timesteps=25000)\n",
    "'''\n",
    "    1) OffPolicyAlgorithm.learn() as an abstract method calls:\n",
    "        a) OffPolicyAlgorithm.collect_rollouts()\n",
    "        b) TD3.train()\n",
    "        \n",
    "    2) For (a) above, collect_rollouts() calls the OffPolicyAlgorithm.predict()\n",
    "    method. The details on the predict() method are given at the deployment\n",
    "    step below.\n",
    "    \n",
    "    3) For (b) above, TD3.train() implements the TD3 algorithm. It uses all the\n",
    "    neural networks built by the constructor with the following\n",
    "    calls in the main loop:\n",
    "        (Line 168) next_actions = (self.actor_target(replay_data.next_observations)\n",
    "                                  + noise).clamp(-1, 1)\n",
    "        (Line 171) next_q_values = th.cat(self.critic_target(replay_data.next_observations, \n",
    "                                   next_actions), dim=1)\n",
    "        (Line 176)  current_q_values = self.critic(replay_data.observations, \n",
    "                                       replay_data.actions)\n",
    "        (Line 190) actor_loss = -self.critic.q1_forward(replay_data.observations, \n",
    "                                self.actor(replay_data.observations)).mean()\n",
    "    All of these are calls to the forward() method of the corresponding network.\n",
    "    (The line numbers refer to the TD3.td3 submodule.)\n",
    "    \n",
    "    4) The critic loss computation and its gradient step are performed in\n",
    "    lines 178 to 185 of the TD3.td3 submodule.\n",
    "    \n",
    "    5) The actor loss computation and its gradient step are performed in \n",
    "    lines 189 to 196 of the Td3.td3 submodule.\n",
    "'''\n",
    "        \n",
    "# Deploy model\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # Predict actions\n",
    "    action, _states = model.predict(obs)\n",
    "    '''\n",
    "        1) At level of abstract classes, the above is BaseAlgorithm.predict(),\n",
    "        which wraps BaseAlgorithm.policy.predict(), which in turn wraps the abstract\n",
    "        method BasePolicy._predict() of the policy class.\n",
    "        \n",
    "        2) In the case of TD3: TD3Policy._predict() wraps the method:\n",
    "            TD3Policy.actor.forward()\n",
    "        (which is the same here as TD3Policy.forward().)\n",
    "        \n",
    "        3) The above wraps the method :\n",
    "            Actor.mu.forward()\n",
    "    '''\n",
    "    # One time step\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca26588-3c53-4b6f-8426-4f03f84b9b08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e0b2af7-c2e1-4353-9e00-d379c09e5027",
   "metadata": {
    "id": "226b5d7f-448d-484c-8c7b-795a6eb1318c"
   },
   "source": [
    "<a id='2.d'></a>\n",
    "### d) Models III - Neural nets for stochastic policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c5576-ff3e-491f-a9f2-0ba1250a243d",
   "metadata": {},
   "source": [
    "<a id='2.d.1'></a>\n",
    "#### d.1 - The *ActorCriticPolicy* class\n",
    "\n",
    "In this section we discuss the second design pattern used by Stable Baselines 3 for policies. We will mostly focus on the stochastic policies of on-policy algorithms such as A2C and PPO, by looking into the details of the *ActorCriticPolicy* class in the *common.policies* submodule.\n",
    "\n",
    "In contrast to the previous section, where the actor and critic networks were represented by distinct classes, the *ActorCriticPolicy* class uses several common layers between the actor and critic networks. In more detail:\n",
    "* On top of the *features_extractor* layers, *ActorCriticPolicy* has a *mlp_extractor* attribute that computes auxilary tensors *latent_vf* and *latent_pi*. These layers are shared by the attributes *value_net*, *action_net* and *log_std*.\n",
    "* The *features_extractor* and *mlp_extractor* layers are specified by the *features_extractor_class*, *features_extractor_kwargs*,*net_arch*, and *activation_fn* parameters of the constructor.\n",
    "* The stochastic policy is encoded by the *action_dist* attribute, which is a *Distribution* object from the *common.distributions* submodule. \n",
    "* The *action_net* attribute represents the neural net computing the mean of *action_dist*, while *log_std* is the net computing the log-standard deviation of this distribution.\n",
    "\n",
    "Section 2.(d.2) gives more details on the *Distribution* class of which *action_dist* is an instance. This is particularly important here, as it solves the additional layer of complexity coming from encoding a probability distribution and appropriately evaluating random actions during training.\n",
    "\n",
    "When instantiating an *ActorCriticPolicy* object, the attributes discussed above are initalized in the following order:\n",
    "1) *ActorCriticPolicy.__init__()* first initializes *features_extractor*.\n",
    "2) *action_dist* is initalized using *common.distributions.make_proba_distribution()*, according to the class of the action space (see next subsection for more details).\n",
    "3) The constructor calls the *_build()* method that calls *_build_mlp_extractor()*, which in turn initializes *mlp_extractor* (a *MlpExtractor* object, implemented in *common.toch_layers*).\n",
    "4) Within *_build()*, the attributes *action_net* and *log_std* are initialized using *action_dist.proba_distribution_net()*, depending on the distribution class used.\n",
    "5) *_build()* initializes *value_net* as a *nn.Linear* object with one output feature.\n",
    "\n",
    "The precise outputs *value_net*, *action_net*, and *log_std* networks are specified in the *ActorCriticPolicy.forward()* method, reproduced in the next cell with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64609b69-8dc4-4206-a171-20f34a40a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, obs: th.Tensor, deterministic: bool = False) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        ### This part calls the common.torch_layers submodule\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        '''\n",
    "            This is a method inherited from BasePolicy that\n",
    "            performs the following sequence of calls:\n",
    "            BasePolicy.extract_features()->BaseFeaturesExtractor.features_extractor.forward().\n",
    "        '''\n",
    "        \n",
    "        # Compute latent policy and latent value function\n",
    "        latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        '''\n",
    "            1) Here, \"latent\" refers to the shared features\n",
    "            by the actor and critic networks, which are\n",
    "            computed by the feature extraction layers.\n",
    "            2) The line above is in fact a call to\n",
    "            MlpExtractor.forward(), which returns the outputs\n",
    "            of:\n",
    "            - MlpExtractor.policy_net.forward()\n",
    "            - MlpExtractor.value_net.forward()\n",
    "        '''\n",
    "        \n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        '''\n",
    "            This line calls ActorCriticPolicy.value_net, whic is \n",
    "            initialized in ActorCriticPolicy._build() as:\n",
    "               self.value_net = nn.Linear(self.mlp_extractor.latent_dim_vf, 1)\n",
    "        '''\n",
    "        \n",
    "        ### This part calls the common.distributions submodule\n",
    "        # Get action_distribution from latent features\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        '''\n",
    "            1) This line involves three attributes:\n",
    "            - ActorCriticPolicy.action_dist \n",
    "            - ActorCriticPolicy.action_net \n",
    "            - ActorCriticPolicy.log_std\n",
    "            and wraps the function:\n",
    "                ActorCriticPolicy.action_dist.proba_distribution(),\n",
    "            which returns the latent probability distribution.\n",
    "            2) For continuous actions distributed according to a Gaussian, \n",
    "            action_dist.proba_distribution_net() sets up the mean and std\n",
    "            via:\n",
    "                action_net =  nn.Linear(latent_dim, self.action_dim) \n",
    "                log_std = nn.Parameter(th.ones(self.action_dim) * log_std_init, \n",
    "                                       requires_grad=True)\n",
    "                (see common.distributions submodule).\n",
    "        '''\n",
    "        \n",
    "        # Compute actions from the action distribution and their log probabilities\n",
    "        actions = distribution.get_actions(deterministic=deterministic)\n",
    "        '''\n",
    "            This calls the Distribution.get_actions() method that\n",
    "            calls one of the following 2 abstract methods: \n",
    "            - sample() if deterministic = False\n",
    "            - mode() if deterministic = True.\n",
    "        '''\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        actions = actions.reshape((-1,) + self.action_space.shape)\n",
    "        \n",
    "        \n",
    "        # return output\n",
    "        return actions, values, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf4755-f96c-4e90-8d61-a8e1b9fa8813",
   "metadata": {},
   "source": [
    "One of the confusing points about *ActorCriticPolicy* are the various methods used to compute the outputs of the neural nets involved. These are as follows:\n",
    "\n",
    "* *forward()*: Forward pass in all networks: feature extractor, MLP extractor, value_net, and action_net. Used when collecting rollouts (see ***OnPolicyAlgorithm.collect_rollouts()***).\n",
    "* *evaluate_actions()*: Given an observation and actions, get corresponding values and log probabilities. Used during training (see ***PPO.train()*** for a concrete example).\n",
    "* *extract_features()*: Inherited from *BaseModel*. Preprocess observation if needed and call *forward()* method of *feature_extractor*.\n",
    "* *get_distribution()*: Get current policy distribution corresponding to observation.\n",
    "* *_get_action_dist_from_latent()*: Returns the action distribution from the latent values of features (*latent_pi* output by *mlp_extractor*)\n",
    "* *_predict()*: Given an observation, get action from the policy distribution. Wraps the instruction self.get_distribution(observation).get_actions(deterministic=deterministic), and is wrapped by *BasePolicy.predict()*. Used when deploying a trained model for instance.\n",
    "* *predict_values()*: Get estimated Q-value according to current policy, and given an observation. (See ***OnPolicyAlgorithm.collect_rollouts()***)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf64c18-92d3-4cd2-8df1-228af4e2f47e",
   "metadata": {},
   "source": [
    "<a id='2.d.2'></a>\n",
    "#### d.2 - The *common.distributions* submodule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a434a5b-9d19-4d77-8ba1-7c3b34bc450a",
   "metadata": {},
   "source": [
    "As one would expect, the *distributions* submodule is central to the implementation of stochastic policies:\n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/torch_layers.py\n",
    "\n",
    "It might be important to note that *common.distributions* imports PyTorch's distributions submodule *torch.distributions*, which is itself inspired by TensorFlow's distributions package. Some useful links:\n",
    "* Torch distributions docs: https://pytorch.org/docs/stable/distributions.html.\n",
    "* TensorFlow distributions paper: https://arxiv.org/pdf/1711.10604.pdf.\n",
    "* PyTorch abstract *Ditribution* class: https://github.com/pytorch/pytorch/blob/master/torch/distributions/normal.py.\n",
    "* PyTorch *Normal* class: https://github.com/pytorch/pytorch/blob/master/torch/distributions/normal.py.\n",
    "* PyTorch Kullback-Liebler submodule: https://github.com/pytorch/pytorch/blob/master/torch/distributions/kl.py\n",
    "\n",
    "In particular, SB3's distributions submodule imports the *Bernoulli*, *Categorical*, and *Normal* classes from PyTorch.\n",
    "\n",
    "For reference, this submodule contains the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63c3783-2bb2-42ff-8ccf-bb9305fcb04d",
   "metadata": {},
   "source": [
    "**Classes:**\n",
    "\n",
    "* *Distribution(ABC)*: Base abstract class for SB3 distributions. Has one attribute *distribution*, and declares most of the abstract **FINISH THIS**.\n",
    "* *DiagGaussianDistribution(Distribution)*: To model Gaussian distributions with diagonal covariance matrix on continuous action spaces. Superclass of *SquashedDiagGaussianDistribution*.\n",
    "* *CategoricalDistribution(Distribution)*: To model discrete distributions on discrete action spaces. Superclass of *MultiCategoricalDisrtibution*.\n",
    "* *BernoulliDistribution(Distribution)*: Bernoulli distribution for MultiBinary action spaces.\n",
    "* *StateDependentNoiseDistribution(Distribution)*: Used for state-dependent noise exploration, as used elsewhere in SB3.\n",
    "* *TanhBijector*: Bijective transformation of a probability distribution. Used in implementation of SAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefea47-6182-4754-bb8f-e886dcf13e5d",
   "metadata": {},
   "source": [
    "**Helper functions:**\n",
    "\n",
    "* *sum_independent_dims()*: Function summing log probabilities when computing the entropy of a Gaussian.\n",
    "* *make_proba_distribution(action_space, use_sde, dist_kwargs)*: Takes a *gym.spaces.Space* argument and distribution arguments to return a *Distribution* instance adapted to the action space class. Notably, if the action space class is *gym.spaces.Box*, then the output is a *DiagGaussianDistribution* object of the appropriate action space dimension. If the action space if of *gym.spaces.Discrete* class, then the output is of *CategoricalDistribution* class.\n",
    "* *kl_divergence(dist_true, dist_pred)*: Wrapper for PyTorch's *torch.distributions.kl_divergence(P,Q)*. Inputs are SB3 distributions, and output is a *torch.Tensor*. **Note:** Comment on how SB3's distributions wrap PyTorch's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a9f98c-9202-483a-bf27-4659789044a3",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Finish this. Discuss sampling and mode().**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664eb94c-f5b4-4d8e-bd08-a422f91d92cd",
   "metadata": {},
   "source": [
    "<a id='2.d.3'></a>\n",
    "#### d.3 - Example: The *PPO.ppo* and *PPO.policy* submodules\n",
    "\n",
    "<span style=\"color:red\">**Finish this...**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83012d-5bbc-4a37-a098-9aad09020c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "\n",
    "# Instantiate TD3 algorithm object\n",
    "model = PPO(policy = \"MlpPolicy\", env=env, verbose=1)\n",
    "'''\n",
    "    1) Differences between \"MlpPolicy\", \"CnnPolicy\", and \"MultiInputPolicy\" for PPO\n",
    "    \n",
    "    2) Action distribution\n",
    "    \n",
    "    3) Actor network: \n",
    "    \n",
    "    4) Critic: \n",
    "    \n",
    "    ?) Aliases?\n",
    "    \n",
    "'''\n",
    "        \n",
    "# Train PPO model\n",
    "model.learn(total_timesteps=25000)\n",
    "'''\n",
    "    1) OnPolicyAlgorithm.learn() as an abstract method calls:\n",
    "        a) OnPolicyAlgorithm.collect_rollouts()\n",
    "        b) PPO.train()\n",
    "        \n",
    "    2) For (a) above, collect_rollouts() calls \n",
    "        (Line 166) actions, values, log_probs = self.policy(obs_tensor)\n",
    "        (Line 210) values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))\n",
    "        (in common.OnPolicyAlgorithm submodule)\n",
    "    \n",
    "    3) For (b) above, PPO.train() implements the PPO algorithm. \n",
    "        (Line 208) values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "    \n",
    "    4) Computation of loss functions and gradient step.\n",
    "'''\n",
    "        \n",
    "# Deploy model\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # Predict actions\n",
    "    action, _states = model.predict(obs)\n",
    "    '''\n",
    "        This line wraps the ActorCriticAlgorithm._predict() method, which\n",
    "        returns:\n",
    "            (Line 613) ActorCriticAlgorithm.get_distribution(observation).get_actions(deterministic=deterministic)\n",
    "            (see common.policies and common.distributions)\n",
    "    '''\n",
    "    # One time step\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b50cc29-819f-4191-a130-72065a79f903",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
