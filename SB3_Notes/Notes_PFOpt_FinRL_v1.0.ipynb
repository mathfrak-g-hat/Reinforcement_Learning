{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a17764-0ac2-4068-b8ae-800b3e78dadf",
   "metadata": {
    "id": "f0a17764-0ac2-4068-b8ae-800b3e78dadf"
   },
   "source": [
    "# Notes on Portfolio Optimization with FinRL and Stable Baselines 3 - v1.0\n",
    "#### 2022/09/14, AJ Zerouali\n",
    "#### Updated: 22/10/17\n",
    "\n",
    "A set of notes for the implementation of portfolio optimization using the FinRL library. Ideally, this notebook should be viewed as a user guide for future modifications of FinRL.\n",
    "\n",
    "The contents of this document grew from an effort to clarify the following tutorial on the library's GitHub page:\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/2-Advance/FinRL_PortfolioAllocation_Explainable_DRL.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8ac3b-4936-4935-94e2-5e82a20238e7",
   "metadata": {
    "id": "ecd8ac3b-4936-4935-94e2-5e82a20238e7"
   },
   "source": [
    "### Contents:\n",
    "\n",
    "[1) The FinRL algorithmic trading pipeline](#1)\n",
    "\n",
    "* 1.a) [Stock trading pipeline](#1.a)\n",
    "* 1.b) [Portfolio optimization pipeline](#1.b)\n",
    "* 1.c) [Paper trading with Alpaca](#1.c)\n",
    "    \n",
    "[2) Stable Baselines 3](#2)\n",
    "\n",
    "* 2.a) [Environment classes](#2.a)\n",
    "* 2.b) [Models I - Algorithm classes](#2.b)\n",
    "* 2.c) [Models II - Neural nets for deterministic policies](#2.c)\n",
    "* 2.d) [Models III - Neural nets for stochastic policies](#2.d)\n",
    "* 2.e) [Loggers](#2.e)\n",
    "* 2.f) [TensorBoard and Callbacks](#2.f)\n",
    "\n",
    "[3) The portfolio optimization environment](#3)\n",
    "\n",
    "* 3.a) [FinRL's updated environment](#3.a)\n",
    "* 3.b) [Attributes](#3.b)\n",
    "* 3.c) [The *step()* and *reset()* methods](#3.c)\n",
    "\n",
    "[4) Deep RL Agents](#4)\n",
    "    \n",
    "* 4.a) [The *DRLAgent* class](#4.a)\n",
    "* 4.b) [Training - The *DRLAgent.train_model()* method](#4.b)\n",
    "* 4.c) [Backtesting - The *DRLAgent.DRL_prediction()* method](#4.c)\n",
    "* 4.d) [Backtest plots and statistics](#4.d)\n",
    "\n",
    "[5) Financial data - Acquisition and Preprocessing](#5)\n",
    "\n",
    "* 5.a) [Downloader](#5.a)\n",
    "* 5.b) [The *FeatureEngineer* class](#5.b)\n",
    "* 5.c) [Return covariances](#5.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8999541-65a9-4a15-bc8f-c5f0822e9422",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b9dcb-4c5b-4a3e-b001-65559ff7c146",
   "metadata": {
    "id": "826b9dcb-4c5b-4a3e-b001-65559ff7c146"
   },
   "source": [
    "<a id='1'></a>\n",
    "## 1 - The FinRL algorithmic trading pipeline\n",
    "\n",
    "We start with the template of FinRL trading algorithm. The idea is to summarize the main function calls in one place, to have a roadmap for the upcoming sections where each task is discussed in more detail. The references for this section are the following tutorials on GitHub:\n",
    "* Basic algo trading: https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/1-Introduction/Stock_NeurIPS2018_SB3.ipynb\n",
    "* Portfolio optimization: https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/2-Advance/FinRL_PortfolioAllocation_Explainable_DRL.ipynb\n",
    "* Paper trading with Alpaca: https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/3-Practical/FinRL_PaperTrading_Demo.ipynb\n",
    "We go into more detail in the next subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf9727-5883-45bc-9d74-e8c442ae0d39",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc"
   },
   "source": [
    "<a id='1.a'></a>\n",
    "### a) Stock trading pipeline:\n",
    "\n",
    "This is the first tutorial one consults for getting a basic understanding of FinRL:\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/1-Introduction/Stock_NeurIPS2018_SB3.ipynb.\n",
    "\n",
    "The main function calls are as follows:\n",
    "\n",
    "1) **Download data from Yahoo Finance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07357cc9-c242-489e-b03f-d1b50dd2dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TRADE_END_DATE,\n",
    "                     ticker_list = config_tickers.DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d025ad-86e0-46a3-8d88-31f5f5852c66",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "   This creates a dataframe with the list of tickers used for our portfolio, with the usual prices and volume. The *YahooDownloader()* function is imported from the *preprocessor* submodule in finrl/meta, which wraps the main functions of Yahoo Finance's API (yfinance). \n",
    "                     \n",
    "2) **Data pre-processing**\n",
    "\n",
    "Feature engineering - Clean data, compute technical indicators and other user-defined quantities:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ae3bc-970e-4053-931d-34f76697726a",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "           fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = INDICATORS,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=True,\n",
    "                    user_defined_feature = False)\n",
    "           processed = fe.preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a0fb5-fd0f-41b2-8759-9c042a8413d7",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "Training/Trading data split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46235f0e-c214-4d14-a81d-543d6516e4e2",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "            trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9bd60-f648-43c8-9452-fb786aa23de5",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "The *FeatureEngineer()* and *data_split()* functions  are imported from the *preprocessor/preprocessors* submodule in finrl/meta:\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/preprocessor/preprocessors.py.\n",
    "               \n",
    "3) **Create the Gym/SB3 environment**\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7864c80-6f25-4a59-bb72-26605f7e99b7",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            # Initialize args\n",
    "            stock_dimension = len(train.tic.unique()) # No. of stocks\n",
    "            state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension # State space dim\n",
    "            env_kwargs = { \n",
    "                \"hmax\": 100,\n",
    "                \"initial_amount\": 1000000,\n",
    "                \"num_stock_shares\": num_stock_shares,\n",
    "                \"buy_cost_pct\": buy_cost_list,\n",
    "                \"sell_cost_pct\": sell_cost_list,\n",
    "                \"state_space\": state_space,\n",
    "                \"stock_dim\": stock_dimension,\n",
    "                \"tech_indicator_list\": INDICATORS,\n",
    "                \"action_space\": stock_dimension,\n",
    "                \"reward_scaling\": 1e-4\n",
    "            }\n",
    "            \n",
    "            # Instantiate FinRL StockTradingEnv()\n",
    "            e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n",
    "            \n",
    "            # Convert to stable_baselines3 environment\n",
    "            env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffbe18c-bd1d-40be-a8d2-6316d68765ff",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "The implementation of *StockTradingEnv()* is at the following link:\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/env_stock_trading/env_stocktrading.py.\n",
    "\n",
    "This environment is in principle implemented in 3 libraries: ElegantRL, RLLib and SB3. For the sake of stability, we only look at the SB3 implementation, which is discussed in detail in section 2 of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359dc0a7-efc3-481a-8826-e420ba6c3d01",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "            \n",
    "4) **Create and train trading agent**\n",
    "\n",
    "    Taking a DDPG agent for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ee53e-dce6-4e82-adf3-8edbe6b4cbb5",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            # Instantiate agent\n",
    "            agent = DRLAgent(env = env_train) \n",
    "            \n",
    "            # Get stable_baselines3 DDPG model\n",
    "            model_ddpg = agent.get_model(\"ddpg\") \n",
    "            \n",
    "            # Train model\n",
    "            trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                                             tb_log_name='ddpg',\n",
    "                                             total_timesteps=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283489a-1464-4c58-b777-1b9b69725405",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "The *DRLAgent()* class is discussed in detail in section 3 below. The implementation is here: https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/agents/stablebaselines3/models.py. The training part is discussed in section 5 below.\n",
    "                                         \n",
    "5) **Backtest with trained agent**\n",
    "\n",
    "Create the trading environment with test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ecb60-d771-4250-9d2f-903e3b8c36b3",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,\n",
    "                                          risk_indicator_col='vix', **env_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ec5e5-11fb-47c7-bd07-bdc2b06f1813",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "Run the backtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a2065-44c4-4629-a83f-fb62f108a53d",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            df_account_value, df_actions = DRLAgent.DRL_prediction(model=trained_ddpg, \n",
    "                                                                   environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cdb327-8bdb-4905-a8a5-163d87518e63",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "The simplicity of this last instruction is one of the main attractive features of the FinRL library. The *DRL_prediction()* method is discussed in section 6 of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df429e06-3b13-40e6-a78b-50e2e155c8a0",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "6) **Plot backtest results using pyfolio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a4d39a-a704-4291-abdf-c6d488e9e323",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            backtest_plot(df_account_value, \n",
    "                          baseline_ticker = '^DJI', \n",
    "                          baseline_start = df_account_value.loc[0,'date'],\n",
    "                          baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3861a408-f973-4115-b151-4631ab95104e",
   "metadata": {
    "id": "2949f392-f13b-46ad-abf1-82c0fe4719bc",
    "tags": []
   },
   "source": [
    "This last instruction produces several performance plots along with statistics of our strategy. The implementation is at the following link: https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/plot.py. \n",
    "\n",
    "It's worth noting that this function imports the *timeseries* submodule of PyFolio. At the time of writing, this file has a bug which is fixed here: https://stackoverflow.com/questions/63554616/attributeerror-numpy-int64-object-has-no-attribute-to-pydatetime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97194db7-aaef-4673-b14e-cee4a7a76ca7",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "source": [
    "<a id='1.b'></a>\n",
    "### b) Portfolio optimization pipeline:\n",
    "\n",
    "In this subsection, our reference is the portfolio optimization tutorial here:\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/2-Advance/FinRL_PortfolioAllocation_Explainable_DRL.ipynb.\n",
    "\n",
    "Setting aside some key differences that we will discuss later, the pipeline is rather similar to the one discussed in subsection (1.a) above. \n",
    "\n",
    "Before getting into the main differences, here are the main calls:\n",
    "\n",
    "1) **Download data with yfinance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02714b4-c1f2-469b-952a-df25682da28d",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            df = YahooDownloader(start_date = '2008-01-01',\n",
    "                                 end_date = '2021-09-02',\n",
    "                                 ticker_list = config_tickers.DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bafc362-003b-4d03-a453-20b0955d3a23",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "source": [
    "2) **Feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f15c0-d443-47c3-9428-b968bf548a40",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            fe = FeatureEngineer(\n",
    "                                use_technical_indicator=True,\n",
    "                                use_turbulence=False,\n",
    "                                user_defined_feature = False)\n",
    "            df = fe.preprocess_data(df)\n",
    "\n",
    "            ### Add covariance matrix to states\n",
    "            df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "            df.index = df.date.factorize()[0]\n",
    "            cov_list = []\n",
    "            return_list = []\n",
    "            lookback=252 # look back is one year\n",
    "\n",
    "            for i in range(lookback,len(df.index.unique())):\n",
    "              data_lookback = df.loc[i-lookback:i,:]\n",
    "              price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
    "              return_lookback = price_lookback.pct_change().dropna()\n",
    "              return_list.append(return_lookback)\n",
    "              covs = return_lookback.cov().values \n",
    "              cov_list.append(covs)\n",
    "\n",
    "            df_cov = pd.DataFrame({'date':df.date.unique()lookback:],\n",
    "                                 'cov_list':cov_list,'return_list':return_list})\n",
    "            df = df.merge(df_cov, on='date')\n",
    "            df = df.sort_values(['date','tic']).reset_index(drop=True)\n",
    "\n",
    "            ### Train/test data split\n",
    "            train = data_split(df, '2009-01-01','2020-06-30')\n",
    "            trade = data_split(df,'2020-07-01', '2021-09-02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007ad4e-e042-42f0-899f-3b9d44cf9f83",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "source": [
    "3) **Create Portfolio Optimization environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09872285-99ac-4b88-b85a-7553836666da",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "outputs": [],
   "source": [
    "           \n",
    "            ### Init. env. arguments\n",
    "            stock_dimension = len(train.tic.unique())\n",
    "            state_space = stock_dimension\n",
    "            print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "            tech_indicator_list = ['macd', 'rsi_30', 'cci_30', 'dx_30']\n",
    "            feature_dimension = len(tech_indicator_list)\n",
    "            env_kwargs = {\n",
    "                \"hmax\": 100, \n",
    "                \"initial_amount\": 1000000, \n",
    "                \"transaction_cost_pct\": 0, \n",
    "                \"state_space\": state_space, \n",
    "                \"stock_dim\": stock_dimension, \n",
    "                \"tech_indicator_list\": tech_indicator_list, \n",
    "                \"action_space\": stock_dimension, \n",
    "                \"reward_scaling\": 1e-1\n",
    "\n",
    "            }\n",
    "\n",
    "            ### Instantiate portfolio alloc. environment\n",
    "            e_train_gym = StockPortfolioEnv(df = train, **env_kwargs)\n",
    "            env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19c8e8-0a85-4e6c-8ee5-d7bb6a23916c",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "source": [
    "4) **Instantiate and train model (PPO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f56feb-d2f4-4f0d-8bdf-5b729e0d9740",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            agent = DRLAgent(env = env_train)\n",
    "            PPO_PARAMS = {\"n_steps\": 2048,\"ent_coef\": 0.005,\"learning_rate\": 0.001,\"batch_size\": 128,}\n",
    "            model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "            trained_ppo = agent.train_model(model=model_ppo, \n",
    "                                            tb_log_name='ppo',\n",
    "                                            total_timesteps=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c45f41-2001-4503-9431-cb868b0be27f",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "source": [
    "5) **Backtest trained agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34815136-f0e5-4c6c-9295-11fba905eec8",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "outputs": [],
   "source": [
    "            e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)\n",
    "            df_daily_return_ppo, df_actions_ppo = DRLAgent.DRL_prediction(model=trained_ppo,\n",
    "                                                                          environment = e_trade_gym) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d2ff8-46de-4070-a3e7-8e577798bd64",
   "metadata": {
    "id": "19460a75-abef-421c-ab1f-680869d70220",
    "tags": []
   },
   "source": [
    "**Note (22/09/15):** To be continued. A functional version of the script is given in the *FinRL_PortfolioOptimization_Reduced.ipynb*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5838ee9-fa01-4b51-8f9d-ffdd2d05b832",
   "metadata": {
    "id": "c5838ee9-fa01-4b51-8f9d-ffdd2d05b832",
    "tags": []
   },
   "source": [
    "<a id='1.c'></a>\n",
    "### c) Paper trading with Alpaca:\n",
    "\n",
    "Reference: https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/3-Practical/FinRL_PaperTrading_Demo.ipynb\n",
    "\n",
    "**Note (22/09/15):** Will write-up the details later. The tutorial above has several critical issues, as it was initially written for ElegantRL which is completely broken at the time of writing. I modified that notebook to run with SB3, but the paper-trading algorithm also has critical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebb5cb-778f-4467-ba09-709a77f10d8c",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bec1f-b91d-4639-bb3b-ba89333f8ceb",
   "metadata": {
    "id": "919bec1f-b91d-4639-bb3b-ba89333f8ceb"
   },
   "source": [
    "<a id='2'></a>\n",
    "## 2 - Stable Baselines 3\n",
    "\n",
    "FinRL is mainly a library of wrappers for financial applications, and when it comes to RL implementations, this section is maybe the most important part of these notes. We address the following topics.\n",
    "\n",
    "[a) Environment classes](#2.a)\n",
    "* a.1 - [The *VecEnv* and *DummyVecEnv* classes](#2.a.1)\n",
    "* a.2 - [The *gym.Env* class](#2.a.2)\n",
    "\n",
    "[b) Models I - Algorithm classes](#2.b)\n",
    "* b.1 - [Basics](#2.b.1)\n",
    "* b.2 - [Abstract classes: *BaseAlgorithm* and *On/OffPolicyAlgorithm*](#2.b.2)\n",
    "* b.3 - [The *learn()* and *train()* methods](#2.b.3)\n",
    "* b.4 - [The *collect_rollouts* method](#2.b.4)\n",
    "* b.5 - [Saving and loading models](#2.b.5)\n",
    "\n",
    "[c) Models II - Neural nets for deterministic policies](#2.c)\n",
    "* c.1 - [Basics](#2.c.1)\n",
    "* c.2 - [The *common.torch_layers* submodule](#2.c.2)\n",
    "* c.3 - [Example: The *TD3.td3* and *TD3.policy* submodules](#2.c.3)\n",
    "\n",
    "[d) Models III - Neural nets for stochastic policies](#2.d)\n",
    "* d.1 - [The *ActorCriticPolicy* class](#2.d.1)\n",
    "* d.2 - [The *common.distributions* submodule](#2.d.2)\n",
    "* d.3 - [Example: The *PPO.ppo* and *PPO.policy* submodules](#2.d.3)\n",
    "\n",
    "[e) Loggers](#2.e)\n",
    "\n",
    "[f) TensorBoard and Callbacks](#2.f)\n",
    "\n",
    "    \n",
    "The main references for the discussion below are the following pages:\n",
    "* SB3 official docs: https://stable-baselines3.readthedocs.io/en/master/index.html\n",
    "* SB3 GitHub: https://github.com/DLR-RM/stable-baselines3\n",
    "* Tutorial repo: https://github.com/araffin/rl-tutorial-jnrr19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4802c-15c4-4725-a43d-ebd4e08c30cd",
   "metadata": {
    "id": "7cd4802c-15c4-4725-a43d-ebd4e08c30cd"
   },
   "source": [
    "<a id='2.a'></a>\n",
    "### a) Environment classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf4647-2512-40a6-be28-05eb657266b6",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "source": [
    "<a id='2.a.1'></a>\n",
    "#### a.1 - The *VecEnv* and *DummyVecEnv* classes\n",
    "\n",
    "The main abstract environment class of SB3 is the vectorized environment *VecEnv*.\n",
    "* A description of vectorized envs is given here: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html;\n",
    "* The class is implemented at: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/base_vec_env.py.\n",
    "\n",
    "The idea is to allow the possibility of having multiple agents evolving in stacked environments (particularly important when dealing with Atari frames for states), and thus get vectorized actions, rewards and state spaces with various shapes. Just as with Gym, there is a plethora of wrappers that the library uses, which are briefly described at the first link above. \n",
    "\n",
    "There are two subclasses of *VecEnv* that are called in practice. From the docs: \n",
    "* ***SubprocEnv***: Creates a multiprocess vectorized wrapper for multiple environments, distributing each environment to its own process, allowing significant speed up when the environment is computationally complex.\n",
    "* ***DummyVecEnv***: Creates a simple vectorized wrapper for multiple environments, calling each environment in sequence on the current Python process. This is useful for computationally simple environment such as cartpole-v1, as the overhead of multiprocess or multithread outweighs the environment computation time. This can also be used for RL methods that require a vectorized environment, but that you want **a single environment** to train with. \n",
    "\n",
    "The latter is the one used by FinRL for SB3 environments. The implementation is here:\n",
    "* https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/dummy_vec_env.py\n",
    "\n",
    "Comments and facts on *DummyVecEnv* and *VecEnv*:\n",
    "* *VecEnv* is quite similar to the *VectorEnv* class of Gym (https://github.com/openai/gym/blob/master/gym/vector/vector_env.py), but in contrast to the latter, *VecEnv* is an **abstract base class** (https://docs.python.org/3/library/abc.html) in SB3.\n",
    "* Concerning the classes representing state and action spaces, *VecEnv* declares these as *gym.spaces.Space* objects in the constructor. \n",
    "* Here is an important note from SB3's Vectorized Envs. docs: When using vectorized environments, the environments are automatically reset at the end of each episode. Thus, the observation returned for the i-th environment when *done[i]* is true will in fact be the first observation of the next episode, **not** the last observation of the episode that has just terminated. You can access the “real” final observation of the terminated episode—that is, the one that accompanied the *done* event provided by the underlying environment—using the *terminal_observation* keys in the info dicts returned by the *VecEnv*.\n",
    "* The *step()* method itself is not implemented in *DummyVecEnv*, it is only inherited from *VecEnv*, where it is implemented in a very abstract manner using the *typing* module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff6c5f4-edce-4b75-bd14-51dbb5dae4ea",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "outputs": [],
   "source": [
    "            # Typing imports\n",
    "            from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple, Type, Union\n",
    "            \n",
    "            # VecEnvStepReturn is what is returned by the step() method\n",
    "            # it contains the observation, reward, done, info for each env\n",
    "            VecEnvStepReturn = Tuple[VecEnvObs, np.ndarray, np.ndarray, List[Dict]]\n",
    "            \n",
    "            # Class def\n",
    "            class VecEnv(ABC):\n",
    "                ....\n",
    "                 def step(self, actions: np.ndarray) -> VecEnvStepReturn:\n",
    "                    \"\"\"\n",
    "                    Step the environments with the given action\n",
    "                    :param actions: the action\n",
    "                    :return: observation, reward, done, information\n",
    "                    \"\"\"\n",
    "                    self.step_async(actions)\n",
    "                    return self.step_wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18537382-24b3-47b4-ae30-28a6e59c881a",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "source": [
    "   Similarly, the usual *reset()*, *close()*, and *render()* are all abstract methods of *VecEnv* that wrap helper methods implemented in *DummyVecEnv*. For instance, *DummyVecEnv* class implements the *step_wait()* method called by *VecEnv*'s *step()* method. \n",
    "* In the end, a *DummyVecEnv* instance is constructed from stacked *gym.Env* objects and other environment functions, as one can see from its constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78b3e6-9dc9-4280-a52b-31d9e339cf94",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "outputs": [],
   "source": [
    "            def __init__(self, env_fns: List[Callable[[], gym.Env]]):\n",
    "                self.envs = [fn() for fn in env_fns]\n",
    "                env = self.envs[0]\n",
    "                VecEnv.__init__(self, len(env_fns), env.observation_space, env.action_space)\n",
    "                obs_space = env.observation_space\n",
    "                self.keys, shapes, dtypes = obs_space_info(obs_space)\n",
    "                ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb43d29-df7a-4325-882e-89165a601d18",
   "metadata": {
    "id": "a3828bdc-405a-48f4-b8f3-58efcd3b8c2d"
   },
   "source": [
    "In summary, and in relation to our use in FinRL, the *DummyVecEnv* class is simply a wrapper for the portfolio optimization environment that allows us to use SB3 agents. Concretely, this means that the state/action spaces and rewards constituting our MDP are specified in the *StockPortfolioEnv* class, which subclasses the foundational *gym.Env* class. In the next part we give some reminders on the main moving parts of this class.\n",
    "\n",
    "<span style=\"color:red\">**To do (22/09/30):**</span> Add a comment on how **custom** *step()*/*reset()*/*init()* methods from a gym environment are adopted by the *VecEnv*/*DummyVecEnv* classes. This is particularly important for the discussion of FinRL's *StockPortFolioEnv* below, since:\n",
    "\n",
    "* At first glance, this is not obvious from the code or the docs. What's the mechanism? Check:\n",
    "    * https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/dummy_vec_env.py\n",
    "    * https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/vec_env/base_vec_env.py\n",
    "* Unclear at the moment on what the argument \n",
    "    \n",
    "        env_fns: List[Callable[[], gym.Env]]\n",
    "  means.\n",
    "* Comment on the *DummyVecEnv.env_method(method_name, *method_args)* method. For instance, it is used for non-gym methods proper to *StockPortfolioEnv*.\n",
    "* Might be useful to give a sequence of method calls here.\n",
    "* There's more to say about environments in the *BaseAlgorithm* section. Algorithms have an *env* parameter that is wrapped by SB3 before being assigned to *BaseAlgorithm.env*. \n",
    "* In FinRL, *StockPortfolioEnv* is converted to a *DummyVecEnv* before being passed to the SB3 algorithm. This closes the loop between FinRL's *DRLAgent* class and SB3 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c11e0e-f73c-4009-af33-8c545097207e",
   "metadata": {
    "id": "dcb2eda3-7ab4-4dd7-899c-7122db66d540"
   },
   "source": [
    "#### Custom environments\n",
    "**Note (22/09/19):** See: https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2a727-fbba-47d6-ae27-5e1705bb093f",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "source": [
    "<a id='2.a.2'></a>\n",
    "#### a.2 - The *gym.Env* class\n",
    "\n",
    "To summarize, this class consists of:\n",
    "\n",
    "* ***action_space:*** This attribute encodes the actions in the environment, and is typically a subclass of *Space* discussed below.\n",
    "* ***observation_space:*** Same as above, but this attribute encodes the state space.\n",
    "* ***reset():*** This method puts resets the agent back to the initial state in the environment. It should always be called after instantiating the *Env* object to obtain the initial observation of the episode.\n",
    "* ***step():*** This method is used as follows:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa00584-2c7a-442c-8ad8-b9256fe8bf7d",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "outputs": [],
   "source": [
    "        next_state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97909674-c6ab-4261-956d-953f92a08f40",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "source": [
    "  The input is the action of the agent, and the output consists of the new state (NumPy array), the corresponding reward (float), and the boolean *done* saying whether or not *next_state* is terminal. The *info* output contains information relevant for debugging and learning.\n",
    "        \n",
    "* ***render():*** This method allows to visualize the evolution of the agent. It calls the PyGame package which in turn calls upon some lower-level C functions (one of the prerequisite packages of Gym is *SWIG*, which calls a C++ compiler). One of the args of *render()* is *mode*, which equals *'human'* by default and displays the game screen. Once done with rendering, one must call ***Env.close()*** to shutdown PyGame.\n",
    "\n",
    "The action space and the state space are represented by the abstract Gym class entitled ***Space***, whose two most relevant methods are:\n",
    "* ***sample()***, which returns a random sample from the space. This method is typically called when performing actions.\n",
    "* ***contains(x)***, which checks whether the state $x$ belongs to the space's domain.\n",
    "\n",
    "These abstract methods are reimplemented in the 3 usual subclasses of *Space*:\n",
    "* ***Discrete:*** This space class models finite spaces, with elements labelled from $0$ to $n$. The values assigned to each label are described in the environment subclass (see example below).\n",
    "\n",
    "* ***Box:*** Boxes represent $n$-dimensional tensor of rational numbers in intervals $[x_\\min, x_\\max]$, and have a *dtype* parameter in their constructor. The first use of this class is to define the bounds of a rectangular region that will be discretized in the background, and the *dtype* specifies the desired accuracy. There is also a *shape* argument in the constructor, which for example is used when the states are represented by screenshots of a game (think of the Atari environments). In the case of images of size 210x160 pixels, one calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e1199-0b21-4b63-82b9-67bb83fd79b1",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "outputs": [],
   "source": [
    "            Box(low = 0, high=255, size = (210, 160, 3)),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9014c-2a28-4ea6-a798-9016fb1540db",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "source": [
    "          \n",
    "    where 3 stands for the RGB channels.\n",
    "    \n",
    "* ***Tuple:*** Some spaces could be of various complexity, such as having discrete and continuous components. The *Tuple* class allows to define such spaces in a nested way, by combining the previous classes for instance (see example of car controls).\n",
    "\n",
    "In terms of RL algorithms, the *step()* method is the crucial one, as it implements the environment dynamics: This method is typically where the state transitions and rewards are computed. This remark will be of particular importance when returning to FinRL.\n",
    "\n",
    "**Some remarks on *gym.Env*:**\n",
    "- When calling *gym.make('Env_name')*, Gym actually calls a wrapper to create the environment, and not exactly the class itself.\n",
    "- Gym's pre-built environments do not readily give access to the possible actions in a given state or the rewards corresponding to a new state. To access these, one can use the ***env.unwrapped*** attribute, such as for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c4985-425c-4e0f-8636-21e0994f4978",
   "metadata": {
    "id": "f2bf3db6-0ed1-4af1-9a36-ec864dcd9c85"
   },
   "outputs": [],
   "source": [
    "        env = gym.make('CartPole-v1')\n",
    "        # Get action space\n",
    "        cartpole_action_space = env.unwrapped.action_space\n",
    "        # Get no. of possible actions\n",
    "        cartpole_n_actions = env.unwrapped.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e4fde-aee0-41f3-a13a-dcfcada436ca",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "<a id='2.b'></a>\n",
    "### b) Models I - Algorithm classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d6935-5c5b-42fc-84f0-765fca8db9bc",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "<a id='2.b.1'></a>\n",
    "#### b.1 - Basics\n",
    "\n",
    "The main documentation can be found at the following links: \n",
    "\n",
    "* Getting started: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html\n",
    "* Developer guide: https://stable-baselines3.readthedocs.io/en/master/guide/developer.html#\n",
    "* Algos: https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
    "* Base RL class: https://stable-baselines3.readthedocs.io/en/master/modules/base.html\n",
    "* Saving and loading: https://stable-baselines3.readthedocs.io/en/master/guide/save_format.html\n",
    "* More algorithms: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "One of the most interesting features of SB3 is the stable implementation of policy gradient and actor-critic methods, along with the intuitive interface for their training and deployment.\n",
    "\n",
    "Before delving into details, let's start with the algorithms structure discussed in the developer guide above: \n",
    "* The DRL algorithms that train agents are stored in individual submodules of stable_baselines3. \n",
    "* Each of these submodules contains two files: policy.py and algo_name.py. The former implements the policy, while the latter implements the training algorithm for the agent.\n",
    "* In SB3, \"policy\" refers to all neural nets involved in the algorithm, and not only the learned policy network.\n",
    "* Each algorithm has **two main methods**. First is *collect_rollouts()* that defines how the samples are collected, and stores them in a *RolloutBuffer* (discarded after grad. update) or a *ReplayBuffer*. Second is the *train()* method that updates the parameters using samples from the buffer.\n",
    "* All the environments handled by agents are inherited from *VecEnv*.\n",
    "* At present, the algorithms that SB3 supports (on- and off-policy versions) are A2C, DDPG, DQN, HER, PPO, SAC and TD3.\n",
    "\n",
    "On top of the algorithms implemented in the main SB3 library, there is a second library called *stable_baselines3-contrib* (sb3-contrib), where there are more experimental algorithms/agents implemented. Among the presently supported algorithms are ARS (Aug. Rand. Search), TRPO, and Recurrent PPO. The GitHub repo is here:\n",
    "\n",
    "https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "At this point, it's helpful to give an example of how to use SB3 models. Say we would like to train and deploy a PPO agent in the cartpole environment. The code will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cda8432-5410-440c-a471-c4910c227edc",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "outputs": [],
   "source": [
    "        import gym\n",
    "        from stable_baselines3 import PPO\n",
    "        from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "        # Create 4 parallel environments\n",
    "        env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "        \n",
    "        # Instantiate agent\n",
    "        model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "        \n",
    "        # Train\n",
    "        model.learn(total_timesteps=25000)\n",
    "        \n",
    "        # Save\n",
    "        model.save(\"ppo_cartpole\")\n",
    "\n",
    "        obs = env.reset()\n",
    "        while True:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, dones, info = env.step(action)\n",
    "            env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d52b0-9ef4-4a07-b6d9-c207d1662cb6",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "Loading a trained agent is done as follows:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f284897-eb71-4319-8e71-cc2db53ce6cd",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "outputs": [],
   "source": [
    "        from stable_baselines3 import PPO\n",
    "        model = PPO.load(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5216e6-2e19-4fa1-8ec8-bfc019f46475",
   "metadata": {
    "id": "378e7d71-9fe4-426d-9f54-e84f8b68f1ea"
   },
   "source": [
    "As mentioned above, any DRL algorithm consists of two parts: \n",
    "* (A) An algorithm class inherited from **BaseAlgorithm(ABC)**, which implements the sample collection (*collect_rollouts()*) and the corresponding learning algorithm (*train()* and *learn()*).\n",
    "* (B) A policy class inherited from **BasePolicy(torch.nn.Module)**, which gathers all neural nets used in the *collect_rollouts()* and *learn()* methods of the algorithm class. Any instance of a *BaseAlgorithm* subclass has a *policy* attribute that is a *BasePolicy* object.\n",
    "\n",
    "The *BasePolicy* class is discussed in Section (2.c). For the remainder of Section (2.b) we focus on the specifics of the subclasses of *BaseAlgorithm*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239e5b81-132d-4135-90c7-bc676787119c",
   "metadata": {
    "id": "53c98835-09a9-49d6-8ea3-ab71db561f4e"
   },
   "source": [
    "<a id='2.b.2'></a>\n",
    "#### b.2 - Abstract classes - *BaseAlgorithm*, *On/OffPolicyAlgorithm*\n",
    "\n",
    "The abstract base class underlying all SB3 algorithms is the *BaseAlgorithm* class in *stable_baselines3.common.base_class*. The implementation can be found here:\n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/base_class.py\n",
    "\n",
    "Stable Baselines 3 separates its DRL implementations into on-policy algorithms and off-policy ones. As such, any algorithm implemented in this library is a subclass of either *OnPolicyAlgorithm* or *OffPolicyAlgorithm*, (inherited from *BaseAlgorithm*). These are also in the submodule *stable_baselines3.common*, and their implementations are at:\n",
    "\n",
    "* On: https://github.com/DLR-RM/stable-baselines3/blob/88e1be9ff5e04b7688efa44951f845b7daf5717f/stable_baselines3/common/on_policy_algorithm.py\n",
    "* Off: https://github.com/DLR-RM/stable-baselines3/blob/88e1be9ff5e04b7688efa44951f845b7daf5717f/stable_baselines3/common/off_policy_algorithm.py\n",
    "\n",
    "We have the following class diagram for the algorithms implemented at the time of writing:\n",
    "\n",
    "<h3 align=\"center\"><img src = \"Figures/SB3_BaseAlgorithm_Class_Diagram.png\"  height= 340 width = 340></h3>\n",
    "\n",
    "For the sake of completeness and for future reference, we close this part by listing some of the interface components of SB3 on/off-policy classes.\n",
    "\n",
    "**Attributes:**\n",
    "* *policy*: *Policy* object, discussed in more detail below. Can be built at instantiation if *_init_setup_model*=True, and customized by passing the attribute *policy_kwargs* to the constructor.\n",
    "* *env*: The *gym.Env* environment to learn from. <span style=\"color:red\">**(Requires comments)**</span>\n",
    "* *learning_rate*: Learning rate for the optimizer, can be a function of the current progress remaining.\n",
    "* *gamma*: Discount factor.\n",
    "* *action_noise*: Action noise type (*None* by default) for hard exploration problems. See *common.noise* for the different action noise types.\n",
    "* *seed*: Seed for the pseudo-random generators.\n",
    "* *use_sde*: Whether to use generalized State Dependent Exploration (gSDE) instead of action noise exploration (default: False). The sample frequency can be customized with *sde_sample_freq*.\n",
    "* *device*: Specify CPU or GPU. Will try to use a Cuda compatible device by default and fallback to CPU otherwise.\n",
    "* For *OnPolicyAlgorithm*, one has attributes *ent_coef* (entropy coeff.), *vf_coef* (value f'n coeff.), and *max_grad_norm*.\n",
    "* For *OffPolicyAlgorithm*, one has attributes *batch_size*, *buffer_size*, *learning_starts*, *tau* (soft update coeff.), *train_freq*, *gradient_steps* etc.\n",
    "* *supported_action_spaces*: The action spaces supported by the algorithm.\n",
    "* In addition to the above, we also have the following attributes:\n",
    "    * For training monitoring: *monitor_wrapper*, *tensorboard_log*, and *verbose*.\n",
    "    * More environment attributes: e.g. *support_multi_env*, *create_eval_env*.\n",
    "\n",
    "The policies are handled using the *stable_baselines3.common.policies* submodule, and the replay buffer classes are in the *stable_baselines3.common.buffers* submodule.\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "* *\\_\\_init\\_\\_()*: The specific parameters of the constructor depend on the algorithm type (on/off-policy).\n",
    "* ***collect_rollouts()***: Collects experiences using the current policy and fills a *RolloutBuffer*. \"Rollout\" here refers to the model-free RL notion and **should not be confused** with the rollout concept of model-based RL or planning. This method is implemented in the *On/OffPolicyAlgorithm* classes (not in the Base), and calls the *step()* method of the environment, .\n",
    "* ***train()***: This is an abstract method of *On/OffPolicyAlgorithm*, and is **implemented for each deep RL algorithm individually**. In particular, this is the method containing the usual PyTorch training loop with *loss.backward()* and *optimizer.step()* (for a concrete example, see implementation at https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/dqn/dqn.py).\n",
    "* ***learn()***: Returns a trained model, included in the Base class and implemented differently in *On/OffPolicyAlgorithm*. The purpose of this method is to manage the *logger* and *callback* objects during training. It first calls *_setup_learn* to initialize training parameters, and next loops over time steps to collect rollouts using the method above. Once all the callback/logger instructions are done, the method *train* is called to update network weights.\n",
    "* ***predict()***: This is a method of *BaseAlgorithm* that wraps the *predict()* method of the *policy* attribute (see below).\n",
    "* *get_parameters()*: Returns the parameters of the agent. This includes parameters from different networks, e.g. critics (value functions) and policies (pi functions).\n",
    "* The remaining methods manage several other aspects of the interface. For instance:\n",
    "    * More deep learning functions: *_setup_model*, *_setup_lr_schedule*, *_update_current_progress_remaining*, *_update_learning_rate*, *_get_torch_save_params*, *_setup_learn*, *set_random_seed*,  (loads files), \n",
    "    * Saving and loading: *set_parameters*, *load*, *save*\n",
    "    * Environment methods: *_wrap_env*, *_get_eval_env*, *get_env*, *set_env*, *get_vec_normalize_env*.\n",
    "    * Memory buffer methods: *_update_info_buffer*, \n",
    "    * Callback methods: *_init_callback*, \n",
    "    * Logger methods: *set_logger*, *logger*, *_dump_logs*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763dcea-d817-4997-a53f-eee632ea29b3",
   "metadata": {},
   "source": [
    "<a id='2.b.3'></a>\n",
    "#### b.3 - The *learn()* and *train()* methods\n",
    "\n",
    "Regarding these two methods, here are the points to keep in mind:\n",
    "\n",
    "* The first notable difference between the *On/OffPolicyAlgorithm* classes is their *learn()* methods. Both call the methods *_setup_learn()*, *callback.on_training_start()*, *callback.on_training_end()*, *collect_rollouts()*, and both contain the main training loop where the *train()* method is called. The main difference is that *OnPolicyAlgorithm.learn()* records the training process in the *logger* attribute.\n",
    "\n",
    "* The *train()* method is where the deep reinforcement learning algorithms are effectively implemented. Indeed, *train()* is an abstract method of *On/OffPolicyAlgorithm.learn()*, and it is only implemented in their subclasses (A2C, PPO, TD3, DDPG, SAC and DQN).\n",
    "\n",
    "* The *collect_rollouts()* method is discussed in Sec. 2.b.4 below.\n",
    "\n",
    "<span style=\"color:red\">**(Requires more comments)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc00598-d99d-4e2d-af48-5db2551a9092",
   "metadata": {},
   "source": [
    "<a id='2.b.4'></a>\n",
    "#### b.4 - The *collect_rollouts()* method\n",
    "\n",
    "This method is implemented in the *On/OffPolicyAlgorithm* classes. <span style=\"color:red\">**(how different are they)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af88fd68-4e90-446b-8831-2303cdf4d8af",
   "metadata": {},
   "source": [
    "<a id='2.b.5'></a>\n",
    "#### b.5 - Saving and loading models\n",
    "\n",
    "* When loading a trained model, you do not need to have an environment argument.\n",
    "* SB3 saves a model into a zip file, containing the PyTorch weights and a text file with training params.\n",
    "* More explanations found in the docs: https://stable-baselines3.readthedocs.io/en/master/guide/save_format.html.\n",
    "\n",
    "<span style=\"color:red\">**(Finish this)**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e782cf0-31ec-42c3-8631-6e432c7f7115",
   "metadata": {
    "id": "53c98835-09a9-49d6-8ea3-ab71db561f4e"
   },
   "source": [
    "    \n",
    "Our next topic is the handling of neural networks and policies in *stable_baselines3*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1542a-6997-4c83-90f1-620087001e55",
   "metadata": {
    "id": "226b5d7f-448d-484c-8c7b-795a6eb1318c"
   },
   "source": [
    "<a id='2.c'></a>\n",
    "### c) Models II - Neural nets for deterministic policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c61200-8dda-4efd-aad8-01f52db2601c",
   "metadata": {
    "id": "226b5d7f-448d-484c-8c7b-795a6eb1318c"
   },
   "source": [
    "<a id='2.c.1'></a>\n",
    "#### c.1 - Basics\n",
    "\n",
    "The submodule of interest here is *stable_baselines3.common.policies*, which can be found at:\n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py.\n",
    "\n",
    "More information on the structure of SB3's neural nets can be found at the article on customization of policies:\n",
    "\n",
    "https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
    "\n",
    "Here we look at the *BaseModule*, *BasePolicy*, and their subclasses that constitute the *policy* attribute of *BaseAlgorithm*. As previously stated, this *BaseAlgorithm.policy* stores and manages **all** of the neural networks involved in a given algorithm, not only the policy network. Here is the class diagram of *BaseModule/BasePolicy*:\n",
    "\n",
    "<h3 align=\"center\"><img src = \"Figures/SB3_BaseModel_Class_Diagram.png\" height= 340 width = 340></h3>\n",
    "\n",
    "For the remainder Section 2.(c), we describe some implementation details of the neural nets used for deterministic gradient algorithms, by focusing on TD3 for the sake of concreteness. The stochastic policies will be discussed in Section 2.(d)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0d5da1-c779-4093-bc3e-5270a6438e9d",
   "metadata": {},
   "source": [
    "<a id='2.c.2'></a>\n",
    "#### c.2 - The *common.torch_layers* submodule\n",
    "\n",
    "Neural networks in SB3 are separated into two main parts:\n",
    "* A **feature extractor** network. This could be a CNN for instance when dealing with images, or a \"combined\" feature extractor when using Dict environments. This part of the model is encoded in the **features_extractor** attribute of  an algorithm's policy class.\n",
    "* A **fully-connected network** mapping features to the policy network (actions) or to the Q-function (values). The various SB3 policy classes (*ActorCriticPolicy*, *Actor*, *ContinuousCritic*, etc.) implement the actor and critic networks differently, depending on whether the policy is deterministic or stochastic.  \n",
    "\n",
    "The feature extractor of an SB3 \"policy\" is implemented as a separate class in the *common.torch_layers* submodule. The code and the documentation can be found at the following links:\n",
    "* Code: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/torch_layers.py.\n",
    "* Custom policies: https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html.\n",
    "\n",
    "The main facts to keep in mind about the feature extraction layers are the following:\n",
    "* The feature extractor of any SB3 policy is made of the level of the abstract *BaseModel* class. In the constructor of *BaseModel*, the relevant parameters are ***observation_space***, ***features_extractor_class***, ***features_extractor_kwargs***, and ***features_extractor***. By default, this contructor instantiates the *features_extractor* attribute using the *BaseModel.make_features_extractor()* method.\n",
    "* For **off-policy algorithms**, SB3 uses the classes *Actor[BasePolicy]* and *ContinuousCritic[BasePolicy]* to encode the actor and critic networks respectively. These have *net_arch* and *activation_fn* parameters to specify the layers of these networks on top of the feature extractor, and are built using the *torch_layers.create_mlp()* helper function.\n",
    "* For **on-policy algorithms**, the actor and critic networks are built quite differently, as they are encoded by the *common.policies.ActorCriticPolicy* class, which uses the *torch_layers.MlpExtractor* class instead of *Actor* and *ContinuousCritic*. We discuss this point in more detail in Section 2.(d) below, when dealing with stochastic policies.\n",
    "\n",
    "The next subsection gives a concrete example of the considerations here. For the sake of completeness, we close this part with a summary of the contents of the *torch_layers* submodule\n",
    "\n",
    "**Classes:**\n",
    "\n",
    "* *BaseFeaturesExtractor(nn.Module)*: Base class for feature extractors.\n",
    "* *FlattenExtractor(BaseFeaturesExtractor)*: Feature extractor flattening input. Used as placeholder when feature extraction not needed.\n",
    "* *NatureCNN(BaseFeaturesExtractor)*: The CNN from Mnih et al 2015 DQN paper.\n",
    "* *CombinedExtractor(BaseFeaturesExtractor)*: Used with *Dict* observation spaces. Builds a feature extractor for each key of the dictionary, so that inputs are fed through separate submodules, and then concatenated through an additional \"combined\" MLP net.\n",
    "* *MlpExtractor(nn.Module)*: Constructs an MLP whose inputs are outputs of a previous feature extractor (e.g. CNN) or environment observations. Outputs a latent representation for the policy and value networks. Has a *net_arch* parameter that specifies the no. and size of hidden layers, as well as the no. of shared layers between the policy and value nets.\n",
    "\n",
    "**Helper functions:**\n",
    "\n",
    "* *create_mlp(input_dim, output_dim, net_arch, activation_fn, squash_output)*: Creates a multi layer perceptron (MLP), which is a collection of fully-connected layers each followed by an activation function. Output is a list of *nn.Module* objects.\n",
    "* *get_actor_critic_arch(net_arch)*: Gets the policy and value network architectures for the off-policy actor-critic algorithms (SAC, DDPG, TD3). Has a *net_arch* argument that specifies the no. and size of hidden layers. Note: Other than the feature extraction layers, no other layers can be shared by the actor and critic networks. This is to avoid evaluation issues with the target networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a5d73-cbae-4f6f-8b9d-40b8453e82ee",
   "metadata": {},
   "source": [
    "<a id='2.c.3'></a>\n",
    "#### c.3 - Example: The *TD3.td3* and *TD3.policies* submodules\n",
    "\n",
    "The source code of this part is at the following links:\n",
    "\n",
    "* TD3 algorithm: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/td3/td3.py\n",
    "* TD3 policy: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/td3/policies.py\n",
    "* Off-policy algorithms: https://github.com/DLR-RM/stable-baselines3/blob/88e1be9ff5e04b7688efa44951f845b7daf5717f/stable_baselines3/common/off_policy_algorithm.py\n",
    "\n",
    "We note that our goal here is to give an idea of how the library is organized, and to give the readers a starting point to implement their own policies. As such, the present description of SB3 is in no way exhaustive, as there are several features that are not discussed (for example, the use of state-dependent exploration noise, or the structure of the replay buffers in *common.buffers*).\n",
    "\n",
    "In Section 2.(b.1), we gave a template of how an SB3 algorithm is instantiated, trained and deployed. We revisit this template below with comments on each step, where we give more details on the objects, functions and submodules involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305a7ba-c872-4861-897b-38ebd4a8c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "'''\n",
    "    Some comments on environments:\n",
    "    1) The instruction above is the same as gym.make(\"env_name\"), \n",
    "    except that we are wrapping the gym.Env object with a SB3 VecEnv class.\n",
    "    More generally, any SB3 algorithm is instatiated\n",
    "    with an env parameter.\n",
    "    \n",
    "    2) If an SB3 model is created with a gym.Env object\n",
    "    for the env argument, the BaseAlgorithm constructor \n",
    "    wraps it with a VecEnv object using: \n",
    "        env = self._wrap_env(env, self.verbose, monitor_wrapper)\n",
    "        (Line 159 of common.base_class)\n",
    "    This method looks for appropriate environment and monitor\n",
    "    wrappers and re-orders image channels.\n",
    "'''\n",
    "\n",
    "# Instantiate TD3 algorithm object\n",
    "model = TD3(policy = \"MlpPolicy\", env=env, verbose=1)\n",
    "'''\n",
    "    This instruction creates a TD3 object, with arguments specifying those of\n",
    "    the corresponding TD3Policy object (TD3.policy attribute).\n",
    "    \n",
    "    I - TD3Policy Constructor arguments:\n",
    "    ------------------------------------\n",
    "    1) For BaseAlgorithm, the policy argument is either \"MlpPolicy\", \"CnnPolicy\",\n",
    "    or \"MultiInputPolicy\". These 3 aliases are always defined in the corresponding \n",
    "    algorithm submodule. In the case of TD3, MlpPolicy = TD3Policy, and the feature\n",
    "    extractor class is FlattenExtractor. \"CnnPolicy\" is a subclass of TD3Policy for \n",
    "    which the feature extractor is a CNN (NatureCNN by default), typically used for\n",
    "    image data. \"MultiInputPolicy\" is a subclass of MlpPolicy for which the \n",
    "    environment has a Dict observation space, and whose feature extractor class\n",
    "    is CombinedFeatureExtractor.\n",
    "    \n",
    "    2) The env parameter could be a gymEnv, VecEnv or DummyVecEnv object, see comments\n",
    "    above. This parameter is not needed to load a trained SB3 model.\n",
    "    \n",
    "    3) To further customize the network architecture, one specifies the following\n",
    "    parameters: net_arch, activation_fn, features_extractor_class, \n",
    "    features_extractor_kwargs, and share_features_extractor.\n",
    "    \n",
    "    4) The optimizer parameters are optimizer_class and optimizer_kwargs. By \n",
    "    default, the optimizer class is torch.optim.Adam with eps = 1e-5.\n",
    "    \n",
    "    5) The training parameters are lr_schedule, optimizer_class, optimizer_kwargs,\n",
    "    and normalize_images.\n",
    "    \n",
    "    II - TD3Policy Neural nets:\n",
    "    ---------------------------\n",
    "    1) The TD3 constructor calls TD3Policy._build() to initialize 4 neural nets, \n",
    "    each of which is an attribute of the class TD3Policy. The helper functions used \n",
    "    in _build() are make_features_extractor(), make_actor(), and make_critic(). \n",
    "    As opposed to the last 2 functions, make_features_extractor() is inherited from \n",
    "    BaseModel (see line 114 of the common.policies submodule).\n",
    "    \n",
    "    2) Actor networks: The TD3Policy.actor and TD3Policy.actor_target attributes are\n",
    "    instances of the Actor[BasePolicy] class from the TD3.policies submodule. \n",
    "    In detail, the constructor of Actor calls common.torch_layers.create_mlp() to \n",
    "    combine the feature extractor layers and the activation function, then compiles\n",
    "    this module with:\n",
    "        (Line 60) self.mu = nn.Sequential(*actor_net) \n",
    "    The function TD3Policy.actor.forward() wraps mu.forward().\n",
    "    \n",
    "    3) Critic: The TD3Policy.critic and TD3Policy.critic_target attributes are\n",
    "    instances of the ContinuousCritic[BaseModel] class from the common.policies submodule.\n",
    "    This class allows to have several critic networks, and to decide whether or not\n",
    "    to share the feature extraction layers with the actor. \n",
    "    The main attribute of ContinuousCritic is the list of q_networks, which ss for the actor,\n",
    "    are created using common.torch_layers.create_mlp() and by setting:\n",
    "        (Line 875) q_net = nn.Sequential(*q_net)\n",
    "    for each critic in (see common.policies).\n",
    "    The ContinuousCritic.forward() function returns a tuple containing the output\n",
    "    of each of these networks, while the ContinuousCritic.q1_forward() function\n",
    "    returns only the output of the first entry in the ContinuousCritic.q_networks list.\n",
    "    \n",
    "    4) Aliases: The TD3 model constructor calls a helper method called _setup_model(),\n",
    "    which initializes the TD3.policy object as a TD3Policy instance, and then calls \n",
    "    _make_aliases() that initialized the TD3.actor, TD3.actor_target, TD3.critic, and \n",
    "    TD3.critic_target attributes. These are equated to the attributes of the same name \n",
    "    of TD3.policy, and used in the TD3.train() method discussed below.\n",
    "    \n",
    "'''\n",
    "        \n",
    "# Train TD3 model\n",
    "model.learn(total_timesteps=25000)\n",
    "'''\n",
    "    1) OffPolicyAlgorithm.learn() as an abstract method calls:\n",
    "        a) OffPolicyAlgorithm.collect_rollouts()\n",
    "        b) TD3.train()\n",
    "        \n",
    "    2) For (a) above, collect_rollouts() calls the OffPolicyAlgorithm.predict()\n",
    "    method. The details on the predict() method are given at the deployment\n",
    "    step below.\n",
    "    \n",
    "    3) For (b) above, TD3.train() implements the TD3 algorithm. It uses all the\n",
    "    neural networks built by the constructor with the following\n",
    "    calls in the main loop:\n",
    "        (Line 168) next_actions = (self.actor_target(replay_data.next_observations)\n",
    "                                  + noise).clamp(-1, 1)\n",
    "        (Line 171) next_q_values = th.cat(self.critic_target(replay_data.next_observations, \n",
    "                                   next_actions), dim=1)\n",
    "        (Line 176)  current_q_values = self.critic(replay_data.observations, \n",
    "                                       replay_data.actions)\n",
    "        (Line 190) actor_loss = -self.critic.q1_forward(replay_data.observations, \n",
    "                                self.actor(replay_data.observations)).mean()\n",
    "    All of these are calls to the forward() method of the corresponding network.\n",
    "    (The line numbers refer to the TD3.td3 submodule.)\n",
    "    \n",
    "    4) The critic loss computation and its gradient step are performed in\n",
    "    lines 178 to 185 of the TD3.td3 submodule.\n",
    "    \n",
    "    5) The actor loss computation and its gradient step are performed in \n",
    "    lines 189 to 196 of the Td3.td3 submodule.\n",
    "'''\n",
    "        \n",
    "# Deploy model\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # Predict actions\n",
    "    action, _states = model.predict(obs)\n",
    "    '''\n",
    "        1) At level of abstract classes, the above is BaseAlgorithm.predict(),\n",
    "        which wraps BaseAlgorithm.policy.predict(), which in turn wraps the abstract\n",
    "        method BasePolicy._predict() of the policy class.\n",
    "        \n",
    "        2) In the case of TD3: TD3Policy._predict() wraps the method:\n",
    "            TD3Policy.actor.forward()\n",
    "        (which is the same here as TD3Policy.forward().)\n",
    "        \n",
    "        3) The above wraps the method :\n",
    "            Actor.mu.forward()\n",
    "    '''\n",
    "    # One time step\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a393572b-195f-44d7-96a7-a9e79d63bc77",
   "metadata": {},
   "source": [
    "Our summary of SB3 policies started with the case of deterministic policies, whose architecture is less involved than that of stochastic policies. In particular, the example above explains SB3's implementations of both TD3 and DDPG, and also gives an idea of how the Q-network is implemented for DDQN (see: https://github.com/DLR-RM/stable-baselines3/tree/master/stable_baselines3/dqn)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785659f-00e6-4515-a71b-538635d8be61",
   "metadata": {
    "id": "226b5d7f-448d-484c-8c7b-795a6eb1318c"
   },
   "source": [
    "<a id='2.d'></a>\n",
    "### d) Models III - Neural nets for stochastic policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c0068-50b6-4bd6-8471-8f1ffa71c9cb",
   "metadata": {},
   "source": [
    "<a id='2.d.1'></a>\n",
    "#### d.1 - The *ActorCriticPolicy* class\n",
    "\n",
    "In this section we discuss the second design pattern used by Stable Baselines 3 for policies. We will mostly focus on the stochastic policies of on-policy algorithms such as A2C and PPO, by looking into the details of the *ActorCriticPolicy* class in the *common.policies* submodule.\n",
    "\n",
    "In contrast to the previous section, where the actor and critic networks were represented by distinct classes, the *ActorCriticPolicy* class uses several common layers between the actor and critic networks. In more detail:\n",
    "* On top of the *features_extractor* layers, *ActorCriticPolicy* has a *mlp_extractor* attribute that computes auxilary tensors *latent_vf* and *latent_pi*. These layers are shared by the attributes *value_net*, *action_net* and *log_std*.\n",
    "* The *features_extractor* and *mlp_extractor* layers are specified by the *features_extractor_class*, *features_extractor_kwargs*,*net_arch*, and *activation_fn* parameters of the constructor.\n",
    "* The stochastic policy is encoded by the *action_dist* attribute, which is a *Distribution* object from the *common.distributions* submodule. \n",
    "* The *action_net* attribute represents the neural net computing the mean of *action_dist*, while *log_std* is the net computing the log-standard deviation of this distribution.\n",
    "\n",
    "Section 2.(d.2) gives more details on the *Distribution* class of which *action_dist* is an instance. This is particularly important here, as it solves the additional layer of complexity coming from encoding a probability distribution and appropriately evaluating random actions during training.\n",
    "\n",
    "When instantiating an *ActorCriticPolicy* object, the attributes discussed above are initalized in the following order:\n",
    "1) *ActorCriticPolicy.__init__()* first initializes *features_extractor*.\n",
    "2) *action_dist* is initalized using *common.distributions.make_proba_distribution()*, according to the class of the action space (see next subsection for more details).\n",
    "3) The constructor calls the *_build()* method that calls *_build_mlp_extractor()*, which in turn initializes *mlp_extractor* (a *MlpExtractor* object, implemented in *common.toch_layers*).\n",
    "4) Within *_build()*, the attributes *action_net* and *log_std* are initialized using *action_dist.proba_distribution_net()*, depending on the distribution class used.\n",
    "5) *_build()* initializes *value_net* as a *nn.Linear* object with one output feature.\n",
    "\n",
    "The precise outputs *value_net*, *action_net*, and *log_std* networks are specified in the *ActorCriticPolicy.forward()* method, reproduced in the next cell with comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1563058-e3a6-478d-9d2f-bd39b899ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, obs: th.Tensor, deterministic: bool = False) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        ### This part calls the common.torch_layers submodule\n",
    "        # Preprocess the observation if needed\n",
    "        features = self.extract_features(obs)\n",
    "        '''\n",
    "            This is a method inherited from BasePolicy that\n",
    "            performs the following sequence of calls:\n",
    "            BasePolicy.extract_features()->BaseFeaturesExtractor.features_extractor.forward().\n",
    "        '''\n",
    "        \n",
    "        # Compute latent policy and latent value function\n",
    "        latent_pi, latent_vf = self.mlp_extractor(features)\n",
    "        '''\n",
    "            1) Here, \"latent\" refers to the shared features\n",
    "            by the actor and critic networks, which are\n",
    "            computed by the feature extraction layers.\n",
    "            2) The line above is in fact a call to\n",
    "            MlpExtractor.forward(), which returns the outputs\n",
    "            of:\n",
    "            - MlpExtractor.policy_net.forward()\n",
    "            - MlpExtractor.value_net.forward()\n",
    "        '''\n",
    "        \n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        '''\n",
    "            This line calls ActorCriticPolicy.value_net, whic is \n",
    "            initialized in ActorCriticPolicy._build() as:\n",
    "               self.value_net = nn.Linear(self.mlp_extractor.latent_dim_vf, 1)\n",
    "        '''\n",
    "        \n",
    "        ### This part calls the common.distributions submodule\n",
    "        # Get action_distribution from latent features\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi)\n",
    "        '''\n",
    "            1) This line involves three attributes:\n",
    "            - ActorCriticPolicy.action_dist \n",
    "            - ActorCriticPolicy.action_net \n",
    "            - ActorCriticPolicy.log_std\n",
    "            and wraps the function:\n",
    "                ActorCriticPolicy.action_dist.proba_distribution(),\n",
    "            which returns the latent probability distribution.\n",
    "            2) For continuous actions distributed according to a Gaussian, \n",
    "            action_dist.proba_distribution_net() sets up the mean and std\n",
    "            via:\n",
    "                action_net =  nn.Linear(latent_dim, self.action_dim) \n",
    "                log_std = nn.Parameter(th.ones(self.action_dim) * log_std_init, \n",
    "                                       requires_grad=True)\n",
    "                (see common.distributions submodule).\n",
    "        '''\n",
    "        \n",
    "        # Compute actions from the action distribution and their log probabilities\n",
    "        actions = distribution.get_actions(deterministic=deterministic)\n",
    "        '''\n",
    "            This calls the Distribution.get_actions() method that\n",
    "            calls one of the following 2 abstract methods: \n",
    "            - sample() if deterministic = False\n",
    "            - mode() if deterministic = True.\n",
    "        '''\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        actions = actions.reshape((-1,) + self.action_space.shape)\n",
    "        \n",
    "        \n",
    "        # return output\n",
    "        return actions, values, log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f2a20-2f4c-4c75-9912-dd599b5893da",
   "metadata": {},
   "source": [
    "One of the confusing points about *ActorCriticPolicy* are the various methods used to compute the outputs of the neural nets involved. These are as follows:\n",
    "\n",
    "* *forward()*: Forward pass in all networks: feature extractor, MLP extractor, value_net, and action_net. Used when collecting rollouts (see ***OnPolicyAlgorithm.collect_rollouts()***).\n",
    "* *evaluate_actions()*: Given an observation and actions, get corresponding values and log probabilities. Used during training (see ***PPO.train()*** for a concrete example).\n",
    "* *extract_features()*: Inherited from *BaseModel*. Preprocess observation if needed and call *forward()* method of *feature_extractor*.\n",
    "* *get_distribution()*: Get current policy distribution corresponding to observation.\n",
    "* *_get_action_dist_from_latent()*: Returns the action distribution from the latent values of features (*latent_pi* output by *mlp_extractor*)\n",
    "* *_predict()*: Given an observation, get action from the policy distribution. Wraps the instruction self.get_distribution(observation).get_actions(deterministic=deterministic), and is wrapped by *BasePolicy.predict()*. Used when deploying a trained model for instance.\n",
    "* *predict_values()*: Get estimated Q-value according to current policy, and given an observation. (See ***OnPolicyAlgorithm.collect_rollouts()***)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbd6d15-adba-4a63-a731-589be034708c",
   "metadata": {},
   "source": [
    "<a id='2.d.2'></a>\n",
    "#### d.2 - The *common.distributions* submodule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d44afb-b9f2-46f5-835a-609b2de8059c",
   "metadata": {},
   "source": [
    "As one would expect, the *distributions* submodule is central to the implementation of stochastic policies:\n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/torch_layers.py\n",
    "\n",
    "It might be important to note that *common.distributions* imports PyTorch's distributions submodule *torch.distributions*, which is itself inspired by TensorFlow's distributions package. Some useful links:\n",
    "* Torch distributions docs: https://pytorch.org/docs/stable/distributions.html.\n",
    "* TensorFlow distributions paper: https://arxiv.org/pdf/1711.10604.pdf.\n",
    "* PyTorch abstract *Ditribution* class: https://github.com/pytorch/pytorch/blob/master/torch/distributions/normal.py.\n",
    "* PyTorch *Normal* class: https://github.com/pytorch/pytorch/blob/master/torch/distributions/normal.py.\n",
    "* PyTorch Kullback-Liebler submodule: https://github.com/pytorch/pytorch/blob/master/torch/distributions/kl.py\n",
    "\n",
    "In particular, SB3's distributions submodule imports the *Bernoulli*, *Categorical*, and *Normal* classes from PyTorch.\n",
    "\n",
    "For reference, this submodule contains the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd264a83-5b9f-4ec3-b47b-24e121416828",
   "metadata": {},
   "source": [
    "**Classes:**\n",
    "\n",
    "* *Distribution(ABC)*: Base abstract class for SB3 distributions. Has one attribute *distribution*, and declares most of the abstract **FINISH THIS**.\n",
    "* *DiagGaussianDistribution(Distribution)*: To model Gaussian distributions with diagonal covariance matrix on continuous action spaces. Superclass of *SquashedDiagGaussianDistribution*.\n",
    "* *CategoricalDistribution(Distribution)*: To model discrete distributions on discrete action spaces. Superclass of *MultiCategoricalDisrtibution*.\n",
    "* *BernoulliDistribution(Distribution)*: Bernoulli distribution for MultiBinary action spaces.\n",
    "* *StateDependentNoiseDistribution(Distribution)*: Used for state-dependent noise exploration, as used elsewhere in SB3.\n",
    "* *TanhBijector*: Bijective transformation of a probability distribution. Used in implementation of SAC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9075cea-c70f-49c2-a364-4fc8b1579392",
   "metadata": {},
   "source": [
    "**Helper functions:**\n",
    "\n",
    "* *sum_independent_dims()*: Function summing log probabilities when computing the entropy of a Gaussian.\n",
    "* *make_proba_distribution(action_space, use_sde, dist_kwargs)*: Takes a *gym.spaces.Space* argument and distribution arguments to return a *Distribution* instance adapted to the action space class. Notably, if the action space class is *gym.spaces.Box*, then the output is a *DiagGaussianDistribution* object of the appropriate action space dimension. If the action space if of *gym.spaces.Discrete* class, then the output is of *CategoricalDistribution* class.\n",
    "* *kl_divergence(dist_true, dist_pred)*: Wrapper for PyTorch's *torch.distributions.kl_divergence(P,Q)*. Inputs are SB3 distributions, and output is a *torch.Tensor*. **Note:** Comment on how SB3's distributions wrap PyTorch's."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22f240e-a1cf-493f-a4ac-bc78edcd4e2b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Finish this. Discuss sampling and mode().**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed58fb-dda1-4554-a282-7802f1232141",
   "metadata": {},
   "source": [
    "<a id='2.d.3'></a>\n",
    "#### d.3 - Example: The *PPO.ppo* and *PPO.policy* submodules\n",
    "\n",
    "<span style=\"color:red\">**Finish this...**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830628f-dfd0-4664-a704-19978b61bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=1)\n",
    "\n",
    "# Instantiate TD3 algorithm object\n",
    "model = PPO(policy = \"MlpPolicy\", env=env, verbose=1)\n",
    "'''\n",
    "    1) Differences between \"MlpPolicy\", \"CnnPolicy\", and \"MultiInputPolicy\" for PPO\n",
    "    \n",
    "    2) Action distribution\n",
    "    \n",
    "    3) Actor network: \n",
    "    \n",
    "    4) Critic: \n",
    "    \n",
    "    ?) Aliases?\n",
    "    \n",
    "'''\n",
    "        \n",
    "# Train PPO model\n",
    "model.learn(total_timesteps=25000)\n",
    "'''\n",
    "    1) OnPolicyAlgorithm.learn() as an abstract method calls:\n",
    "        a) OnPolicyAlgorithm.collect_rollouts()\n",
    "        b) PPO.train()\n",
    "        \n",
    "    2) For (a) above, collect_rollouts() calls \n",
    "        (Line 166) actions, values, log_probs = self.policy(obs_tensor)\n",
    "        (Line 210) values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))\n",
    "        (in common.OnPolicyAlgorithm submodule)\n",
    "    \n",
    "    3) For (b) above, PPO.train() implements the PPO algorithm. \n",
    "        (Line 208) values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
    "    \n",
    "    4) Computation of loss functions and gradient step.\n",
    "'''\n",
    "        \n",
    "# Deploy model\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    # Predict actions\n",
    "    action, _states = model.predict(obs)\n",
    "    '''\n",
    "        This line wraps the ActorCriticAlgorithm._predict() method, which\n",
    "        returns:\n",
    "            (Line 613) ActorCriticAlgorithm.get_distribution(observation).get_actions(deterministic=deterministic)\n",
    "            (see common.policies and common.distributions)\n",
    "    '''\n",
    "    # One time step\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b03ec2-df57-4ad5-a135-d27aab1831be",
   "metadata": {
    "id": "41b03ec2-df57-4ad5-a135-d27aab1831be"
   },
   "source": [
    "<a id='2.d'></a>\n",
    "### d) Loggers\n",
    "**Note (22/09/19):** This is about the information logged during model training. In *BaseAlgorithm*, can find the methods *logger()* and *set_logger()* for instance See: https://stable-baselines3.readthedocs.io/en/master/common/logger.html#logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e987a5-1a80-4f0a-9b88-8d296e1e981e",
   "metadata": {
    "id": "a2e987a5-1a80-4f0a-9b88-8d296e1e981e"
   },
   "source": [
    "<a id='2.e'></a>\n",
    "### e) TensorBoard and Callbacks\n",
    "**Note (22/09/19):** This is mostly about Tensorboard integration. See:\n",
    "\n",
    "* https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n",
    "* https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc3472-ae98-41a9-853c-7dd0e113aeb7",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881bcf69-5f50-485a-9830-6d39eca07b25",
   "metadata": {
    "id": "881bcf69-5f50-485a-9830-6d39eca07b25"
   },
   "source": [
    "<a id='3'></a>\n",
    "## 3 - Portfolio optimization environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27cb88-997a-433b-b63d-00d71ebb00fd",
   "metadata": {
    "id": "9f27cb88-997a-433b-b63d-00d71ebb00fd"
   },
   "source": [
    "<a id='3.a'></a>\n",
    "### a) FinRL's updated environment\n",
    "\n",
    "This class definition is from: https://github.com/AI4Finance-Foundation/FinRL/blob/master/tutorials/2-Advance/FinRL_PortfolioAllocation_Explainable_DRL.ipynb. \"Updated\" here refers to the fact that the implementation below isn't the one in the library files at the time of writing (Sept. 2022).\n",
    "\n",
    "Below is the code with my comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efedbe7e-6f99-4462-a61f-a8c5e55e7c84",
   "metadata": {
    "id": "efedbe7e-6f99-4462-a61f-a8c5e55e7c84"
   },
   "outputs": [],
   "source": [
    "#######################################\n",
    "##### STOCK PORTFOLIO ENVIRONMENT #####\n",
    "#######################################\n",
    "\n",
    "'''\n",
    "### Imports\n",
    "# Other than the usual: np, pd, gym, plt, matplotlib.\n",
    "\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "'''\n",
    "\n",
    "class StockPortfolioEnv(gym.Env):\n",
    "    \"\"\"A portfolio allocation environment for OpenAI gym\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        df: DataFrame\n",
    "            input data\n",
    "        stock_dim : int\n",
    "            number of unique stocks\n",
    "        hmax : int\n",
    "            maximum number of shares to trade\n",
    "        initial_amount : int\n",
    "            start money\n",
    "        transaction_cost_pct: float\n",
    "            transaction cost percentage per trade\n",
    "        reward_scaling: float\n",
    "            scaling factor for reward, good for training\n",
    "        state_space: int\n",
    "            the dimension of input features\n",
    "        action_space: int\n",
    "            equals stock dimension\n",
    "        tech_indicator_list: list\n",
    "            a list of technical indicator names\n",
    "        turbulence_threshold: int\n",
    "            a threshold to control risk aversion\n",
    "        day: int\n",
    "            an increment number to control date\n",
    "            \n",
    "        ### Addendas (22/09/28, AJ Zerouali)\n",
    "        # Several other attributes are used below\n",
    "        reward: float\n",
    "            Current portfolio value. Only computed in step().\n",
    "        lookback: int\n",
    "            No. of lookback days for computation of covariances of asset returns\n",
    "        data: pd.DataFrame\n",
    "            Data on self.day, i.e. stock prices, tech. indicators, cov and returns\n",
    "        terminal: bool\n",
    "            Attribute signaling last day of input data df\n",
    "        portfolio_return_memory: list\n",
    "            List of  what?\n",
    "        portfolio_value: ?\n",
    "        covs: np.ndarray, shape = (stock_dim, stock_dim)\n",
    "            Covariance of portfolio asset returns on self.day.\n",
    "        date_memory: list of str\n",
    "            List of dates corresponding to self.day attribute.\n",
    "        \n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    _sell_stock() (NOT IMPLEMENTED)\n",
    "        perform sell action based on the sign of the action\n",
    "    _buy_stock() (NOT IMPLEMENTED)\n",
    "        perform buy action based on the sign of the action\n",
    "    step()\n",
    "        at each step the agent will return actions, then \n",
    "        we will calculate the reward, and return the next observation.\n",
    "    reset()\n",
    "        reset the environment\n",
    "    render()\n",
    "        use render to return other functions\n",
    "    save_asset_memory()\n",
    "        return history of portfolio returns over dates in df\n",
    "    save_action_memory()\n",
    "        return history of portfolio weights\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    ###################\n",
    "    ### Constructor ###\n",
    "    ###################\n",
    "    def __init__(self, \n",
    "                df,\n",
    "                stock_dim,\n",
    "                hmax,\n",
    "                initial_amount,\n",
    "                transaction_cost_pct,\n",
    "                reward_scaling,\n",
    "                state_space,\n",
    "                action_space,\n",
    "                tech_indicator_list,\n",
    "                turbulence_threshold=None,\n",
    "                lookback=252,\n",
    "                day = 0):\n",
    "\n",
    "        self.day = day\n",
    "        self.lookback=lookback # Clarify\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax # Max no. of stocks to trade\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct =transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling # Clarify\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "       \n",
    "        '''\n",
    "         Although they normalize the action and state space elements, \n",
    "         the portfolio weights (which are supposed to be actions) are\n",
    "         not included in the states.\n",
    "        '''\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,)) # Clarify\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape = (self.state_space+len(self.tech_indicator_list),self.state_space))\n",
    "        \n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day,:] # Clarify. Do not confuse with self.df.\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0) # Clarify\n",
    "        self.terminal = False     \n",
    "        self.turbulence_threshold = turbulence_threshold        \n",
    "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] # Clarify\n",
    "        \n",
    "        '''\n",
    "         Notes:\n",
    "         - There are no attributes for the state/action spaces\n",
    "           dimensions.\n",
    "         - They call the number of stocks the stock dimension.\n",
    "         - Actions correspond to outputs of neural nets. Portfolio\n",
    "           weights are softmax outputs of actions.\n",
    "         - States consist of covariance matrices of returns (computed \n",
    "           over lookback period) and chosen technical indicators.\n",
    "         - The number of days in one episode should be added as\n",
    "           an attribute. The method len(df.index.unique()) used \n",
    "           to compute it is slow.\n",
    "         - There is a self.data attribute whose function is not \n",
    "           explained, but that appears in main computations.\n",
    "           It's a dataframe of with columns given by:\n",
    "           date-OHLCV-tech. ind.-cov.- asset returns.\n",
    "         - The self.day attribute is a date index that is incremented\n",
    "           with the step() method during transitions.\n",
    "         - Attibutes with the '_memory' suffix are daily lists.\n",
    "        '''\n",
    "\n",
    "    ############################\n",
    "    ### Gym's reset() method ###\n",
    "    ############################\n",
    "    def reset(self):\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day,:]\n",
    "        # load states\n",
    "        self.covs = self.data['cov_list'].values[0]\n",
    "        self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "        self.portfolio_value = self.initial_amount\n",
    "        #self.cost = 0\n",
    "        #self.trades = 0\n",
    "        self.terminal = False \n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory=[[1/self.stock_dim]*self.stock_dim]\n",
    "        self.date_memory=[self.data.date.unique()[0]] \n",
    "        return self.state\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        return self.state\n",
    "    \n",
    "    ###########################\n",
    "    ### Gym's step() method ###\n",
    "    ###########################\n",
    "    def step(self, actions):\n",
    "        # Update self.terminal attribute\n",
    "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
    "        '''\n",
    "          They using len(self.df.index.unique()) for the number of days\n",
    "          in the dataset, because there's no attribute storing that value.\n",
    "          The length of the data set equals the number of stocks times\n",
    "          the number of days in the df input (after pre-processing).\n",
    "        '''\n",
    "        \n",
    "        ## If currently at terminal state ##\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory) # Clarify\n",
    "            df.columns = ['daily_return']\n",
    "            plt.plot(df.daily_return.cumsum(),'r')\n",
    "            plt.savefig('results/cumulative_reward.png')\n",
    "            plt.close()\n",
    "            \n",
    "            plt.plot(self.portfolio_return_memory,'r')\n",
    "            plt.savefig('results/rewards.png')\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))           \n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = ['daily_return']\n",
    "            \n",
    "            # Compute the Sharpe ratio at last step of episode.\n",
    "            if df_daily_return['daily_return'].std() !=0:\n",
    "                sharpe = (252**0.5)*df_daily_return['daily_return'].mean()/ \\\n",
    "                       df_daily_return['daily_return'].std()\n",
    "                print(\"Sharpe: \",sharpe)\n",
    "            print(\"=================================\")\n",
    "            \n",
    "            return self.state, self.reward, self.terminal,{}\n",
    "\n",
    "        ## If not at end of episode ##\n",
    "        else:\n",
    "            \n",
    "            # Compute current weights from actions\n",
    "            weights = self.softmax_normalization(actions) \n",
    "\n",
    "            # Store data\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            #Load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day,:]\n",
    "            self.covs = self.data['cov_list'].values[0]\n",
    "            self.state =  np.append(np.array(self.covs), [self.data[tech].values.tolist() for tech in self.tech_indicator_list ], axis=0)\n",
    "\n",
    "            # Compute potfolio return and log return\n",
    "            portfolio_return = sum(((self.data.close.values / last_day_memory.close.values)-1)*weights)\n",
    "            log_portfolio_return = np.log(sum((self.data.close.values / last_day_memory.close.values)*weights))\n",
    "\n",
    "            # Compute reward (portfolio value)\n",
    "            ### (Reward is the new portfolio value or end portfolo value)\n",
    "            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "            # Update memory attributes\n",
    "            self.actions_memory.append(weights)\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data.date.unique()[0])            \n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "            \n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "    \n",
    "        \n",
    "    ###############\n",
    "    ### Softmax ###\n",
    "    ###############\n",
    "    ### 22/09/28, AJ Zerouali\n",
    "    '''\n",
    "      This is to compute the portfolio weights. \n",
    "      The actions in this environment correspond\n",
    "      to portfolio weights, but the output of the policy\n",
    "      networks used is not necessarily normalized.\n",
    "    '''\n",
    "    def softmax_normalization(self, actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator/denominator\n",
    "        return softmax_output\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        \"\"\"\n",
    "            Makes an 'account_value' dataframe with dates and daily portfolio values\n",
    "            Should actually be called get_pf_value_hist.\n",
    "        \"\"\" \n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        #print(len(date_list))\n",
    "        #print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame({'date':date_list,'daily_return':portfolio_return})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        \"\"\"\n",
    "            Makes a dataframe with dates and daily portfolio weights.\n",
    "            Should actually be called get_pf_weights_hist.\n",
    "        \"\"\" \n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = ['date']\n",
    "        \n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        #df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    ###################\n",
    "    ### Seed method ###\n",
    "    ###################\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    #######################\n",
    "    ### SB3 environment ###\n",
    "    #######################\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397ce1fa-bed4-4ac9-b72f-ae6acd228b00",
   "metadata": {
    "id": "397ce1fa-bed4-4ac9-b72f-ae6acd228b00"
   },
   "source": [
    "**Remark:** From the *StockPortfolioEnv* class above, we see that there's a method *get_sb_env()* which instantiates a *DummyVecEnv* object (in the submodule *stable_baselines3.common.vec_env*). In brief, since we use SB3 agents/models, we convert the FinRL environment to an SB3 one using *get_sb_env()*. We emphasize that this last method only converts the FinRL environment to an SB3 one that is reset to the first date of the (pre-processed) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adca147d-2677-4927-ae55-00041eac4576",
   "metadata": {
    "id": "bb6c480a-a57d-45f3-b799-2adcdf3a336b"
   },
   "source": [
    "<a id='3.b'></a>\n",
    "### b) Attributes\n",
    "\n",
    "We give a brief description of the attributes of *StockPortfolioEnv*, grouped by category:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecb2c9c-4824-4d3e-9646-7fdff90d65fa",
   "metadata": {
    "id": "bb6c480a-a57d-45f3-b799-2adcdf3a336b"
   },
   "source": [
    "**Data management attributes:**\n",
    "\n",
    "* *df*: The input *pd.DataFrame* with pre-processed data. The columns consist of 'date', OHLCV columns, technical indicator columns, 'tic' for ticker, 'cov_list' and 'return_list'. Furthermore:\n",
    "    * The indexes of the *df* dataframe correspond to the day index (see *day* below), while the 'date' column contains the dates as strings in the 'yyyy-mm-dd' format. \n",
    "    * The 'return_list' column stores the daily asset returns for the portfolio assets over the previous n=*lookback* days. On a given date, all assets in the portfolio have the same list of returns.\n",
    "    * The 'cov_list' column stores the covariance matrix of the asset returns over the previous n=*lookback* days. On a given date, all assets in the portfolio have the same covariance array.\n",
    "\n",
    "* *lookback*: Number of lookback days used for the computation of the arrays in the 'cov_list' and 'return_list' columns. This parameter is specified at the data processing step.\n",
    "\n",
    "* *day*: Day index in the df dataset, or \"current time step\" of the episode/trading period. The methods of *StockPortfolioEnv* use this *int* for the computations rather than the dates. *day* is set to 0 in *reset()* and incremented in the *step()* method. \n",
    "\n",
    "* *stock_dim*: Number of (unique) stocks in the 'tic' column of *df*, i.e. no. of portfolio assets. Used for the contruction of the state space and action space, as well as their related attributes.\n",
    "\n",
    "**State attributes:**\n",
    "\n",
    "* *initial_amount*: Initial cash in hand for the portfolio modelled by the environment.\n",
    "\n",
    "* *data*: Sub-dataframe of *df* with index t=*day*. Contains the prices, tech. indicators and covariances/asst. rets. on a fixed day, for all tickers. Updated in the *step()* method.\n",
    "\n",
    "* *state_space*: Described as the no. of input features, but used as the no. of portfolio stocks.\n",
    "\n",
    "* *observation_space*: Represents the state space of the MDP. Initialized in the constructor as a *gym.spaces.Box* object, of shape = (n_stocks+n_tech_ind, n_stocks) and no upper/lower bounds. Here, n_tech_ind is the length of the *tech_indicator_list* list below.\n",
    "\n",
    "* *tech_indicator_list*: List of technical indicators used for the construction of the MDP states. It only needs to be a subset of the technical indicators in the input dataframe *df*.\n",
    "\n",
    "* *terminal*: Boolean indicator whether the current state of the environment is terminal, i.e. whether we have reached the last *day* index in *df*. True when *day* is grater or equals *len(self.df.index.unique())-1* (see step method).\n",
    "\n",
    "* *covs*: Stores the covariance matrix of asset returns at time step t=*day*, i.e. the entry of the 'cov_list' column of *data*.\n",
    "\n",
    "* ***state*** The state is obtained by concatenating the covariance matrix *covs* and the technical indicators specified in the *tech_indicator_list* attribute. The states are encoded as *float64* *np.ndarray*'s, with *shape*=*(n_stocks+n_tech_indicators, n_stocks)*.\n",
    "\n",
    "* *portfolio_value*: The portfolio value at time-step *day*, computed in terms of the portfolio return. In the *step()* method, the portfolio return is computed as the weighted sum of asset returns over the close prices in *data*.\n",
    "\n",
    "**Action attributes:**\n",
    "\n",
    "* *action_space*: As a parameter for the constructor, this is the number of portfolio stocks. The constructor then assigns a *gym.spaces.Box* object to this attribute, with shape (n_stocks,) and no upper/lower bounds.\n",
    "\n",
    "* *hmax*: Maximum number of shares to trade. Not used in this implementation.\n",
    "\n",
    "**Reward attributes:**\n",
    "\n",
    "* *reward*: This attribute is not initialized in the constructor. The only part where is appears is in the *step()* method.\n",
    "\n",
    "* *transaction_cost_pct*: Transaction cost per trade. Not used in the present implementation.\n",
    "\n",
    "* *reward_scaling*: Scaling factor for rewards. Not used in this implementation.\n",
    "\n",
    "* *turbulence_threshold*: Risk aversion threshold related to the turbulence technical indicator. Not used in this implementation.\n",
    "\n",
    "**Portfolio history attributes:**\n",
    "\n",
    "All of the following are lists updated when the *step()* method is executed.\n",
    "\n",
    "* *date_memory*: A list of dates in *str* format. Initialized with the date in *df* corresponding to *day*=0, and then filled with the contents of the 'date' column as the *day* index is incremented.\n",
    "\n",
    "* *actions_memory*: A list of daily portfolio weights (the latter are also encoded as a list). Stores the portfolio weights history, and can be accessed using the *save_action_memory()* method.\n",
    "\n",
    "* *portfolio_return_memory*: History of daily portfolio returns, list of floats.\n",
    "\n",
    "* *asset_memory*: History of daily portfolio values, list of floats. This is the return of the *save_asset_memory()* method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03755a92-2aa8-4368-b38c-eb178db58a4c",
   "metadata": {
    "id": "bb6c480a-a57d-45f3-b799-2adcdf3a336b"
   },
   "source": [
    "**Comments:** \n",
    "\n",
    "1) For a more realistic implementation of a portfolio optimization environment, there should also be:\n",
    "\n",
    "* An array of integers storing the number of shares for each of the porfolio assets.\n",
    "\n",
    "* A cash-in-hand attribute.\n",
    "\n",
    "* The state of the portfolio could possibly store the number of shares, the cash in hand, as well as the portfolio value.\n",
    "\n",
    "* The portfolio weights should be converted to numbers of shares to buy or sell at a given time step.\n",
    "   \n",
    "2) In terms of software design:\n",
    "\n",
    "* Instead of *stock_dim*, I would've called this attribute *N_stocks*, and I would have added *N_days* for the number of days in the input dataset. The implementation accessed this quantity using *len(self.df.date.unique())*, which is slow given that Pandas needs to search through a 15MB dataframe.\n",
    "\n",
    "* The format of the input dataframe *df* is far from optimal. Using the sub-dataframe *data* at each time step could be inadequate for deploying models to paper-trading over 30min rebalancing of the portfolio for instance.\n",
    "    \n",
    "We additionally give some related comments at the end of the next subsection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1958d45c-901f-4260-90df-823318db7afa",
   "metadata": {
    "id": "bb6c480a-a57d-45f3-b799-2adcdf3a336b"
   },
   "source": [
    "<a id='3.c'></a>\n",
    "### c) The *step()* and *reset()* methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3e5087-9a2b-46c3-8795-038c51ff4d0f",
   "metadata": {},
   "source": [
    "In brief, here are the main things to record from the transitions obtained using this implementation of *step()*:\n",
    "\n",
    "**(1)** When the environment is not in a terminal state:\n",
    "\n",
    "* FinRL encode portfolio weights as the softmax of the actions obtained from the agents. \n",
    "* The reward function used is **not** the portfolio return, it's the entire **portfolio value**. This raises a conceptual question regarding the meaning of the expected cumulative returns obtained from these rewards.\n",
    "* We note that the *transaction_cost_pct* is not used anywhere in this implementation.\n",
    "\n",
    "Other than that, the overall methodology is standard. \n",
    "\n",
    "**(2)** When the environment is at a terminal state:\n",
    "* The Sharpe ratio is computed over the portfolio values list *asset_memory*. \n",
    "* The cumulative returns and the daily returns are plotted and saved to png files.\n",
    "* The reward is not updated in this implementation.\n",
    "\n",
    "The *reset()* method executes essentially the same instructions, with the appropriate instructions for the portfolio value and return on *day*=0.\n",
    "\n",
    "Now we comment on what is missing from the *step()* method, and more generally from *StockPortfolioEnv*:\n",
    "\n",
    "* There should be a *get_N_shares()* method that converts the portfolio weights to signed integers representing the number of shares to buy or sell for each asset. This pre-supposes the existence of a *N_stock_shares* attribute. The *step()* method should call *get_N_shares()* right after the computation of the portfolio weights from the actions determined by the agent.\n",
    "\n",
    "* There should be an *execute_trades()* method that buys and sells asset shares to realize the output of *get_N_shares()*. The tricky part here is selling shares before buying new ones, and the order in which each trade should be executed (sort by return?). This seems like a mathematical finance question of its own.\n",
    "\n",
    "* There should be a *rebalance_portfolio()* method. This one would be called by *step()* before the state and hitory lists updates, and would in turn call the execute trades method. It could possibly also be the method that sends orders to a brokerage API during paper-trading, and would potentially require the implementation of additional *buy()* and *sell()* methods.\n",
    "\n",
    "* Instead of computing the portfolio weights in *step()* using *StockPortfolioEnv.softmax_normalization()*, it might be more optimal and accurate to use:\n",
    "\n",
    "        weights = torch.nn.functional.softmax(torch.tensor(action)).numpy().\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882dd99-6a42-4352-8dde-eb2122a32159",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4554b00e-5484-4f56-9337-a7721cd823b1",
   "metadata": {
    "id": "59712f71-0291-4743-b9d3-c4472b6ae060",
    "tags": []
   },
   "source": [
    "<a id='4'></a>\n",
    "## 4 - Deep RL agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd945e7c-8bb1-4191-a481-db870122245a",
   "metadata": {
    "id": "59712f71-0291-4743-b9d3-c4472b6ae060",
    "tags": []
   },
   "source": [
    "The purpose of this section is to give a description of how a FinRL agent is implemented and used. In many respects however, this implementation is impractical for several reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c29bf8f-8422-4389-ba11-345a616ffc42",
   "metadata": {
    "id": "c5ceb156-6a25-4589-96be-d21d78bdf37a"
   },
   "source": [
    "Recall that in FinRL, creating a deep RL agent, training it, and running a backtest is executed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09391d8b-5103-4704-abf9-a0f18b940280",
   "metadata": {
    "id": "c5ceb156-6a25-4589-96be-d21d78bdf37a"
   },
   "outputs": [],
   "source": [
    "    # Create gym/SB3 training environments\n",
    "    gym_env_train = StockTradingEnv(df = data_train, **env_kwargs)\n",
    "    sb3_env_train, _ = gym_env_train.get_sb_env()\n",
    "    \n",
    "    # (a) - Inst. agent and model \n",
    "    agent = DRLAgent(env = env_train)\n",
    "    model_ = agent.get_model(\"model_name\")\n",
    "    \n",
    "    # (b) -Train agent\n",
    "    trained_model = agent.train_model(model=model_, \n",
    "                                     tb_log_name='model_name',\n",
    "                                     total_timesteps=50000)\n",
    "                                     \n",
    "    # (c) - Run a backtest\n",
    "    gym_env_test = StockPortfolioEnv(df = data_test, **env_kwargs)\n",
    "    sb3_env_test, _ = gym_env_test.get_sb_env()\n",
    "    df_test_daily_return, df_test_actions = DRLAgent.DRL_prediction(model=trained_model,\n",
    "                                                                   environment = sb3_env_test)\n",
    "    # (d) - Plot results and get performance stats\n",
    "    backtest_plot(df_daily_return_ppo, \n",
    "             baseline_ticker = 'Baseline_ticker', \n",
    "             baseline_start = df_test_daily_return.loc[0,'date'],\n",
    "             baseline_end = df_test_daily_return.loc[len(df_test_daily_return)-1,'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64468e3-c8df-4add-a849-de28e9aa8317",
   "metadata": {
    "id": "c5ceb156-6a25-4589-96be-d21d78bdf37a"
   },
   "source": [
    "The next subsections discuss each of blocks (a)-(d) above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb5db4-27df-42ed-8f80-06278c4706d4",
   "metadata": {
    "id": "b9308c84-0db4-4801-8578-ccfc8fa335bf"
   },
   "source": [
    "<a id='4.a'></a>\n",
    "### a) The DRLAgent class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294cce3-aeff-419a-b0b0-98c7e7b2ff3d",
   "metadata": {
    "id": "b9308c84-0db4-4801-8578-ccfc8fa335bf"
   },
   "source": [
    "This class is imported via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a16a88-ba5a-41ee-be02-d6791fc4aa77",
   "metadata": {
    "id": "b9308c84-0db4-4801-8578-ccfc8fa335bf"
   },
   "outputs": [],
   "source": [
    "            from finrl.agents.stablebaselines3.models import DRLAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ff998-51a7-474c-9933-5126c8faa7a9",
   "metadata": {
    "id": "b9308c84-0db4-4801-8578-ccfc8fa335bf"
   },
   "source": [
    "and is implemented here: \n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/agents/stablebaselines3/models.py, \n",
    "\n",
    "where one finds the description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e066e-f214-492f-9d7a-4c59af58c4fa",
   "metadata": {
    "id": "b9308c84-0db4-4801-8578-ccfc8fa335bf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "            Attributes\n",
    "            ----------\n",
    "                env: gym environment class\n",
    "                    user-defined class\n",
    "            Methods\n",
    "            -------\n",
    "                get_model()\n",
    "                    setup DRL algorithms\n",
    "                train_model()\n",
    "                    train DRL algorithms in a train dataset\n",
    "                    and output the trained model\n",
    "                DRL_prediction()\n",
    "                    make a prediction in a test dataset and get results\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db471a1b-3201-487e-948f-80014a46fece",
   "metadata": {
    "id": "b9308c84-0db4-4801-8578-ccfc8fa335bf"
   },
   "source": [
    "Looking at the file linked above, we note that:\n",
    "* The constructor of DRLAgent assigns only the environment as an attribute. This is an SB3 environment passed-on to the constructor of the SB3 algorithm used. As mentioned in the sections on Stable Baselines 3 algorithms, the environment is required for the training, but the argument can be *None* when **loading** a trained agent.\n",
    "* The file models.py imports one of the following models from sb3: A2C, DDPG, PPO or SAC. The point here is that *get_model()* returns one of the algorithms implemented in stable_baselines3, but FinRL does not support all SB3 agents because of how the actions are defined in this library.\n",
    "* Another point to discuss below is the policy argument of *get_model()*. By default, it's an \"MlpPolicy\" object from SB3.\n",
    "* The *train_model(self, model, tb_log_name, total_timesteps)* method is essentially a wrapper for the *model.train()* method of SB3 models.\n",
    "* The method *DRL_prediction(model, env, deterministic)* first creates a test environment, and then loops over the contents of the attribute *env.df* of the environment and calls . **Remark:** Understanding this method hinges on understanding FinRL's environments.\n",
    "* Throughout the implementation, one finds several instructions for the management of tensorboard callbacks. \n",
    "\n",
    "\n",
    "Now let's comment on some downsides:\n",
    "* The DRLAgent class **does not store** the SB3 algorithm as an attribute. There is only one attribute in this class, which is the environment. I don't understand this design choice, and I absolutely do not like it. I will re-write/modify this class at some point.\n",
    "* From the implementation, it seems like the DRLAgent is just a book-keeping object that creates a model on the side. It doesn't even have a save method.\n",
    "* The *DRL_prediction()* method is also clunky and unclear. Why do they use a test environment?\n",
    "* There are several tautological instructions in this implementation. See comments in code here and next subsections.\n",
    "\n",
    "For future reference, here is the original implementation of DRLAgent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37780b-31e6-43a2-927e-943808096537",
   "metadata": {
    "id": "2f37780b-31e6-43a2-927e-943808096537"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "##### Imports #####\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.noise import OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from finrl import config\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "\n",
    "'''\n",
    "MODELS = {\"a2c\": A2C, \"ddpg\": DDPG, \"td3\": TD3, \"sac\": SAC, \"ppo\": PPO}\n",
    "\n",
    "MODEL_KWARGS = {x: config.__dict__[f\"{x.upper()}_PARAMS\"] for x in MODELS.keys()}\n",
    "\n",
    "NOISE = {\n",
    "    \"normal\": NormalActionNoise,\n",
    "    \"ornstein_uhlenbeck\": OrnsteinUhlenbeckActionNoise,\n",
    "}\n",
    "\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        try:\n",
    "            self.logger.record(key=\"train/reward\", value=self.locals[\"rewards\"][0])\n",
    "        except BaseException:\n",
    "            self.logger.record(key=\"train/reward\", value=self.locals[\"reward\"][0])\n",
    "        return True\n",
    "\n",
    "\n",
    "class DRLAgent:\n",
    "    \"\"\"Provides implementations for DRL algorithms\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        env: gym environment class\n",
    "            user-defined class\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "        get_model()\n",
    "            setup DRL algorithms\n",
    "        train_model()\n",
    "            train DRL algorithms in a train dataset\n",
    "            and output the trained model\n",
    "        DRL_prediction()\n",
    "            make a prediction in a test dataset and get results\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    #######################\n",
    "    ### Build SB3 model ###\n",
    "    #######################\n",
    "    def get_model(\n",
    "        self,\n",
    "        model_name,\n",
    "        policy=\"MlpPolicy\",\n",
    "        policy_kwargs=None,\n",
    "        model_kwargs=None,\n",
    "        verbose=1,\n",
    "        seed=None,\n",
    "        tensorboard_log=None,\n",
    "    ):\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = MODEL_KWARGS[model_name]\n",
    "\n",
    "        if \"action_noise\" in model_kwargs:\n",
    "            n_actions = self.env.action_space.shape[-1]\n",
    "            model_kwargs[\"action_noise\"] = NOISE[model_kwargs[\"action_noise\"]](\n",
    "                mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions)\n",
    "            )\n",
    "        print(model_kwargs)\n",
    "        return MODELS[model_name](\n",
    "            policy=policy,\n",
    "            env=self.env,\n",
    "            tensorboard_log=tensorboard_log,\n",
    "            verbose=verbose,\n",
    "            policy_kwargs=policy_kwargs,\n",
    "            seed=seed,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "\n",
    "    def train_model(self, model, tb_log_name, total_timesteps=5000):\n",
    "        model = model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            tb_log_name=tb_log_name,\n",
    "            callback=TensorboardCallback(),\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    ######################\n",
    "    ### Predict method ###\n",
    "    ######################\n",
    "    @staticmethod\n",
    "    def DRL_prediction(model, environment, deterministic=True):\n",
    "        test_env, test_obs = environment.get_sb_env() # Clarify what this does\n",
    "        \"\"\"make a prediction\"\"\"\n",
    "        account_memory = []\n",
    "        actions_memory = []\n",
    "        #         state_memory=[] #add memory pool to store states\n",
    "        test_env.reset() # Clarify\n",
    "        for i in range(len(environment.df.index.unique())): # Fix this\n",
    "            action, _states = model.predict(test_obs, deterministic=deterministic)\n",
    "            # account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "            # actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "            if i == (len(environment.df.index.unique()) - 2): # Fix this\n",
    "                account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "            #                 state_memory=test_env.env_method(method_name=\"save_state_memory\") # add current state to state memory\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "        return account_memory[0], actions_memory[0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def DRL_prediction_load_from_file(model_name, environment, cwd, deterministic=True):\n",
    "        if model_name not in MODELS:\n",
    "            raise NotImplementedError(\"NotImplementedError\")\n",
    "        try:\n",
    "            # load agent\n",
    "            model = MODELS[model_name].load(cwd)\n",
    "            print(\"Successfully load model\", cwd)\n",
    "        except BaseException:\n",
    "            raise ValueError(\"Fail to load agent!\")\n",
    "\n",
    "        # test on the testing env\n",
    "        state = environment.reset()\n",
    "        episode_returns = []  # the cumulative_return / initial_account\n",
    "        episode_total_assets = [environment.initial_total_asset]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = model.predict(state, deterministic=deterministic)[0]\n",
    "            state, reward, done, _ = environment.step(action)\n",
    "\n",
    "            total_asset = (\n",
    "                environment.amount\n",
    "                + (environment.price_ary[environment.day] * environment.stocks).sum()\n",
    "            )\n",
    "            episode_total_assets.append(total_asset)\n",
    "            episode_return = total_asset / environment.initial_total_asset\n",
    "            episode_returns.append(episode_return)\n",
    "\n",
    "        print(\"episode_return\", episode_return)\n",
    "        print(\"Test Finished!\")\n",
    "        return episode_total_assets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb8612-188d-4142-a895-fe9511430d44",
   "metadata": {
    "id": "3cbb8612-188d-4142-a895-fe9511430d44"
   },
   "source": [
    "<a id='4.b'></a>\n",
    "### b) Training - The *DRLAgent.train_model()* model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7291c-6318-4d83-bbd7-511d3791d2f8",
   "metadata": {
    "id": "a658c3c8-0181-4bdb-a89e-0b58c05c77ee"
   },
   "source": [
    "This is the first method that links SB3 algorithm classes to the FinRL portfolio environment. The training method is implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018dbee-3bae-4b19-bdc5-83a73eb19487",
   "metadata": {
    "id": "a658c3c8-0181-4bdb-a89e-0b58c05c77ee"
   },
   "outputs": [],
   "source": [
    "        def train_model(self, model, tb_log_name, total_timesteps=5000):\n",
    "            \n",
    "            model = model.learn(\n",
    "                total_timesteps=total_timesteps,\n",
    "                tb_log_name=tb_log_name,\n",
    "                callback=TensorboardCallback())\n",
    "            \n",
    "            return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561ce46a-a8d6-4a6b-ba97-51746e55b34f",
   "metadata": {
    "id": "a658c3c8-0181-4bdb-a89e-0b58c05c77ee"
   },
   "source": [
    "           \n",
    "* The *learn()* method of *On/OffPolicyAlgorithm* calls its *collect_rollouts()* method, which takes the training environment of the algorithm as an argument. (The latter is where the *step()* method is called)\n",
    "* The detailed description of functions called through *BaseAlgorithm.learn()*, the reader is referred to Sections 2.b-c on SB3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c2b356-d970-412f-9306-3becfc8b8eac",
   "metadata": {
    "id": "35c2b356-d970-412f-9306-3becfc8b8eac"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6902f5c-5787-4460-ad30-371dcb7df903",
   "metadata": {
    "id": "f6902f5c-5787-4460-ad30-371dcb7df903"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d336224-366d-4da9-a3c3-2a64acb5745a",
   "metadata": {
    "id": "3cbb8612-188d-4142-a895-fe9511430d44"
   },
   "source": [
    "<a id='4.c'></a>\n",
    "### c) Backtesting - The *DRLAgent.DRL_prediction()* method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab986c-8d42-4339-bc43-78eefbdeba7e",
   "metadata": {
    "id": "14103da6-2c23-4854-9bf2-e538c86518f9"
   },
   "source": [
    "Here is the backtesting algorithm, from *agents.stablebaselines3.models* submodule of FinRL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f55c5c2-5a2e-419a-9f3d-a904a743a335",
   "metadata": {
    "id": "14103da6-2c23-4854-9bf2-e538c86518f9"
   },
   "outputs": [],
   "source": [
    "    \n",
    "        def DRL_prediction(model, environment, deterministic=True):\n",
    "            '''\n",
    "                Perform backtest of portfolio optimization using\n",
    "                a stable_baselines3 deep RL algorithm\n",
    "            '''\n",
    "        \n",
    "            # Init. SB3 environment\n",
    "            test_env, test_obs = environment.get_sb_env()\n",
    "            test_env.reset() # Tautological, \n",
    "            \n",
    "            # Init. lists\n",
    "            account_memory = []\n",
    "            actions_memory = []\n",
    "            \n",
    "            # Main loop\n",
    "            for i in range(len(environment.df.index.unique())):\n",
    "                # Compute action corresponding to current observation\n",
    "                action, _states = model.predict(test_obs, deterministic=deterministic)\n",
    "                \n",
    "                # Execute the step() function\n",
    "                test_obs, rewards, dones, info = test_env.step(action)\n",
    "                \n",
    "                # Store history\n",
    "                if i == (len(environment.df.index.unique()) - 2):\n",
    "                    account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                    actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "                    \n",
    "                if dones[0]:\n",
    "                    print(\"hit end!\")\n",
    "                    break\n",
    "                    \n",
    "            return account_memory[0], actions_memory[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad8824-addc-41b3-acfc-f15b4350208b",
   "metadata": {
    "id": "14103da6-2c23-4854-9bf2-e538c86518f9"
   },
   "source": [
    "            \n",
    "Ref: https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/agents/stablebaselines3/models.py.\n",
    "\n",
    "We note the following:\n",
    "\n",
    "* The *test_env* object is an SB3 *DummyVecEnv*. This is for normalization purposes since *model* is an SB3 algorithm.\n",
    "* As mentioned in the section on SB3's *DummyVecEnv*/*VecEnv* environment classes, the *reset()* and *step()* methods of these classes are just wrappers for the corresponding methods of *StockPortfolioEnv*. The *step()* method called above is precisely FinRL's.\n",
    "* Regarding the output: The code above returns the *asset_memory* and *actions_memory* attributes of the underlying *StockPortfolioEnv*. These are respectively the daily portfolio values and portfolio weights during the backtest, up to the day before last.\n",
    "* A more in-depth description of the process behind *model.predict()* call is given in Sections 2.b-c on SB3 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62316e7a-6f5e-46ab-b4a7-6c82fec91633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc065ea1-1842-4055-b5a4-589b41aa409b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06bc5d1-2520-4b19-9594-630ac9cc5f6f",
   "metadata": {
    "id": "3cbb8612-188d-4142-a895-fe9511430d44"
   },
   "source": [
    "<a id='4.d'></a>\n",
    "### d) Backtest plots and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0dd935-0e5c-4701-a2f8-31b8fd558d52",
   "metadata": {},
   "source": [
    "The submodule *finrl.plot* contains several utilities for analyzing the results of a backtest:\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/plot.py.\n",
    "\n",
    "The function used in FinRL's tutorials is *backtest_plot()*, for which:\n",
    "\n",
    "* One needs the start and end dates of the backtest, as well as the baseline ticker. \n",
    "* For the baseline, *backtest_plot()* calls *finrl.plot.get_baseline()* which uses the Yahoo downloader to acquire the baseline data.\n",
    "* The backtest output required by the method is the daily portfolio value. *backtest_plot()* has a *value_col_name* for specifying which column of the input dataframe *account_value* contains the daily account/portfolio values.\n",
    "\n",
    "The last block of *backtest_plot()* opens a context manager to call the *create_full_tear_sheet()* function of Quantopian's *PyFolio*. We note that:\n",
    "\n",
    "* The inputs of *create_full_tear_sheet()* are the daily portfolio returns and benchmark returns, and the \n",
    "* The implemetation is here: https://github.com/quantopian/pyfolio/blob/master/pyfolio/tears.py.\n",
    "* For the computation of drawdown periods, this method calls the *timeseries* submodule of *pyfolio*, in which there is (still) a bug at the time of writing (22/09/30). *pyfolio.timeseries* can easily be patched following the discussion here: https://stackoverflow.com/questions/63554616/attributeerror-numpy-int64-object-has-no-attribute-to-pydatetime.\n",
    "\n",
    "To summarize, FinRL's performance plots and statistics are just those of *PyFolio*. More details on the metrics and statistics computed by this library's tearsheets can be found at the following links:\n",
    "\n",
    "* Brief description: https://pyfolio.ml4trading.io/.\n",
    "* Full tear sheet example: https://pyfolio.ml4trading.io/notebooks/zipline_algo_example.html#Full-tear-sheet-example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d824a-fbc2-453b-9aa5-ac28575a9622",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c2d1a-332d-493e-8bc0-3144ce97312b",
   "metadata": {},
   "source": [
    "<a id='5'></a>\n",
    "## 5 - Financial data - Acquisition and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54e851-be33-4b42-8840-fb9ba4e28e4e",
   "metadata": {},
   "source": [
    "<a id='5.a'></a>\n",
    "### a) Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e57d9-6039-41e2-b902-52950c07c5cb",
   "metadata": {},
   "source": [
    "The most used downloader in FinRL's tutorials is the *YahooDownloader* class in *meta.preprocessor.yahoodownloader*:\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/preprocessor/yahoodownloader.py\n",
    "\n",
    "As illustrated in the first section on general pipelines, these objects are used for their *fetch_data()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0776cabc-188a-48e8-9c9c-6e5bbb0b2e00",
   "metadata": {},
   "outputs": [],
   "source": [
    " def fetch_data(self, proxy=None) -> pd.DataFrame:\n",
    "        \"\"\"Fetches data from Yahoo API\n",
    "        Parameters\n",
    "        ----------\n",
    "        Returns\n",
    "        -------\n",
    "        `pd.DataFrame`\n",
    "            7 columns: A date, open, high, low, close, volume and tick symbol\n",
    "            for the specified stock ticker\n",
    "        \"\"\"\n",
    "        # Download and save the data in a pandas DataFrame:\n",
    "        data_df = pd.DataFrame()\n",
    "        for tic in self.ticker_list:\n",
    "            temp_df = yf.download(\n",
    "                tic, start=self.start_date, end=self.end_date, proxy=proxy\n",
    "            )\n",
    "            temp_df[\"tic\"] = tic\n",
    "            data_df = data_df.append(temp_df)\n",
    "        # reset the index, we want to use numbers as index instead of dates\n",
    "        data_df = data_df.reset_index()\n",
    "        try:\n",
    "            # convert the column names to standardized names\n",
    "            data_df.columns = [\"date\", \"open\", \"high\", \"low\", \n",
    "                               \"close\", \"adjcp\",\"volume\",\"tic\"]\n",
    "            # use adjusted close price instead of close price\n",
    "            data_df[\"close\"] = data_df[\"adjcp\"]\n",
    "            # drop the adjusted close price column\n",
    "            data_df = data_df.drop(labels=\"adjcp\", axis=1)\n",
    "        except NotImplementedError:\n",
    "            print(\"the features are not supported currently\")\n",
    "        # create day of the week column (monday = 0)\n",
    "        data_df[\"day\"] = data_df[\"date\"].dt.dayofweek\n",
    "        # convert date to standard string format, easy to filter\n",
    "        data_df[\"date\"] = data_df.date.apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "        # drop missing data\n",
    "        data_df = data_df.dropna()\n",
    "        data_df = data_df.reset_index(drop=True)\n",
    "        print(\"Shape of DataFrame: \", data_df.shape)\n",
    "        # print(\"Display DataFrame: \", data_df.head())\n",
    "\n",
    "        data_df = data_df.sort_values(by=[\"date\", \"tic\"]).reset_index(drop=True)\n",
    "\n",
    "        return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345c63a5-03b1-4d47-b439-d15b143bb5e7",
   "metadata": {},
   "source": [
    "Here, the principal takeaway is the formatting of the Pandas dataframes into FinRL's format as described in Section 3.b (see *df* input of *StockPortfolioEnv* constructor).\n",
    "\n",
    "**Comment:** I doubt that dropping the adjusted close column is a good idea for portfolio optimization. It makes sense for the stock trading environment since the adjusted closes aren't used for (live) paper trading, but this information is important for realistic backtests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2356dff-f8cc-4345-bd7a-3c5b25d9e1f2",
   "metadata": {},
   "source": [
    "<a id='5.b'></a>\n",
    "### b) The *FeatureEngineer* class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94929438-9090-4b82-a9ab-8a21452107a3",
   "metadata": {},
   "source": [
    "This class is found in the *meta.preprocessor.preprocessors* submodule:\n",
    "\n",
    "https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/preprocessor/preprocessors.py.\n",
    "\n",
    "Its principal method is *preprocess_data()*, which is used for example as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611d22c-8b30-4d31-ac4a-e7b745dbd1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "           fe = FeatureEngineer(\n",
    "                    use_technical_indicator=True,\n",
    "                    tech_indicator_list = INDICATORS,\n",
    "                    use_vix=True,\n",
    "                    use_turbulence=True,\n",
    "                    user_defined_feature = False)\n",
    "           df = fe.preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fb185-906c-4068-980d-17d97314df68",
   "metadata": {},
   "source": [
    "To comment on the arguments of *FeatureEngineer*, each of the following are computed using a corresponding class method:\n",
    "\n",
    "* The users can implement their own features by modifying the *preprocess_data()* method.\n",
    "\n",
    "* The *use_turbulence* boolean indicates whether to add the *turbulence index* of stocks (calls *calculate_turbulence()*) to the dataset. In technical terms, the turbulence index is the Mahalanobis distance between the historical average of asset returns and their values at a given period (see https://www.top1000funds.com/wp-content/uploads/2010/11/FAJskulls.pdf by Kritzman-Li). More informally, it is supposed to quantify the degree of unusual return behavior within a \"universe\" of assets.\n",
    "\n",
    "* The *use_vix* boolean indicates whether to add the *CBOE volatility index* (VIX) to the data. This is essentially an additional (real-time) ticker that is derived from prices of SPX index options with near-term expiration dates, the VIX generates a 30-day forward projection of volatility (https://www.investopedia.com/terms/v/vix.asp). This is supposed to gauge market sentiment.\n",
    "\n",
    "* The *use_technical_indicator* boolean indicates whether to add the technical indicators specified in *tech_indicator_list* to the dataset. These are computed using the *stockstats* library, a certain add-on to *Pandas* that we discuss more below.\n",
    "\n",
    "* Before computing any of the quantities above, *preprocess_data()* calls *clean_data()* that drops tickers with inconsistent merged closes (see code on GitHub).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8417567-bf19-4f83-a2f1-77f41bb4882a",
   "metadata": {},
   "source": [
    "Returning now to the *stockstats* library, the description and code are at the following pages:\n",
    "* Docs: https://openbase.com/python/stockstats/documentation\n",
    "* GitHub Page: https://github.com/jealous/stockstats\n",
    "\n",
    "This library provides a wrapper class *StockDataFrame* for *pandas.DataFrame* and computes about 40 technical indicators from the date-close-high-low-volume columns of the data.\n",
    "\n",
    "In FinRL's portfolio optimization tutorial, the technical indicators used are the following:\n",
    "\n",
    "* Moving Average Convergence Divergence(MACD): A trend-following momentum indicator indicator that shows the relationship between two exponential moving averages (EMA's) of a security’s price. The MACD is calculated by subtracting the 26-period exponential moving average (EMA) from the 12-period EMA. (Source: https://www.investopedia.com/terms/m/macd.asp)\n",
    "\n",
    "* Relative Strength Index (RSI): A momentum indicator used in technical analysis. RSI measures the speed and magnitude of a security's recent price changes to evaluate overvalued or undervalued conditions in the price of that security (source: https://www.investopedia.com/terms/r/rsi.asp). FinRL use *rsi_30* for RSI over 30 days.\n",
    "\n",
    "* Commodity Channel Index (CCI):  A momentum-based oscillator used to help determine when an investment vehicle is reaching a condition of being overbought or oversold (source: https://www.investopedia.com/terms/c/commoditychannelindex.asp). FinRL use *cci_30* for CCI over 30 days.\n",
    "\n",
    "* Directional Movement Index (DX): A technical indicator that measures both the strength and direction of a price movement and is intended to reduce false signals (source: https://www.investopedia.com/terms/d/dmi.asp). FinRL use 'dx_30'.\n",
    "\n",
    "**Comments:** \n",
    "\n",
    "1) In a more refined version of *StockPortfolioEnv*, it would be ideal to have technical indicators computed using the C-based *TA-Lib* library (https://mrjbq7.github.io/ta-lib/doc_index.html). I've noticed some inconsistent values in the dataframes obtained from *stockstats*, and I would rather use *TA-Lib* if possible, given that it's been developed and maintained for 17 years before *stockstats*.\n",
    "\n",
    "2) Technical analysis is astrology for traders. (Source: https://i.redd.it/0hkqhlj7e4w61.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e165fc7-4d2d-4d3b-8f27-6c1acc52e27d",
   "metadata": {},
   "source": [
    "<a id='5.c'></a>\n",
    "### c) Return covariances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c79da7-1534-4917-b678-86c6922249ff",
   "metadata": {},
   "source": [
    "In principle, the calculation of return covariances over the lookback period should be part of the *FeatureEngineer.preprocess_data()* function discussed above. In the portfolio optimization tutorial it is implemented by hand before the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0b397-573e-47d2-bdad-f4f0ba828e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add covariance matrix to states\n",
    "df=df.sort_values(['date','tic'],ignore_index=True)\n",
    "df.index = df.date.factorize()[0]\n",
    "cov_list = []\n",
    "return_list = []\n",
    "lookback=252 # look back is one year\n",
    "\n",
    "for i in range(lookback,len(df.index.unique())):\n",
    "    # Init. temp. lookback dataframe\n",
    "    data_lookback = df.loc[i-lookback:i,:]\n",
    "    \n",
    "    # Init. temp. close prices dataframe\n",
    "    price_lookback=data_lookback.pivot_table(index = 'date',columns = 'tic', values = 'close')\n",
    "    \n",
    "    # Compute daily returns of price_lookback and append return_list\n",
    "    return_lookback = price_lookback.pct_change().dropna()\n",
    "    return_list.append(return_lookback)\n",
    "    \n",
    "    # Compute returns covariance and append to cov_list\n",
    "    covs = return_lookback.cov().values \n",
    "    cov_list.append(covs)\n",
    "\n",
    "df_cov = pd.DataFrame({'date':df.date.unique()[lookback:],'cov_list':cov_list,'return_list':return_list})\n",
    "\n",
    "# Assign 'cov_list' and 'return_list' to all tickers and sort\n",
    "df = df.merge(df_cov, on='date')\n",
    "df = df.sort_values(['date','tic']).reset_index(drop=True) # This is what drops the first 252 days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b02c7-94fe-4a2c-98e8-2590bc3663c9",
   "metadata": {},
   "source": [
    "_____________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cbc4d6-eb10-4930-9743-3ea784be7b7f",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efa818-16a9-46b0-8d30-5a727aee2f2e",
   "metadata": {
    "id": "a53c0697-947f-4131-8f5c-8b9adb05ff0e"
   },
   "source": [
    "### a) Imports\n",
    "\n",
    "For the sake of completeness, we include a (more or less complete) list of imports to give an idea of he packages used in the text above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8dd5ee-279f-4836-99d2-fe2b91acc166",
   "metadata": {
    "id": "1b8dd5ee-279f-4836-99d2-fe2b91acc166"
   },
   "outputs": [],
   "source": [
    "# Kill warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a42083-0de9-4801-b596-72d77445091e",
   "metadata": {
    "id": "44a42083-0de9-4801-b596-72d77445091e"
   },
   "outputs": [],
   "source": [
    "### Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "### OpenAI Gym and StableBaselines3\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "### Data: Acquisition and preprocessing\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader \n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "\n",
    "### FinRL: Environments\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "## NOTE: FinRL re-implement their portfolio optimization environment in a notebook.\n",
    "\n",
    "### FinRL: Deep RL algos\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "\n",
    "### Backtesting\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from pprint import pprint\n",
    "\n",
    "### What is this?\n",
    "'''\n",
    "import sys\n",
    "sys.path.append(\"../FinRL\")\n",
    "'''\n",
    "# Backtest imports\n",
    "import torch\n",
    "import plotly.express as px\n",
    "\n",
    "### PyPortFolioOpt\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import objective_functions\n",
    "\n",
    "### PyFolio and FinRL backtesting\n",
    "import pyfolio\n",
    "from pyfolio import timeseries # WARNING: Make sure this one is debugged\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "from finrl.plot import convert_daily_return_to_pyfolio_ts # 22/09/15: Issue here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4fa98-e855-4493-aad7-88836ed9958b",
   "metadata": {
    "id": "4f8ceb0f-7fe9-4d79-9980-39d1a7cec6d8"
   },
   "source": [
    "### b) Questions that should be answered (22/10/13):\n",
    "\n",
    "The actual purpose of this notebook is to clarify the following for future users of FinRL:\n",
    "\n",
    "* Write-down the typical FinRL pipeline, from data acquisition to backtest result analysis. <span style=\"color:red\">**(Done)**</span>\n",
    "* The portfolio optimization environment:\n",
    "    * Where are the states/actions written and/or modified. <span style=\"color:red\">**(Done)**</span>\n",
    "    * Where are the rewards modified. <span style=\"color:red\">**(Done)**</span>\n",
    "    * Objective functions: FinRL tutorial use the Sharpe ratio by default. Find how to switch this to cumulative returns. <span style=\"color:red\">**(Done. Not Sharpe, see rewards)**</span>\n",
    "* The FinRL DRLAgent class VS StableBaselines3:\n",
    "    * Find the original stablebaselines Model class, clarify how the latter is modified. <span style=\"color:red\">**(Done)**</span>\n",
    "    * Understand the implementation of states, actions, and rewards. <span style=\"color:red\">**(Done)**</span>\n",
    "    * Understand how the deep network(s) associated to an agent are instantiated <span style=\"color:red\">**(Done)**</span>, and how to customize them <span style=\"color:red\">**(Done. See SB3.)**</span>.\n",
    "    * Understand the training method of the DRLAgent class. <span style=\"color:red\">**(Done)**</span>\n",
    "    * Clarify what the output of an agent's network is in FinRL's implementation. <span style=\"color:red\">**(Done. Actions are outputs of actor networks. Their softmax gives the portfolio weights)**</span>\n",
    "    * Continuing previous, clarify how the policy is used for stock trading and for portfolio optimization. Might have to do this case by case (A2C, PPO, DDPG, SAC etc.) <span style=\"color:red\">**(Done)**</span>\n",
    "    * Clarify where the transaction costs are computed/added and if they are accounted for in the rewards. <span style=\"color:red\">**(Done. Not used in present implementation.)**</span>\n",
    "    * Understand argument passing between the environment and the agents in FinRL. <span style=\"color:red\"> **(Done. See StockPortfolioEnv.step() and DRLAgent.DRL_prediction())**</span>\n",
    "    * **Conceptual:** What does the discounted expected cumulative returns represent if the returns are the portfolio value?\n",
    "* Backtesting:\n",
    "    * Clarify FinRL's backtesting algorithms. <span style=\"color:red\">**(Done)**</span>\n",
    "    * Analyzing results with pyfolio (comment on bugs in that library). <span style=\"color:red\">**(Done)**</span>\n",
    "    * Saving the results of a backtest. <span style=\"color:green\">**(Just export to csv?)**</span>\n",
    "* Data handling:\n",
    "    * Clarify the preprocessing routines.  <span style=\"color:red\">**(Done)**</span>\n",
    "    * Comment on technical indicators, turbulence and adding other alpha factors. <span style=\"color:red\">**(Done)**</span>\n",
    "* Software engineering:\n",
    "    * Document everything. Several classes and functions will potentially have to be modified.\n",
    "    * Create a submodule with the modifications, and a functional \"\\_\\_main\\_\\_\" file with the full pipeline.\n",
    "    * Update the environment requirements file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701f1e7-6bf9-48c4-9ddd-062a9d732e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
